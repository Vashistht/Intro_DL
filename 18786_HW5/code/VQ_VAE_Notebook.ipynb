{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Lu_ii31MbQLY"},"outputs":[],"source":["!pip install umap-learn"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ODuTi7rklhtj"},"outputs":[],"source":["from __future__ import print_function\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.signal import savgol_filter\n","\n","\n","from six.moves import xrange\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torchvision.utils import make_grid"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0ioJq8eUbQLZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# Use this if you are using any Cuda enabled system\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","# elif torch.backends.mps.is_available():\n","#     device = 'mps' \n","else:\n","    device = 'cpu'\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"-9In5sLUbQLZ"},"source":["## Load Data"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"esdbxPadbQLZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [01:10<00:00, 2421713.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data/cifar-10-python.tar.gz to data\n","Files already downloaded and verified\n"]}],"source":["training_data = datasets.CIFAR10(root=\"data\", train=True, download=True,\n","                                  transform=transforms.Compose([\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n","                                  ]))\n","\n","validation_data = datasets.CIFAR10(root=\"data\", train=False, download=True,\n","                                  transform=transforms.Compose([\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n","                                  ]))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"YjLiqJ18bQLa"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.06328692405746414\n"]}],"source":["data_variance = np.var(training_data.data / 255.0)\n","print(data_variance)"]},{"cell_type":"markdown","metadata":{"id":"ZE7JB8CsbQLa"},"source":["## Vector Quantizer Layer"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"HKSgQ2WebQLb"},"outputs":[],"source":["class VectorQuantizer(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n","        super(VectorQuantizer, self).__init__()\n","\n","        self._embedding_dim = embedding_dim\n","        self._num_embeddings = num_embeddings\n","\n","        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n","        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n","        self._commitment_cost = commitment_cost\n","\n","    def forward(self, inputs):\n","        # convert inputs from BCHW -> BHWC\n","        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n","        input_shape = inputs.shape\n","\n","        # Flatten input\n","        flat_input = inputs.view(-1, self._embedding_dim)\n","\n","        # Calculate distances\n","        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n","                    + torch.sum(self._embedding.weight**2, dim=1)\n","                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n","\n","        # Encoding\n","        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n","        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n","        encodings.scatter_(1, encoding_indices, 1)\n","\n","        # Quantize and unflatten\n","        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n","\n","        # Loss\n","        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n","        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n","        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n","\n","        quantized = inputs + (quantized - inputs).detach()\n","        avg_probs = torch.mean(encodings, dim=0)\n","        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n","\n","        # convert quantized from BHWC -> BCHW\n","        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"]},{"cell_type":"markdown","metadata":{"id":"tUcURB0ubQLb"},"source":["## Encoder & Decoder Architecture"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eGpSGUWbbQLb"},"outputs":[],"source":["class Residual(nn.Module):\n","    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n","        super(Residual, self).__init__()\n","        self._block = nn.Sequential(\n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=in_channels,\n","                      out_channels=num_residual_hiddens,\n","                      kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=num_residual_hiddens,\n","                      out_channels=num_hiddens,\n","                      kernel_size=1, stride=1, bias=False)\n","        )\n","\n","    def forward(self, x):\n","        return x + self._block(x)\n","\n","\n","class ResidualStack(nn.Module):\n","    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n","        super(ResidualStack, self).__init__()\n","        self._num_residual_layers = num_residual_layers\n","        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n","                             for _ in range(self._num_residual_layers)])\n","\n","    def forward(self, x):\n","        for i in range(self._num_residual_layers):\n","            x = self._layers[i](x)\n","        return F.relu(x)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"g7itZyE1bQLc"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n","        super(Encoder, self).__init__()\n","\n","        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n","                                 out_channels=num_hiddens//2,\n","                                 kernel_size=4,\n","                                 stride=2, padding=1)\n","        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2,\n","                                 out_channels=num_hiddens,\n","                                 kernel_size=4,\n","                                 stride=2, padding=1)\n","        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,\n","                                 out_channels=num_hiddens,\n","                                 kernel_size=3,\n","                                 stride=1, padding=1)\n","        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n","                                             num_hiddens=num_hiddens,\n","                                             num_residual_layers=num_residual_layers,\n","                                             num_residual_hiddens=num_residual_hiddens)\n","\n","    def forward(self, inputs):\n","        x = self._conv_1(inputs)\n","        x = F.relu(x)\n","\n","        x = self._conv_2(x)\n","        x = F.relu(x)\n","\n","        x = self._conv_3(x)\n","        return self._residual_stack(x)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"b659HKpRbQLc"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, in_channels, out_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n","        super(Decoder, self).__init__()\n","\n","        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n","                                 out_channels=num_hiddens,\n","                                 kernel_size=3,\n","                                 stride=1, padding=1)\n","\n","        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n","                                             num_hiddens=num_hiddens,\n","                                             num_residual_layers=num_residual_layers,\n","                                             num_residual_hiddens=num_residual_hiddens)\n","\n","        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens,\n","                                                out_channels=num_hiddens//2,\n","                                                kernel_size=4,\n","                                                stride=2, padding=1)\n","\n","        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2,\n","                                                out_channels=out_channels,\n","                                                kernel_size=4,\n","                                                stride=2, padding=1)\n","\n","    def forward(self, inputs):\n","        x = self._conv_1(inputs)\n","\n","        x = self._residual_stack(x)\n","\n","        x = self._conv_trans_1(x)\n","        x = F.relu(x)\n","\n","        return self._conv_trans_2(x)"]},{"cell_type":"markdown","metadata":{"id":"jIimSGqZbQLc"},"source":["## Train"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"7GYTRFrBbQLc"},"outputs":[],"source":["batch_size = 256\n","num_training_updates = 5000\n","\n","num_hiddens = 128\n","num_residual_hiddens = 32\n","num_residual_layers = 2\n","\n","embedding_dim = 64\n","num_embeddings = 512\n","\n","commitment_cost = 0.25\n","\n","learning_rate = 1e-3"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"F_afHsOcbQLc"},"outputs":[],"source":["training_loader = DataLoader(training_data,\n","                             batch_size=batch_size,\n","                             shuffle=True,\n","                             pin_memory=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"HPbufjtUbQLd"},"outputs":[],"source":["validation_loader = DataLoader(validation_data,\n","                               batch_size=32,\n","                               shuffle=True,\n","                               pin_memory=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"4ufINNhjbQLd"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n","                 num_embeddings, embedding_dim, commitment_cost):\n","        super(Model, self).__init__()\n","\n","        self._encoder = Encoder(3, num_hiddens,\n","                                num_residual_layers,\n","                                num_residual_hiddens)\n","        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,\n","                                      out_channels=embedding_dim,\n","                                      kernel_size=1,\n","                                      stride=1)\n","\n","        self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n","                                           commitment_cost)\n","        self._decoder = Decoder(embedding_dim, 3,\n","                                num_hiddens,\n","                                num_residual_layers,\n","                                num_residual_hiddens)\n","\n","    def forward(self, x):\n","        z = self._encoder(x)\n","        z = self._pre_vq_conv(z)\n","        loss, quantized, perplexity, _ = self._vq_vae(z)\n","        x_recon = self._decoder(quantized)\n","\n","        return loss, x_recon, perplexity"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"SusDYDHNbQLd"},"outputs":[],"source":["model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n","              num_embeddings, embedding_dim,\n","              commitment_cost).to(device)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"aih3wscibQLd"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"j4rA2fnebQLd"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m recon_error \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(data_recon, data) \u001b[38;5;241m/\u001b[39m data_variance\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m recon_error \u001b[38;5;241m+\u001b[39m vq_loss\n\u001b[0;32m---> 12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m train_res_recon_error\u001b[38;5;241m.\u001b[39mappend(recon_error\u001b[38;5;241m.\u001b[39mitem())\n","File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model.train()\n","train_res_recon_error = []\n","\n","for i in xrange(num_training_updates):\n","    (data, _) = next(iter(training_loader))\n","    data = data.to(device)\n","    optimizer.zero_grad()\n","\n","    vq_loss, data_recon, perplexity = model(data)\n","    recon_error = F.mse_loss(data_recon, data) / data_variance\n","    loss = recon_error + vq_loss\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    train_res_recon_error.append(recon_error.item())\n","\n","    if (i+1) % 500 == 0:\n","        print('%d iterations' % (i+1))\n","        print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n","        print()"]},{"cell_type":"markdown","metadata":{"id":"1GsqECDVbQLd"},"source":["## Plot Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zey9c3aNbQLd"},"outputs":[],"source":["train_res_recon_error_smooth = savgol_filter(train_res_recon_error, 201, 7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeHYsInwbQLd"},"outputs":[],"source":["plt.figure(figsize=(12,6))\n","plt.plot(train_res_recon_error_smooth)\n","plt.yscale('log')\n","plt.title('Smoothed NMSE.')\n","plt.xlabel('iteration')\n","plt.savefig('nmse.png')"]},{"cell_type":"markdown","metadata":{"id":"_q8GkAkNbQLd"},"source":["## View Reconstructions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9O4VCCafbQLd"},"outputs":[],"source":["model.eval()\n","\n","(valid_originals, _) = next(iter(validation_loader))\n","valid_originals = valid_originals.to(device)\n","\n","vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n","_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n","valid_reconstructions = model._decoder(valid_quantize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DX_Wl-tGC3Oi"},"outputs":[],"source":["torch.max(model._encoder(valid_originals))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-Ih8lIHbQLd"},"outputs":[],"source":["(train_originals, _) = next(iter(training_loader))\n","train_originals = train_originals.to(device)\n","_, train_reconstructions, _, _ = model._vq_vae(train_originals)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W914Ld-EbQLd"},"outputs":[],"source":["def show(img):\n","    npimg = img.numpy()\n","    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n","    fig.axes.get_xaxis().set_visible(False)\n","    fig.axes.get_yaxis().set_visible(False)\n","    img = str(img)\n","    plt.savefig(f'{img}.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOJaXwsmbQLd"},"outputs":[],"source":["show(make_grid(valid_reconstructions.cpu().data)+0.5, )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ENIZHz9bQLe"},"outputs":[],"source":["show(make_grid(valid_originals.cpu()+0.5))"]},{"cell_type":"markdown","metadata":{"id":"1U8NlkOYbQLe"},"source":["## View Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDLuTFShbQLe"},"outputs":[],"source":["import umap.umap_ as umap\n","\n","proj = umap.UMAP(n_neighbors=3,\n","                 min_dist=0.1,\n","                 metric='cosine').fit_transform(model._vq_vae._embedding.weight.data.cpu())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wY7WdEtSbQLe"},"outputs":[],"source":["plt.scatter(proj[:,0], proj[:,1], alpha=0.3)\n","plt.savefig('embedding.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGQ931FYbQLe"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
