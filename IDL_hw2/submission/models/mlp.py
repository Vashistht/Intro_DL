import numpy as np
from mytorch.nn.activation import ReLU, Sigmoid, Tanh, LinearActivation
from mytorch.nn.initialization import Xavier, He
from mytorch.nn.linear import Linear
from mytorch.optim.optimizer import SGD, Adam

import numpyNN # given file 

'''
Comment string generated by copilot
    Paramters:
    input_dim: int (The dimension of the input data)
    output_dim: int (The dimension of the output data)
    hidden_neuron_list: list of int (The number of neurons in each hidden layer)
    activation_list: list of string (The activation function for each layer)
    opt_init: string (The initialization method for the weights in the linear layers)   
'''

class MLP():
    def __init__(self, input_dim, output_dim, hidden_neuron_list, activation_list, opt_init):

        self.input_dim = input_dim
        self.output_dim = output_dim
        # add input and output dim to hidden_neuron_list
        self.hidden_neuron_list = [self.input_dim] + hidden_neuron_list + [self.output_dim] 
        self.activation_list = activation_list
        self.opt_init = opt_init
        self.layers = self._build_layers()
    
            
    def _build_layers(self):
        layers = []
        for i in range(len(self.hidden_neuron_list) - 1):
            # Linear layer
            linear_layer = Linear(self.hidden_neuron_list[i], self.hidden_neuron_list[i+1], initialization=self.opt_init)
            layers.append(linear_layer)
            
            # Activation layer based on activation_list
            activation_fn = self._get_activation_fn(self.activation_list[i])
            layers.append(activation_fn)
        # print(layers)
        return layers

    

    def _get_activation_fn(self, name):
        if name == 'ReLU':
            return ReLU()
        elif (name == 'Sigmoid'):
            return Sigmoid()
        elif name == 'Tanh':
            return Tanh()
        elif name == 'LinearActivation':
            return LinearActivation()
        else:
            print("Invalid activation function")
            return None
    
    def forward(self, A):
        no_layers = len(self.layers)
        for i in range(no_layers):
            A = self.layers[i].forward(A)
        return A

    def backward(self, dLdA):
        no_layers = len(self.layers)
        for i in reversed(range(no_layers)):
            dLdA = self.layers[i].backward(dLdA)
        return dLdA


    def summary(self): # copilot  
        print("Model Summary")
        total_params = 0
        for i, layer in enumerate(self.layers):
            # source: https://stackoverflow.com/questions/1549801/what-are-the-differences-between-type-and-isinstance
            layer_type = "Linear" if isinstance(layer, Linear) else print('Invalid Layer Type')
            if isinstance(layer, Linear): 
                params = layer.dim_in * layer.dim_out + layer.dim_out  # W (in * out) + b (out)
                print(f"Layer {i+1}: {layer_type} - A Dim: {layer.dim_in}, Output Dim: {layer.dim_out}, Parameters: {params}")
            else:
                print(f"Layer {i+1}: {layer_type}")
                params = 0
            total_params += params
        print(f"Total Parameters: {total_params}")

    
    def copy(self): # did this to sort of mimic eval / not update any parameteres of the trained model
        copied_mlp = MLP(self.input_dim, self.output_dim, 
                         self.hidden_neuron_list[1:-1], self.activation_list, self.opt_init)
        for original_layer, copied_layer in zip(self.layers, copied_mlp.layers):
            if isinstance(original_layer, Linear):
                copied_layer.W = original_layer.W.copy()
                copied_layer.b = original_layer.b.copy()
        
        return copied_mlp