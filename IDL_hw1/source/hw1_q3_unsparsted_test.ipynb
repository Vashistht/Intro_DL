{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c574ceac-8715-4368-9f04-1342c814e8fd",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch\n",
    "\n",
    "# Problem 1: Simple MLP for Binary Classification\n",
    "In this problem, you will train a simple MLP to classify two handwritten digits: 0 vs 1. We provide some starter codes to do this task with steps. However, you do not need to follow the exact steps as long as you can complete the task in sections marked as <span style=\"color:red\">[YOUR TASK]</span>.\n",
    "\n",
    "## Dataset Setup\n",
    "We will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The `torchvision` package has supported this dataset. We can load the dataset in this way (the dataset will take up 63M of your disk space):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7de796-0049-4efe-9ea0-ce64a5e24c85",
   "metadata": {},
   "source": [
    "# Problem 3: Handling Class Imbalance in MNIST Dataset\n",
    "In this problem, we will explore how to handle class imbalance problems, which are very common in real-world applications. A modified MNIST dataset is created as follows: we choose all instances of digit “0”, and choose only 1\\% instances of digit “1” for both training and test sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22857293-024a-4252-a3fb-2cb72cafe0e0",
   "metadata": {},
   "source": [
    "For such a class imbalance problem, accuracy may not be a good metric. Always predicting \"0\" regardless of the input can be 99\\% accurate. Instead, we use the $F_1$ score as the evaluation metric:\n",
    "$$F_1 = 2\\cdot\\frac{\\text{precision}\\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "where precision and recall are defined as:\n",
    "$$\\text{precision}=\\frac{\\text{number of instances correctly predicted as \"1\"}}{\\text{number of instances predicted as \"1\"}}$$\n",
    "$$\\text{recall}=\\frac{\\text{number of instances correctly predicted as \"1\"}}{\\text{number of instances labeled as \"1\"}}$$\n",
    "\n",
    "To handle such a problem, some changes to the training may be necessary. Some suggestions include: \n",
    "1) Adjusting the class weights in the loss function, i.e., use a larger weight for the minority class when computing the loss.\n",
    "2) Implementing resampling techniques (either undersampling the majority class or oversampling the minority class).\n",
    "\n",
    "<span style=\"color:red\">[YOUR TASK]</span>\n",
    "- Create the imbalance datasets with all \"0\" digits and only 1\\% \"1\" digits.\n",
    "- Implement the training loop and evaluation section (implementing the $F_1$ metric). \n",
    "- Ignore the class imbalance problem and train the MLP. Report your hyper-parameter details and the $F_1$ score performance on the test set (as the baseline).\n",
    "- Explore modifications to improve the performance of the class imbalance problem. Report your modifications and the $F_1$ scores performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "872aca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94ec73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af33ec5-782b-4a0b-819b-f649500627c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data pre-processing\n",
    "# convert the input to the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "    )\n",
    "\n",
    "# Load the MNIST dataset \n",
    "# this command requires Internet to download the dataset\n",
    "mnist = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transform)\n",
    "mnist_test = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data',   # './data'\n",
    "                            train=False, \n",
    "                            download=True, \n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a14f718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies:  tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "print(\"Frequencies: \", torch.bincount(mnist.targets))\n",
    "print(len(torch.bincount(mnist.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d6f033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=4, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define your MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Your code goes here\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Your code goes here\n",
    "hidden_dim = 4\n",
    "model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daf6df17-48b9-4617-b12d-1d37865a5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def precision_score(labels, predictions):\n",
    "    predictions, labels = np.array(labels), np.array(predictions)\n",
    "    predictions_1 = np.sum(predictions==1)\n",
    "    correct_1 = np.sum( (predictions==1) & (labels==1))\n",
    "    precision = correct_1/ predictions_1 if predictions_1 > 0 else 1e-6\n",
    "    return precision\n",
    "\n",
    "def recall_score(labels, predictions):\n",
    "    predictions, labels = np.array(labels), np.array(predictions)\n",
    "    correct_1 = np.sum( (predictions==1) & (labels==1))\n",
    "    labels_1 = np.sum(labels==1)\n",
    "    recall = correct_1/ labels_1 if labels_1 > 0 else 1e-6\n",
    "    return recall\n",
    "\n",
    "def f1_score(labels, predictions):\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = (2 * (recall * precision)) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fdd534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_digit(weight, batch_size=64):\n",
    "    model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "        # print(f'Training accuracy: {train_acc:.2f}%')\n",
    "\n",
    "    training_time = time.time()- start_time\n",
    "    # print(training_time)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    val_preds = []; val_labels=[]\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        val_preds.append(pred)\n",
    "        val_labels.append(target)\n",
    "        # print(type(target))\n",
    "\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "    assert len(val_preds) == len(val_set)\n",
    "    \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_validation = f1_score(labels = val_labels, predictions = val_preds)\n",
    "    # print(f'F1 score validation: {f1_validation:.2f}')\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    test_preds = []; test_labels=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            test_preds.append(pred)\n",
    "            test_labels.append(target)\n",
    "        \n",
    "    test_preds = torch.cat(test_preds).numpy()\n",
    "    test_labels = torch.cat(test_labels).numpy()\n",
    "    assert len(test_preds) == len(test_set)   \n",
    "    test_acc = 100. * correct / total\n",
    "    # print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_test = f1_score(labels = test_labels, predictions =test_preds)\n",
    "    # print(f'F1 score test: {f1_test:.2f}')\n",
    "\n",
    "    \n",
    "    return training_time, train_acc, val_acc, test_acc, f1_validation, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ae61895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for digits 0 and 1\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a45cc01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set (before sparsing) 5923 6742 12665\n"
     ]
    }
   ],
   "source": [
    "train_0_original = [data for data in mnist if data[1] == 0]\n",
    "train_1_original = [data for data in mnist if data[1] == 1]\n",
    "print('Train set (before sparsing)', len(train_0_original), len(train_1_original), len(train_1_original) + len( train_0_original) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd07a3-e41b-4650-b4ae-ed4e2ff6c99d",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[EXTRA BONUS]</span>\n",
    "\n",
    "If the hyper-parameters are chosen properly, the baseline can perform satisfactorily on the class imbalance problem with 1% digit \"1\". We want to challenge the baseline and handle more class-imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0b82a-def4-45d0-9350-d79f00bfb671",
   "metadata": {},
   "source": [
    "Can you propose new ways for the class imbalance problem and achieve stable and satisfactory performance for large $N = 500, \\; 1000, \\; \\cdots$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af2babd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>F1-Val</th>\n",
       "      <th>F1-Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [N, Batch size, Weight, Train Time , Train Acc,  Val Acc, Test Acc, F1-Val, F1-Test]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = ['N', 'Batch size', 'Weight', 'Train Time ', 'Train Acc' ,' Val Acc', 'Test Acc', 'F1-Val', 'F1-Test']\n",
    "question3_df =  pd.DataFrame(columns = headers)\n",
    "question3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4c3f831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Train set (before sparsing) 5923 67 5990\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_26395/4147859766.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  question3_df = pd.concat([question3_df, pd.DataFrame([row], columns=headers)], ignore_index=True)\n",
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_26395/4147859766.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weight = torch.tensor(weight, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 Train set (before sparsing) 5923 26 5949\n",
      "\n",
      "\n",
      "500 Train set (before sparsing) 5923 13 5936\n",
      "\n",
      "\n",
      "750 Train set (before sparsing) 5923 8 5931\n",
      "\n",
      "\n",
      "1000 Train set (before sparsing) 5923 6 5929\n",
      "\n",
      "\n",
      "1250 Train set (before sparsing) 5923 5 5928\n",
      "\n",
      "\n",
      "1500 Train set (before sparsing) 5923 4 5927\n",
      "\n",
      "\n",
      "1750 Train set (before sparsing) 5923 3 5926\n",
      "\n",
      "\n",
      "2000 Train set (before sparsing) 5923 3 5926\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_list = [100] + [250*(i+1) for i in range(8)]\n",
    "for N in N_list:\n",
    "    train_0 = train_0_original.copy()\n",
    "    train_1 =  train_1_original.copy()\n",
    "    random.shuffle(train_1)\n",
    "    train_1 = train_1[:len(train_1) // N]\n",
    "    print(N, 'Train set (before sparsing)', len(train_0), len(train_1), len(train_1) + len( train_0) )# train_set = train_0 + train_1\n",
    "\n",
    "    # Split training data (1s)into training and validation sets\n",
    "    train_1len = int(len(train_1) *.8)\n",
    "    val_1len = len(train_1) - train_1len\n",
    "    train1_set, val1_set = random_split(train_1, [train_1len, val_1len])\n",
    "\n",
    "    # Split training data (0s) into training and validation sets\n",
    "    train_0len = int(len(train_0) *.8)\n",
    "    val_0len = len(train_0) - train_0len\n",
    "    train0_set, val0_set = random_split(train_0, [train_0len, val_0len])\n",
    "    \n",
    "    train_set = train0_set + train1_set\n",
    "    val_set = val0_set + val1_set\n",
    "    len(train_set), len(val_set)\n",
    "\n",
    "    # creating test set\n",
    "    test_0 = [data for data in mnist_test if data[1] == 0]\n",
    "    test_1 = [data for data in mnist_test if data[1] == 1]\n",
    "    # print(N,'Test set (before sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "\n",
    "    # test_1 = test_1[:len(test_1) // N]\n",
    "    # print(N,'Test set (after sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "    test_set = test_0 + test_1\n",
    "    print('\\n')\n",
    "    # Define DataLoaders to access data in batches\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size = 64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size = 64, shuffle=False)\n",
    "\n",
    "    # compensation = torch.tensor([1, N], dtype=torch.float32)\n",
    "    compensation = torch.tensor([1, (train_0len/ train_1len )], dtype=torch.float32)\n",
    "    weights = [[1,1], [1, int(N/10)], [1, int(N/2)], compensation]\n",
    "    batch_size = 64\n",
    "    results = []\n",
    "\n",
    "    # for batch_size in batch_sizes:\n",
    "    for weight in weights:\n",
    "        reweight_factor = weight[1]/ weight[0]\n",
    "        reweight_factor = float(reweight_factor)\n",
    "        weight = torch.tensor(weight, dtype=torch.float32)\n",
    "        weight = weight.to(device)\n",
    "        training_time, train_acc, val_acc, test_acc, f1_validation, f1_test = two_digit(batch_size=batch_size, weight = weight)\n",
    "        \n",
    "        row = [N, batch_size, reweight_factor, training_time, train_acc, val_acc, test_acc, f1_validation, f1_test]\n",
    "        question3_df = pd.concat([question3_df, pd.DataFrame([row], columns=headers)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5027811a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>F1-Val</th>\n",
       "      <th>F1-Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475049</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.581560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.531175</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.527187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.412803</td>\n",
       "      <td>98.893759</td>\n",
       "      <td>98.832360</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>89.396225</td>\n",
       "      <td>0.374980</td>\n",
       "      <td>98.893759</td>\n",
       "      <td>98.832360</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.381790</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.916037</td>\n",
       "      <td>97.021277</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.971454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.458914</td>\n",
       "      <td>99.957966</td>\n",
       "      <td>99.832074</td>\n",
       "      <td>99.669031</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.996912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>0.407742</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.916037</td>\n",
       "      <td>99.527187</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.995579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>236.899994</td>\n",
       "      <td>0.452447</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.916037</td>\n",
       "      <td>99.101655</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.991559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361602</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915825</td>\n",
       "      <td>86.997636</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.862155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.359780</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.784870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>0.404902</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.683215</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>473.799988</td>\n",
       "      <td>0.375496</td>\n",
       "      <td>99.978939</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.919622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428702</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>80.709220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.452313</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915754</td>\n",
       "      <td>85.531915</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.844196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>0.469320</td>\n",
       "      <td>99.978921</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>95.791962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>789.666687</td>\n",
       "      <td>0.515146</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.831508</td>\n",
       "      <td>80.189125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.773636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.673624</td>\n",
       "      <td>99.978912</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.075650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.586943</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.962175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0.442588</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>87.943262</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>1184.500000</td>\n",
       "      <td>0.404991</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>94.704492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.438534</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>52.907801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>0.419959</td>\n",
       "      <td>99.915647</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>0.508033</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>79.338061</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.761593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>1184.500000</td>\n",
       "      <td>0.473801</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>74.137116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.682530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506510</td>\n",
       "      <td>99.936722</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.471000</td>\n",
       "      <td>99.957815</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>93.995272</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.940737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>0.436189</td>\n",
       "      <td>99.873444</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.501182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>1579.333374</td>\n",
       "      <td>0.443803</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>86.619385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377484</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>72.860520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.661557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>0.383061</td>\n",
       "      <td>99.957806</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>0.407639</td>\n",
       "      <td>99.978903</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>75.366430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>2369.000000</td>\n",
       "      <td>0.400341</td>\n",
       "      <td>99.978903</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>92.955083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.444653</td>\n",
       "      <td>99.957806</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.432921</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>90.591017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.419397</td>\n",
       "      <td>99.957806</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>2369.000000</td>\n",
       "      <td>0.468812</td>\n",
       "      <td>99.957806</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       N Batch size       Weight  Train Time    Train Acc     Val Acc  \\\n",
       "0    100         64     1.000000     0.475049  100.000000  100.000000   \n",
       "1    100         64    10.000000     0.531175  100.000000  100.000000   \n",
       "2    100         64    50.000000     0.412803   98.893759   98.832360   \n",
       "3    100         64    89.396225     0.374980   98.893759   98.832360   \n",
       "4    250         64     1.000000     0.381790  100.000000   99.916037   \n",
       "5    250         64    25.000000     0.458914   99.957966   99.832074   \n",
       "6    250         64   125.000000     0.407742  100.000000   99.916037   \n",
       "7    250         64   236.899994     0.452447  100.000000   99.916037   \n",
       "8    500         64     1.000000     0.361602  100.000000   99.915825   \n",
       "9    500         64    50.000000     0.359780  100.000000  100.000000   \n",
       "10   500         64   250.000000     0.404902  100.000000  100.000000   \n",
       "11   500         64   473.799988     0.375496   99.978939  100.000000   \n",
       "12   750         64     1.000000     0.428702  100.000000  100.000000   \n",
       "13   750         64    75.000000     0.452313  100.000000   99.915754   \n",
       "14   750         64   375.000000     0.469320   99.978921  100.000000   \n",
       "15   750         64   789.666687     0.515146  100.000000   99.831508   \n",
       "16  1000         64     1.000000     0.673624   99.978912  100.000000   \n",
       "17  1000         64   100.000000     0.586943  100.000000  100.000000   \n",
       "18  1000         64   500.000000     0.442588  100.000000  100.000000   \n",
       "19  1000         64  1184.500000     0.404991  100.000000  100.000000   \n",
       "20  1250         64     1.000000     0.438534  100.000000   99.915683   \n",
       "21  1250         64   125.000000     0.419959   99.915647   99.915683   \n",
       "22  1250         64   625.000000     0.508033  100.000000  100.000000   \n",
       "23  1250         64  1184.500000     0.473801  100.000000   99.915683   \n",
       "24  1500         64     1.000000     0.506510   99.936722   99.915683   \n",
       "25  1500         64   150.000000     0.471000   99.957815  100.000000   \n",
       "26  1500         64   750.000000     0.436189   99.873444  100.000000   \n",
       "27  1500         64  1579.333374     0.443803  100.000000   99.915683   \n",
       "28  1750         64     1.000000     0.377484  100.000000   99.915683   \n",
       "29  1750         64   175.000000     0.383061   99.957806   99.915683   \n",
       "30  1750         64   875.000000     0.407639   99.978903   99.915683   \n",
       "31  1750         64  2369.000000     0.400341   99.978903  100.000000   \n",
       "32  2000         64     1.000000     0.444653   99.957806   99.915683   \n",
       "33  2000         64   200.000000     0.432921  100.000000  100.000000   \n",
       "34  2000         64  1000.000000     0.419397   99.957806   99.915683   \n",
       "35  2000         64  2369.000000     0.468812   99.957806   99.915683   \n",
       "\n",
       "     Test Acc    F1-Val   F1-Test  \n",
       "0   98.581560  1.000000  0.986607  \n",
       "1   99.527187  1.000000  0.995579  \n",
       "2   46.335697  0.000000  0.000000  \n",
       "3   46.335697  0.000000  0.000000  \n",
       "4   97.021277  0.909091  0.971454  \n",
       "5   99.669031  0.857143  0.996912  \n",
       "6   99.527187  0.909091  0.995579  \n",
       "7   99.101655  0.909091  0.991559  \n",
       "8   86.997636  0.800000  0.862155  \n",
       "9   96.784870  1.000000  0.969119  \n",
       "10  97.683215  1.000000  0.977938  \n",
       "11  97.919622  1.000000  0.980234  \n",
       "12  80.709220  1.000000  0.780881  \n",
       "13  85.531915  0.666667  0.844196  \n",
       "14  95.791962  1.000000  0.959193  \n",
       "15  80.189125  0.000000  0.773636  \n",
       "16  96.075650  1.000000  0.962048  \n",
       "17  91.962175  1.000000  0.919048  \n",
       "18  87.943262  1.000000  0.873449  \n",
       "19  94.704492  1.000000  0.948100  \n",
       "20  52.907801  0.000000  0.218210  \n",
       "21  46.335697  0.000000  0.000000  \n",
       "22  79.338061  1.000000  0.761593  \n",
       "23  74.137116  0.000000  0.682530  \n",
       "24  46.335697  0.000000  0.000000  \n",
       "25  93.995272  1.000000  0.940737  \n",
       "26  96.501182  1.000000  0.966302  \n",
       "27  86.619385  0.000000  0.857574  \n",
       "28  72.860520  0.000000  0.661557  \n",
       "29  46.335697  0.000000  0.000000  \n",
       "30  75.366430  0.000000  0.702115  \n",
       "31  92.955083  1.000000  0.929750  \n",
       "32  46.335697  0.000000  0.000000  \n",
       "33  90.591017  1.000000  0.903911  \n",
       "34  46.335697  0.000000  0.000000  \n",
       "35  46.335697  0.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question3_df.to_csv(f'q3_hyperopt_weight_unsparsed_test.csv')\n",
    "display(question3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb7f41-2e28-4935-a9cd-124e4963fa3d",
   "metadata": {},
   "source": [
    "# Weighted Resampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b00fcc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>F1-Val</th>\n",
       "      <th>F1-Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [N, Batch Size, Weight, Train Time , Train Acc,  Val Acc, Test Acc, F1-Val, F1-Test]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def convert_tensordata(dataset):\n",
    "#     dataset_data = [data[0] for data in dataset]\n",
    "#     dataset_labels = [data[1] for data in dataset]\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "headers = ['N', 'Batch Size', 'Weight', 'Train Time ', 'Train Acc' ,' Val Acc', 'Test Acc', 'F1-Val', 'F1-Test']\n",
    "question3_df_resample =  pd.DataFrame(columns = headers)\n",
    "question3_df_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab92cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_digit_resampling():\n",
    "    model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "    batch_size = 64\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "        # print(f'Training accuracy: {train_acc:.2f}%')\n",
    "\n",
    "    training_time = time.time()- start_time\n",
    "    # print(training_time)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    val_preds = []; val_labels=[]\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        val_preds.append(pred)\n",
    "        val_labels.append(target)\n",
    "        # print(type(target))\n",
    "\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "    assert len(val_preds) == len(val_set)\n",
    "    \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_validation = f1_score(labels = val_labels, predictions = val_preds)\n",
    "    # print(f'F1 score validation: {f1_validation:.2f}')\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    test_preds = []; test_labels=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            test_preds.append(pred)\n",
    "            test_labels.append(target)\n",
    "        \n",
    "    test_preds = torch.cat(test_preds).numpy()\n",
    "    test_labels = torch.cat(test_labels).numpy()\n",
    "    assert len(test_preds) == len(test_set)   \n",
    "    test_acc = 100. * correct / total\n",
    "    # print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_test = f1_score(labels = test_labels, predictions =test_preds)\n",
    "    # print(f'F1 score test: {f1_test:.2f}')\n",
    "\n",
    "    return training_time, train_acc, val_acc, test_acc, f1_validation, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4e3a88e-e3b2-4c82-bf42-ab789c452b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Train set (before sparsing) 5923 67 5990\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_26395/3702318992.py:56: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  question3_df_resample = pd.concat([question3_df_resample, pd.DataFrame([row], columns=headers)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 Train set (before sparsing) 5923 26 5949\n",
      "\n",
      "\n",
      "500 Train set (before sparsing) 5923 13 5936\n",
      "\n",
      "\n",
      "750 Train set (before sparsing) 5923 8 5931\n",
      "\n",
      "\n",
      "1000 Train set (before sparsing) 5923 6 5929\n",
      "\n",
      "\n",
      "1250 Train set (before sparsing) 5923 5 5928\n",
      "\n",
      "\n",
      "1500 Train set (before sparsing) 5923 4 5927\n",
      "\n",
      "\n",
      "1750 Train set (before sparsing) 5923 3 5926\n",
      "\n",
      "\n",
      "2000 Train set (before sparsing) 5923 3 5926\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_list = [100] + [250*(i+1) for i in range(8)]\n",
    "for N in N_list:\n",
    "    train_0 = train_0_original.copy()\n",
    "    train_1 =  train_1_original.copy()\n",
    "    random.shuffle(train_1)\n",
    "    train_1 = train_1[:len(train_1) // N]\n",
    "    print(N, 'Train set (before sparsing)', len(train_0), len(train_1), len(train_1) + len( train_0) )# train_set = train_0 + train_1\n",
    "\n",
    "    # Split training data (1s)into training and validation sets\n",
    "    train_1len = int(len(train_1) *.8)\n",
    "    val_1len = len(train_1) - train_1len\n",
    "    train1_set, val1_set = train_1[:train_1len], train_1[train_1len:]\n",
    "\n",
    "    # Split training data (0s) into training and validation sets\n",
    "    train_0len = int(len(train_0) *.8)\n",
    "    val_0len = len(train_0) - train_0len\n",
    "    train0_set, val0_set = train_0[:train_0len], train_0[train_0len:]\n",
    "    \n",
    "    # train and val set\n",
    "    train_set = train0_set + train1_set\n",
    "    val_set = val0_set + val1_set\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(val_set)\n",
    "    len(train_set), len(val_set)\n",
    "\n",
    "    # creating test set\n",
    "    test_0 = [data for data in mnist_test if data[1] == 0]\n",
    "    test_1 = [data for data in mnist_test if data[1] == 1]\n",
    "    # print(N,'Test set (before sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "\n",
    "    # test_1 = test_1[:len(test_1) // N]\n",
    "    # print(N,'Test set (after sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "    test_set = test_0 + test_1\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    print('\\n')\n",
    "    # compensation = torch.tensor([1, N], dtype=torch.float32)\n",
    "    compensation = int(train_0len/ train_1len)\n",
    "    weight_factors = [1, int(N/10), int(N/2), compensation]\n",
    "    batch_size = 64\n",
    "    results = []\n",
    "\n",
    "    # for batch_size in batch_sizes:\n",
    "    for weight_factor in weight_factors:\n",
    "        \n",
    "        weights = np.array( [1.0 if data[1] == 0 else weight_factor for data in train_set])\n",
    "        weights = torch.from_numpy(weights)\n",
    "        \n",
    "        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "        \n",
    "        train_loader = DataLoader(train_set, batch_size=64, sampler=sampler)\n",
    "        val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "        \n",
    "        training_time, train_acc, val_acc, test_acc, f1_validation, f1_test = two_digit_resampling()\n",
    "        \n",
    "        row = [N, batch_size, weight_factor, training_time, train_acc, val_acc, test_acc, f1_validation, f1_test]\n",
    "        question3_df_resample = pd.concat([question3_df_resample, pd.DataFrame([row], columns=headers)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b60a9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>F1-Val</th>\n",
       "      <th>F1-Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.388137</td>\n",
       "      <td>99.979128</td>\n",
       "      <td>99.916597</td>\n",
       "      <td>98.817967</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.988864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.374013</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.716312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>0.408951</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.858156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>89</td>\n",
       "      <td>0.422724</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.479905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416720</td>\n",
       "      <td>99.600673</td>\n",
       "      <td>99.496222</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>25</td>\n",
       "      <td>0.389752</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.541371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>125</td>\n",
       "      <td>0.512834</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.763593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>236</td>\n",
       "      <td>0.399923</td>\n",
       "      <td>99.978983</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.763593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.454007</td>\n",
       "      <td>99.831508</td>\n",
       "      <td>99.747475</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>0.412547</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915825</td>\n",
       "      <td>95.366430</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.954880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>250</td>\n",
       "      <td>0.547272</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.338061</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>473</td>\n",
       "      <td>0.512915</td>\n",
       "      <td>99.894693</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.763593</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.438797</td>\n",
       "      <td>99.873524</td>\n",
       "      <td>99.831508</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>75</td>\n",
       "      <td>0.407191</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915754</td>\n",
       "      <td>76.548463</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.720406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>375</td>\n",
       "      <td>0.373476</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.494090</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>789</td>\n",
       "      <td>0.362313</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915754</td>\n",
       "      <td>92.482270</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.924680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.385908</td>\n",
       "      <td>99.894559</td>\n",
       "      <td>99.831508</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>0.360009</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915754</td>\n",
       "      <td>81.323877</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.789333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.417010</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.014184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>1184</td>\n",
       "      <td>0.453959</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.489362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.913876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.397826</td>\n",
       "      <td>99.957824</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>125</td>\n",
       "      <td>0.391967</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>88.794326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>625</td>\n",
       "      <td>0.420383</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.290780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>1184</td>\n",
       "      <td>0.402556</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.716312</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436392</td>\n",
       "      <td>99.978907</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>150</td>\n",
       "      <td>0.368916</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>75.839243</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.709494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>750</td>\n",
       "      <td>0.334540</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>88.605201</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>1579</td>\n",
       "      <td>0.341033</td>\n",
       "      <td>99.978907</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.773050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.428044</td>\n",
       "      <td>99.957806</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>175</td>\n",
       "      <td>0.398755</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>81.087470</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.786096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>875</td>\n",
       "      <td>0.421275</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.690307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>2369</td>\n",
       "      <td>0.388738</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.643026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.421667</td>\n",
       "      <td>99.915612</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>46.335697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>200</td>\n",
       "      <td>0.374887</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>59.952719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.349644</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>85.248227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>2369</td>\n",
       "      <td>0.370446</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>93.853428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.939252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       N Batch Size Weight  Train Time    Train Acc     Val Acc   Test Acc  \\\n",
       "0    100         64      1     0.388137   99.979128   99.916597  98.817967   \n",
       "1    100         64     10     0.374013  100.000000  100.000000  99.716312   \n",
       "2    100         64     50     0.408951  100.000000  100.000000  99.858156   \n",
       "3    100         64     89     0.422724  100.000000  100.000000  99.479905   \n",
       "4    250         64      1     0.416720   99.600673   99.496222  46.335697   \n",
       "5    250         64     25     0.389752  100.000000  100.000000  97.541371   \n",
       "6    250         64    125     0.512834  100.000000  100.000000  99.763593   \n",
       "7    250         64    236     0.399923   99.978983  100.000000  99.763593   \n",
       "8    500         64      1     0.454007   99.831508   99.747475  46.335697   \n",
       "9    500         64     50     0.412547  100.000000   99.915825  95.366430   \n",
       "10   500         64    250     0.547272  100.000000  100.000000  99.338061   \n",
       "11   500         64    473     0.512915   99.894693  100.000000  99.763593   \n",
       "12   750         64      1     0.438797   99.873524   99.831508  46.335697   \n",
       "13   750         64     75     0.407191  100.000000   99.915754  76.548463   \n",
       "14   750         64    375     0.373476  100.000000  100.000000  97.494090   \n",
       "15   750         64    789     0.362313  100.000000   99.915754  92.482270   \n",
       "16  1000         64      1     0.385908   99.894559   99.831508  46.335697   \n",
       "17  1000         64    100     0.360009  100.000000   99.915754  81.323877   \n",
       "18  1000         64    500     0.417010  100.000000  100.000000  98.014184   \n",
       "19  1000         64   1184     0.453959  100.000000  100.000000  91.489362   \n",
       "20  1250         64      1     0.397826   99.957824   99.915683  46.335697   \n",
       "21  1250         64    125     0.391967  100.000000  100.000000  88.794326   \n",
       "22  1250         64    625     0.420383  100.000000  100.000000  99.290780   \n",
       "23  1250         64   1184     0.402556  100.000000  100.000000  99.716312   \n",
       "24  1500         64      1     0.436392   99.978907   99.915683  46.335697   \n",
       "25  1500         64    150     0.368916  100.000000  100.000000  75.839243   \n",
       "26  1500         64    750     0.334540  100.000000  100.000000  88.605201   \n",
       "27  1500         64   1579     0.341033   99.978907  100.000000  91.773050   \n",
       "28  1750         64      1     0.428044   99.957806   99.915683  46.335697   \n",
       "29  1750         64    175     0.398755  100.000000  100.000000  81.087470   \n",
       "30  1750         64    875     0.421275  100.000000  100.000000  96.690307   \n",
       "31  1750         64   2369     0.388738  100.000000  100.000000  96.643026   \n",
       "32  2000         64      1     0.421667   99.915612   99.915683  46.335697   \n",
       "33  2000         64    200     0.374887  100.000000   99.915683  59.952719   \n",
       "34  2000         64   1000     0.349644  100.000000   99.915683  85.248227   \n",
       "35  2000         64   2369     0.370446  100.000000   99.915683  93.853428   \n",
       "\n",
       "      F1-Val   F1-Test  \n",
       "0   0.962963  0.988864  \n",
       "1   1.000000  0.997350  \n",
       "2   1.000000  0.998679  \n",
       "3   1.000000  0.995131  \n",
       "4   0.000000  0.000000  \n",
       "5   1.000000  0.976555  \n",
       "6   1.000000  0.997792  \n",
       "7   1.000000  0.997792  \n",
       "8   0.000000  0.000000  \n",
       "9   0.800000  0.954880  \n",
       "10  1.000000  0.993794  \n",
       "11  1.000000  0.997794  \n",
       "12  0.000000  0.000000  \n",
       "13  0.666667  0.720406  \n",
       "14  1.000000  0.976094  \n",
       "15  0.666667  0.924680  \n",
       "16  0.000000  0.000000  \n",
       "17  0.666667  0.789333  \n",
       "18  1.000000  0.981166  \n",
       "19  1.000000  0.913876  \n",
       "20  0.000000  0.000000  \n",
       "21  1.000000  0.883424  \n",
       "22  1.000000  0.993348  \n",
       "23  1.000000  0.997352  \n",
       "24  0.000000  0.000000  \n",
       "25  1.000000  0.709494  \n",
       "26  1.000000  0.881222  \n",
       "27  1.000000  0.916985  \n",
       "28  0.000000  0.000000  \n",
       "29  1.000000  0.786096  \n",
       "30  1.000000  0.968182  \n",
       "31  1.000000  0.967713  \n",
       "32  0.000000  0.000000  \n",
       "33  0.000000  0.404779  \n",
       "34  0.000000  0.840654  \n",
       "35  0.000000  0.939252  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question3_df_resample.to_csv(f'q3_hyperopt_resampling_unsparsed_test.csv')\n",
    "display(question3_df_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4abf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
