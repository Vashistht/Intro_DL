{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c574ceac-8715-4368-9f04-1342c814e8fd",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch\n",
    "\n",
    "# Problem 1: Simple MLP for Binary Classification\n",
    "In this problem, you will train a simple MLP to classify two handwritten digits: 0 vs 1. We provide some starter codes to do this task with steps. However, you do not need to follow the exact steps as long as you can complete the task in sections marked as <span style=\"color:red\">[YOUR TASK]</span>.\n",
    "\n",
    "## Dataset Setup\n",
    "We will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The `torchvision` package has supported this dataset. We can load the dataset in this way (the dataset will take up 63M of your disk space):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7de796-0049-4efe-9ea0-ce64a5e24c85",
   "metadata": {},
   "source": [
    "# Problem 3: Handling Class Imbalance in MNIST Dataset\n",
    "In this problem, we will explore how to handle class imbalance problems, which are very common in real-world applications. A modified MNIST dataset is created as follows: we choose all instances of digit “0”, and choose only 1\\% instances of digit “1” for both training and test sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22857293-024a-4252-a3fb-2cb72cafe0e0",
   "metadata": {},
   "source": [
    "For such a class imbalance problem, accuracy may not be a good metric. Always predicting \"0\" regardless of the input can be 99\\% accurate. Instead, we use the $F_1$ score as the evaluation metric:\n",
    "$$F_1 = 2\\cdot\\frac{\\text{precision}\\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "where precision and recall are defined as:\n",
    "$$\\text{precision}=\\frac{\\text{number of instances correctly predicted as \"1\"}}{\\text{number of instances predicted as \"1\"}}$$\n",
    "$$\\text{recall}=\\frac{\\text{number of instances correctly predicted as \"1\"}}{\\text{number of instances labeled as \"1\"}}$$\n",
    "\n",
    "To handle such a problem, some changes to the training may be necessary. Some suggestions include: \n",
    "1) Adjusting the class weights in the loss function, i.e., use a larger weight for the minority class when computing the loss.\n",
    "2) Implementing resampling techniques (either undersampling the majority class or oversampling the minority class).\n",
    "\n",
    "<span style=\"color:red\">[YOUR TASK]</span>\n",
    "- Create the imbalance datasets with all \"0\" digits and only 1\\% \"1\" digits.\n",
    "- Implement the training loop and evaluation section (implementing the $F_1$ metric). \n",
    "- Ignore the class imbalance problem and train the MLP. Report your hyper-parameter details and the $F_1$ score performance on the test set (as the baseline).\n",
    "- Explore modifications to improve the performance of the class imbalance problem. Report your modifications and the $F_1$ scores performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872aca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_94026/2989715331.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ec73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af33ec5-782b-4a0b-819b-f649500627c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data pre-processing\n",
    "# convert the input to the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "    )\n",
    "\n",
    "# Load the MNIST dataset \n",
    "# this command requires Internet to download the dataset\n",
    "mnist = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transform)\n",
    "mnist_test = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data',   # './data'\n",
    "                            train=False, \n",
    "                            download=True, \n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14f718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies:  tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "print(\"Frequencies: \", torch.bincount(mnist.targets))\n",
    "print(len(torch.bincount(mnist.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6f033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=4, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define your MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Your code goes here\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Your code goes here\n",
    "hidden_dim = 4\n",
    "model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf6df17-48b9-4617-b12d-1d37865a5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def precision_score(labels, predictions):\n",
    "    predictions, labels = np.array(labels), np.array(predictions)\n",
    "    predictions_1 = np.sum(predictions==1)\n",
    "    correct_1 = np.sum( (predictions==1) & (labels==1))\n",
    "    precision = correct_1/ predictions_1 if predictions_1 > 0 else 1e-6\n",
    "    return precision\n",
    "\n",
    "def recall_score(labels, predictions):\n",
    "    predictions, labels = np.array(labels), np.array(predictions)\n",
    "    correct_1 = np.sum( (predictions==1) & (labels==1))\n",
    "    labels_1 = np.sum(labels==1)\n",
    "    recall = correct_1/ labels_1 if labels_1 > 0 else 1e-6\n",
    "    return recall\n",
    "\n",
    "def f1_score(labels, predictions):\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = (2 * (recall * precision)) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fdd534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_digit(weight, batch_size=64):\n",
    "    model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "        # print(f'Training accuracy: {train_acc:.2f}%')\n",
    "\n",
    "    training_time = time.time()- start_time\n",
    "    # print(training_time)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    val_preds = []; val_labels=[]\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        val_preds.append(pred)\n",
    "        val_labels.append(target)\n",
    "        # print(type(target))\n",
    "\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "    assert len(val_preds) == len(val_set)\n",
    "    \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_validation = f1_score(labels = val_labels, predictions = val_preds)\n",
    "    # print(f'F1 score validation: {f1_validation:.2f}')\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    test_preds = []; test_labels=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            test_preds.append(pred)\n",
    "            test_labels.append(target)\n",
    "        \n",
    "    test_preds = torch.cat(test_preds).numpy()\n",
    "    test_labels = torch.cat(test_labels).numpy()\n",
    "    assert len(test_preds) == len(test_set)   \n",
    "    test_acc = 100. * correct / total\n",
    "    # print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_test = f1_score(labels = test_labels, predictions =test_preds)\n",
    "    # print(f'F1 score test: {f1_test:.2f}')\n",
    "\n",
    "    \n",
    "    return training_time, train_acc, val_acc, test_acc, f1_validation, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae61895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for digits 0 and 1\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45cc01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set (before sparsing) 5923 6742 12665\n"
     ]
    }
   ],
   "source": [
    "train_0_original = [data for data in mnist if data[1] == 0]\n",
    "train_1_original = [data for data in mnist if data[1] == 1]\n",
    "print('Train set (before sparsing)', len(train_0_original), len(train_1_original), len(train_1_original) + len( train_0_original) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd07a3-e41b-4650-b4ae-ed4e2ff6c99d",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[EXTRA BONUS]</span>\n",
    "\n",
    "If the hyper-parameters are chosen properly, the baseline can perform satisfactorily on the class imbalance problem with 1% digit \"1\". We want to challenge the baseline and handle more class-imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0b82a-def4-45d0-9350-d79f00bfb671",
   "metadata": {},
   "source": [
    "Can you propose new ways for the class imbalance problem and achieve stable and satisfactory performance for large $N = 500, \\; 1000, \\; \\cdots$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af2babd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>F1-Val</th>\n",
       "      <th>F1-Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [N, Batch size, Weight, Train Time , Train Acc,  Val Acc, Test Acc, F1-Val, F1-Test]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = ['N', 'Batch size', 'Weight', 'Train Time ', 'Train Acc' ,' Val Acc', 'Test Acc', 'F1-Val', 'F1-Test']\n",
    "question3_df =  pd.DataFrame(columns = headers)\n",
    "question3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c3f831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Train set (before sparsing) 5923 67 5990\n",
      "100 Test set (before sparsing) 980 1135 2115\n",
      "100 Test set (after sparsing) 980 11 991\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_94026/4246043833.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  question3_df = pd.concat([question3_df, pd.DataFrame([row], columns=headers)], ignore_index=True)\n",
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_94026/4246043833.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weight = torch.tensor(weight, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 Train set (before sparsing) 5923 26 5949\n",
      "250 Test set (before sparsing) 980 1135 2115\n",
      "250 Test set (after sparsing) 980 4 984\n",
      "\n",
      "\n",
      "500 Train set (before sparsing) 5923 13 5936\n",
      "500 Test set (before sparsing) 980 1135 2115\n",
      "500 Test set (after sparsing) 980 2 982\n",
      "\n",
      "\n",
      "750 Train set (before sparsing) 5923 8 5931\n",
      "750 Test set (before sparsing) 980 1135 2115\n",
      "750 Test set (after sparsing) 980 1 981\n",
      "\n",
      "\n",
      "1000 Train set (before sparsing) 5923 6 5929\n",
      "1000 Test set (before sparsing) 980 1135 2115\n",
      "1000 Test set (after sparsing) 980 1 981\n",
      "\n",
      "\n",
      "1250 Train set (before sparsing) 5923 5 5928\n",
      "1250 Test set (before sparsing) 980 1135 2115\n",
      "1250 Test set (after sparsing) 980 0 980\n",
      "\n",
      "\n",
      "1500 Train set (before sparsing) 5923 4 5927\n",
      "1500 Test set (before sparsing) 980 1135 2115\n",
      "1500 Test set (after sparsing) 980 0 980\n",
      "\n",
      "\n",
      "1750 Train set (before sparsing) 5923 3 5926\n",
      "1750 Test set (before sparsing) 980 1135 2115\n",
      "1750 Test set (after sparsing) 980 0 980\n",
      "\n",
      "\n",
      "2000 Train set (before sparsing) 5923 3 5926\n",
      "2000 Test set (before sparsing) 980 1135 2115\n",
      "2000 Test set (after sparsing) 980 0 980\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_list = [100] + [250*(i+1) for i in range(8)]\n",
    "for N in N_list:\n",
    "    train_0 = train_0_original.copy()\n",
    "    train_1 =  train_1_original.copy()\n",
    "    random.shuffle(train_1)\n",
    "    train_1 = train_1[:len(train_1) // N]\n",
    "    print(N, 'Train set (before sparsing)', len(train_0), len(train_1), len(train_1) + len( train_0) )# train_set = train_0 + train_1\n",
    "\n",
    "    # Split training data (1s)into training and validation sets\n",
    "    train_1len = int(len(train_1) *.8)\n",
    "    val_1len = len(train_1) - train_1len\n",
    "    train1_set, val1_set = random_split(train_1, [train_1len, val_1len])\n",
    "\n",
    "    # Split training data (0s) into training and validation sets\n",
    "    train_0len = int(len(train_0) *.8)\n",
    "    val_0len = len(train_0) - train_0len\n",
    "    train0_set, val0_set = random_split(train_0, [train_0len, val_0len])\n",
    "    \n",
    "    train_set = train0_set + train1_set\n",
    "    val_set = val0_set + val1_set\n",
    "    len(train_set), len(val_set)\n",
    "\n",
    "    # creating test set\n",
    "    test_0 = [data for data in mnist_test if data[1] == 0]\n",
    "    test_1 = [data for data in mnist_test if data[1] == 1]\n",
    "    print(N,'Test set (before sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "\n",
    "    test_1 = test_1[:len(test_1) // N]\n",
    "    print(N,'Test set (after sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "    test_set = test_0 + test_1\n",
    "    print('\\n')\n",
    "    # Define DataLoaders to access data in batches\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size = 64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size = 64, shuffle=False)\n",
    "\n",
    "    # compensation = torch.tensor([1, N], dtype=torch.float32)\n",
    "    compensation = torch.tensor([1, (train_0len/ train_1len )], dtype=torch.float32)\n",
    "    weights = [[1,1], [1, int(N/10)], [1, int(N/2)], compensation]\n",
    "    batch_size = 64\n",
    "    results = []\n",
    "\n",
    "    # for batch_size in batch_sizes:\n",
    "    for weight in weights:\n",
    "        reweight_factor = weight[1]/ weight[0]\n",
    "        reweight_factor = float(reweight_factor)\n",
    "        weight = torch.tensor(weight, dtype=torch.float32)\n",
    "        weight = weight.to(device)\n",
    "        training_time, train_acc, val_acc, test_acc, f1_validation, f1_test = two_digit(batch_size=batch_size, weight = weight)\n",
    "        \n",
    "        row = [N, batch_size, reweight_factor, training_time, train_acc, val_acc, test_acc, f1_validation, f1_test]\n",
    "        question3_df = pd.concat([question3_df, pd.DataFrame([row], columns=headers)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5027811a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>F1-Val</th>\n",
       "      <th>F1-Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441928</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.444450</td>\n",
       "      <td>99.979128</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.425864</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.899092</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>89.396225</td>\n",
       "      <td>0.429034</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421473</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.916037</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.408722</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.832074</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>0.438926</td>\n",
       "      <td>99.978983</td>\n",
       "      <td>99.916037</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>236.899994</td>\n",
       "      <td>0.412361</td>\n",
       "      <td>99.831862</td>\n",
       "      <td>99.832074</td>\n",
       "      <td>99.796748</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.411593</td>\n",
       "      <td>99.957877</td>\n",
       "      <td>99.831650</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.403737</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>0.469201</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.831650</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>473.799988</td>\n",
       "      <td>0.495480</td>\n",
       "      <td>99.957877</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460369</td>\n",
       "      <td>99.957841</td>\n",
       "      <td>99.831508</td>\n",
       "      <td>99.898063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.411340</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>0.432767</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>789.666687</td>\n",
       "      <td>0.446825</td>\n",
       "      <td>99.873524</td>\n",
       "      <td>99.831508</td>\n",
       "      <td>99.898063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440238</td>\n",
       "      <td>99.915647</td>\n",
       "      <td>99.831508</td>\n",
       "      <td>99.898063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.409171</td>\n",
       "      <td>99.978912</td>\n",
       "      <td>99.915754</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0.436480</td>\n",
       "      <td>99.810207</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.898063</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>1184.500000</td>\n",
       "      <td>0.456940</td>\n",
       "      <td>99.599325</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.486057</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>0.412337</td>\n",
       "      <td>99.936736</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>0.428437</td>\n",
       "      <td>99.768030</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>1184.500000</td>\n",
       "      <td>0.473218</td>\n",
       "      <td>99.852383</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.897959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.402057</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>0.445018</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>1579.333374</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>99.978907</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.456237</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>0.409596</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>875.000000</td>\n",
       "      <td>0.445411</td>\n",
       "      <td>99.978903</td>\n",
       "      <td>99.831366</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>2369.000000</td>\n",
       "      <td>0.448272</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465735</td>\n",
       "      <td>99.957806</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.443340</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.429006</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>2369.000000</td>\n",
       "      <td>0.510838</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       N Batch size       Weight  Train Time    Train Acc     Val Acc  \\\n",
       "0    100         64     1.000000     0.441928  100.000000  100.000000   \n",
       "1    100         64    10.000000     0.444450   99.979128  100.000000   \n",
       "2    100         64    50.000000     0.425864  100.000000  100.000000   \n",
       "3    100         64    89.396225     0.429034  100.000000  100.000000   \n",
       "4    250         64     1.000000     0.421473  100.000000   99.916037   \n",
       "5    250         64    25.000000     0.408722  100.000000   99.832074   \n",
       "6    250         64   125.000000     0.438926   99.978983   99.916037   \n",
       "7    250         64   236.899994     0.412361   99.831862   99.832074   \n",
       "8    500         64     1.000000     0.411593   99.957877   99.831650   \n",
       "9    500         64    50.000000     0.403737  100.000000  100.000000   \n",
       "10   500         64   250.000000     0.469201  100.000000   99.831650   \n",
       "11   500         64   473.799988     0.495480   99.957877  100.000000   \n",
       "12   750         64     1.000000     0.460369   99.957841   99.831508   \n",
       "13   750         64    75.000000     0.411340  100.000000  100.000000   \n",
       "14   750         64   375.000000     0.432767   99.915683  100.000000   \n",
       "15   750         64   789.666687     0.446825   99.873524   99.831508   \n",
       "16  1000         64     1.000000     0.440238   99.915647   99.831508   \n",
       "17  1000         64   100.000000     0.409171   99.978912   99.915754   \n",
       "18  1000         64   500.000000     0.436480   99.810207  100.000000   \n",
       "19  1000         64  1184.500000     0.456940   99.599325  100.000000   \n",
       "20  1250         64     1.000000     0.486057  100.000000   99.915683   \n",
       "21  1250         64   125.000000     0.412337   99.936736  100.000000   \n",
       "22  1250         64   625.000000     0.428437   99.768030  100.000000   \n",
       "23  1250         64  1184.500000     0.473218   99.852383  100.000000   \n",
       "24  1500         64     1.000000     0.413333  100.000000   99.915683   \n",
       "25  1500         64   150.000000     0.402057  100.000000   99.915683   \n",
       "26  1500         64   750.000000     0.445018  100.000000   99.915683   \n",
       "27  1500         64  1579.333374     0.461400   99.978907   99.915683   \n",
       "28  1750         64     1.000000     0.456237  100.000000  100.000000   \n",
       "29  1750         64   175.000000     0.409596  100.000000  100.000000   \n",
       "30  1750         64   875.000000     0.445411   99.978903   99.831366   \n",
       "31  1750         64  2369.000000     0.448272  100.000000  100.000000   \n",
       "32  2000         64     1.000000     0.465735   99.957806   99.915683   \n",
       "33  2000         64   200.000000     0.443340  100.000000  100.000000   \n",
       "34  2000         64  1000.000000     0.429006  100.000000  100.000000   \n",
       "35  2000         64  2369.000000     0.510838  100.000000  100.000000   \n",
       "\n",
       "      Test Acc    F1-Val   F1-Test  \n",
       "0   100.000000  1.000000  1.000000  \n",
       "1   100.000000  1.000000  1.000000  \n",
       "2    99.899092  1.000000  0.956522  \n",
       "3   100.000000  1.000000  1.000000  \n",
       "4   100.000000  0.909091  1.000000  \n",
       "5   100.000000  0.800000  1.000000  \n",
       "6   100.000000  0.909091  1.000000  \n",
       "7    99.796748  0.833333  0.800000  \n",
       "8   100.000000  0.500000  1.000000  \n",
       "9   100.000000  1.000000  1.000000  \n",
       "10  100.000000  0.500000  1.000000  \n",
       "11  100.000000  1.000000  1.000000  \n",
       "12   99.898063  0.000000  0.000000  \n",
       "13  100.000000  1.000000  1.000000  \n",
       "14  100.000000  1.000000  1.000000  \n",
       "15   99.898063  0.000000  0.000000  \n",
       "16   99.898063  0.000000  0.000000  \n",
       "17  100.000000  0.666667  1.000000  \n",
       "18   99.898063  1.000000  0.666667  \n",
       "19  100.000000  1.000000  1.000000  \n",
       "20  100.000000  0.000000  0.000001  \n",
       "21  100.000000  1.000000  0.000001  \n",
       "22  100.000000  1.000000  0.000001  \n",
       "23   99.897959  1.000000  0.000000  \n",
       "24  100.000000  0.000000  0.000001  \n",
       "25  100.000000  0.000000  0.000001  \n",
       "26  100.000000  0.000000  0.000001  \n",
       "27  100.000000  0.000000  0.000001  \n",
       "28  100.000000  1.000000  0.000001  \n",
       "29  100.000000  1.000000  0.000001  \n",
       "30  100.000000  0.500000  0.000001  \n",
       "31  100.000000  1.000000  0.000001  \n",
       "32  100.000000  0.000000  0.000001  \n",
       "33  100.000000  1.000000  0.000001  \n",
       "34  100.000000  1.000000  0.000001  \n",
       "35  100.000000  1.000000  0.000001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question3_df.to_csv(f'q3_hyperopt_weight_unsparsted_test.csv')\n",
    "display(question3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb7f41-2e28-4935-a9cd-124e4963fa3d",
   "metadata": {},
   "source": [
    "# Weighted Resampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b00fcc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>F1-Val</th>\n",
       "      <th>F1-Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [N, Batch Size, Weight, Train Time , Train Acc,  Val Acc, Test Acc, F1-Val, F1-Test]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def convert_tensordata(dataset):\n",
    "#     dataset_data = [data[0] for data in dataset]\n",
    "#     dataset_labels = [data[1] for data in dataset]\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "headers = ['N', 'Batch Size', 'Weight', 'Train Time ', 'Train Acc' ,' Val Acc', 'Test Acc', 'F1-Val', 'F1-Test']\n",
    "question3_df_resample =  pd.DataFrame(columns = headers)\n",
    "question3_df_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab92cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_digit_resampling():\n",
    "    model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "    batch_size = 64\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "        # print(f'Training accuracy: {train_acc:.2f}%')\n",
    "\n",
    "    training_time = time.time()- start_time\n",
    "    # print(training_time)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    val_preds = []; val_labels=[]\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        val_preds.append(pred)\n",
    "        val_labels.append(target)\n",
    "        # print(type(target))\n",
    "\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "    assert len(val_preds) == len(val_set)\n",
    "    \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_validation = f1_score(labels = val_labels, predictions = val_preds)\n",
    "    # print(f'F1 score validation: {f1_validation:.2f}')\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    test_preds = []; test_labels=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            test_preds.append(pred)\n",
    "            test_labels.append(target)\n",
    "        \n",
    "    test_preds = torch.cat(test_preds).numpy()\n",
    "    test_labels = torch.cat(test_labels).numpy()\n",
    "    assert len(test_preds) == len(test_set)   \n",
    "    test_acc = 100. * correct / total\n",
    "    # print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_test = f1_score(labels = test_labels, predictions =test_preds)\n",
    "    # print(f'F1 score test: {f1_test:.2f}')\n",
    "\n",
    "    return training_time, train_acc, val_acc, test_acc, f1_validation, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4e3a88e-e3b2-4c82-bf42-ab789c452b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Train set (before sparsing) 5923 67 5990\n",
      "100 Test set (before sparsing) 980 1135 2115\n",
      "100 Test set (after sparsing) 980 11 991\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_25852/2442262052.py:56: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  question3_df_resample = pd.concat([question3_df_resample, pd.DataFrame([row], columns=headers)], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 Train set (before sparsing) 5923 26 5949\n",
      "250 Test set (before sparsing) 980 1135 2115\n",
      "250 Test set (after sparsing) 980 4 984\n",
      "\n",
      "\n",
      "500 Train set (before sparsing) 5923 13 5936\n",
      "500 Test set (before sparsing) 980 1135 2115\n",
      "500 Test set (after sparsing) 980 2 982\n",
      "\n",
      "\n",
      "750 Train set (before sparsing) 5923 8 5931\n",
      "750 Test set (before sparsing) 980 1135 2115\n",
      "750 Test set (after sparsing) 980 1 981\n",
      "\n",
      "\n",
      "1000 Train set (before sparsing) 5923 6 5929\n",
      "1000 Test set (before sparsing) 980 1135 2115\n",
      "1000 Test set (after sparsing) 980 1 981\n",
      "\n",
      "\n",
      "1250 Train set (before sparsing) 5923 5 5928\n",
      "1250 Test set (before sparsing) 980 1135 2115\n",
      "1250 Test set (after sparsing) 980 0 980\n",
      "\n",
      "\n",
      "1500 Train set (before sparsing) 5923 4 5927\n",
      "1500 Test set (before sparsing) 980 1135 2115\n",
      "1500 Test set (after sparsing) 980 0 980\n",
      "\n",
      "\n",
      "1750 Train set (before sparsing) 5923 3 5926\n",
      "1750 Test set (before sparsing) 980 1135 2115\n",
      "1750 Test set (after sparsing) 980 0 980\n",
      "\n",
      "\n",
      "2000 Train set (before sparsing) 5923 3 5926\n",
      "2000 Test set (before sparsing) 980 1135 2115\n",
      "2000 Test set (after sparsing) 980 0 980\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_list = [100] + [250*(i+1) for i in range(8)]\n",
    "for N in N_list:\n",
    "    train_0 = train_0_original.copy()\n",
    "    train_1 =  train_1_original.copy()\n",
    "    random.shuffle(train_1)\n",
    "    train_1 = train_1[:len(train_1) // N]\n",
    "    print(N, 'Train set (before sparsing)', len(train_0), len(train_1), len(train_1) + len( train_0) )# train_set = train_0 + train_1\n",
    "\n",
    "    # Split training data (1s)into training and validation sets\n",
    "    train_1len = int(len(train_1) *.8)\n",
    "    val_1len = len(train_1) - train_1len\n",
    "    train1_set, val1_set = train_1[:train_1len], train_1[train_1len:]\n",
    "\n",
    "    # Split training data (0s) into training and validation sets\n",
    "    train_0len = int(len(train_0) *.8)\n",
    "    val_0len = len(train_0) - train_0len\n",
    "    train0_set, val0_set = train_0[:train_0len], train_0[train_0len:]\n",
    "    \n",
    "    # train and val set\n",
    "    train_set = train0_set + train1_set\n",
    "    val_set = val0_set + val1_set\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(val_set)\n",
    "    len(train_set), len(val_set)\n",
    "\n",
    "    # creating test set\n",
    "    test_0 = [data for data in mnist_test if data[1] == 0]\n",
    "    test_1 = [data for data in mnist_test if data[1] == 1]\n",
    "    print(N,'Test set (before sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "\n",
    "    test_1 = test_1[:len(test_1) // N]\n",
    "    print(N,'Test set (after sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "    test_set = test_0 + test_1\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    print('\\n')\n",
    "    # compensation = torch.tensor([1, N], dtype=torch.float32)\n",
    "    compensation = int(train_0len/ train_1len)\n",
    "    weight_factors = [1, int(N/10), int(N/2), compensation]\n",
    "    batch_size = 64\n",
    "    results = []\n",
    "\n",
    "    # for batch_size in batch_sizes:\n",
    "    for weight_factor in weight_factors:\n",
    "        \n",
    "        weights = np.array( [1.0 if data[1] == 0 else weight_factor for data in train_set])\n",
    "        weights = torch.from_numpy(weights)\n",
    "        \n",
    "        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "        \n",
    "        train_loader = DataLoader(train_set, batch_size=64, sampler=sampler)\n",
    "        val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "        \n",
    "        training_time, train_acc, val_acc, test_acc, f1_validation, f1_test = two_digit_resampling()\n",
    "        \n",
    "        row = [N, batch_size, weight_factor, training_time, train_acc, val_acc, test_acc, f1_validation, f1_test]\n",
    "        question3_df_resample = pd.concat([question3_df_resample, pd.DataFrame([row], columns=headers)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b60a9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>F1-Val</th>\n",
       "      <th>F1-Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.413916</td>\n",
       "      <td>99.853893</td>\n",
       "      <td>99.833194</td>\n",
       "      <td>99.798184</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.453152</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.916597</td>\n",
       "      <td>99.899092</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>0.399540</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>89</td>\n",
       "      <td>0.443699</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.899092</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.402522</td>\n",
       "      <td>99.936948</td>\n",
       "      <td>99.916037</td>\n",
       "      <td>99.796748</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>25</td>\n",
       "      <td>0.469228</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>125</td>\n",
       "      <td>0.377950</td>\n",
       "      <td>99.957966</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.796748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>250</td>\n",
       "      <td>64</td>\n",
       "      <td>236</td>\n",
       "      <td>0.404280</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.471906</td>\n",
       "      <td>99.957877</td>\n",
       "      <td>99.915825</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>0.491204</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>250</td>\n",
       "      <td>0.412180</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>500</td>\n",
       "      <td>64</td>\n",
       "      <td>473</td>\n",
       "      <td>0.554037</td>\n",
       "      <td>99.978939</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.898167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524819</td>\n",
       "      <td>99.873524</td>\n",
       "      <td>99.831508</td>\n",
       "      <td>99.898063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>75</td>\n",
       "      <td>0.536037</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>375</td>\n",
       "      <td>0.478645</td>\n",
       "      <td>99.978921</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>750</td>\n",
       "      <td>64</td>\n",
       "      <td>789</td>\n",
       "      <td>0.361053</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.559330</td>\n",
       "      <td>99.810207</td>\n",
       "      <td>99.831508</td>\n",
       "      <td>99.898063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>0.439239</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>500</td>\n",
       "      <td>0.421760</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1000</td>\n",
       "      <td>64</td>\n",
       "      <td>1184</td>\n",
       "      <td>0.386407</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.481794</td>\n",
       "      <td>99.957824</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>125</td>\n",
       "      <td>0.373254</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>625</td>\n",
       "      <td>0.383272</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1250</td>\n",
       "      <td>64</td>\n",
       "      <td>1184</td>\n",
       "      <td>0.401946</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410053</td>\n",
       "      <td>99.873444</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>150</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>750</td>\n",
       "      <td>0.426304</td>\n",
       "      <td>99.978907</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.897959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1500</td>\n",
       "      <td>64</td>\n",
       "      <td>1579</td>\n",
       "      <td>0.444462</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.421583</td>\n",
       "      <td>99.957806</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>175</td>\n",
       "      <td>0.579675</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>875</td>\n",
       "      <td>0.450617</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1750</td>\n",
       "      <td>64</td>\n",
       "      <td>2369</td>\n",
       "      <td>0.410894</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.419797</td>\n",
       "      <td>99.978903</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>200</td>\n",
       "      <td>0.513345</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.915683</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.508458</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.897959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2000</td>\n",
       "      <td>64</td>\n",
       "      <td>2369</td>\n",
       "      <td>0.455658</td>\n",
       "      <td>99.978903</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       N Batch Size Weight  Train Time    Train Acc     Val Acc    Test Acc  \\\n",
       "0    100         64      1     0.413916   99.853893   99.833194   99.798184   \n",
       "1    100         64     10     0.453152  100.000000   99.916597   99.899092   \n",
       "2    100         64     50     0.399540  100.000000  100.000000  100.000000   \n",
       "3    100         64     89     0.443699  100.000000  100.000000   99.899092   \n",
       "4    250         64      1     0.402522   99.936948   99.916037   99.796748   \n",
       "5    250         64     25     0.469228  100.000000  100.000000  100.000000   \n",
       "6    250         64    125     0.377950   99.957966  100.000000   99.796748   \n",
       "7    250         64    236     0.404280  100.000000  100.000000  100.000000   \n",
       "8    500         64      1     0.471906   99.957877   99.915825  100.000000   \n",
       "9    500         64     50     0.491204  100.000000  100.000000  100.000000   \n",
       "10   500         64    250     0.412180  100.000000  100.000000  100.000000   \n",
       "11   500         64    473     0.554037   99.978939  100.000000   99.898167   \n",
       "12   750         64      1     0.524819   99.873524   99.831508   99.898063   \n",
       "13   750         64     75     0.536037  100.000000  100.000000  100.000000   \n",
       "14   750         64    375     0.478645   99.978921  100.000000  100.000000   \n",
       "15   750         64    789     0.361053  100.000000  100.000000  100.000000   \n",
       "16  1000         64      1     0.559330   99.810207   99.831508   99.898063   \n",
       "17  1000         64    100     0.439239  100.000000  100.000000  100.000000   \n",
       "18  1000         64    500     0.421760  100.000000  100.000000  100.000000   \n",
       "19  1000         64   1184     0.386407  100.000000  100.000000  100.000000   \n",
       "20  1250         64      1     0.481794   99.957824   99.915683  100.000000   \n",
       "21  1250         64    125     0.373254  100.000000   99.915683  100.000000   \n",
       "22  1250         64    625     0.383272  100.000000   99.915683  100.000000   \n",
       "23  1250         64   1184     0.401946  100.000000   99.915683  100.000000   \n",
       "24  1500         64      1     0.410053   99.873444   99.915683  100.000000   \n",
       "25  1500         64    150     0.395911  100.000000  100.000000  100.000000   \n",
       "26  1500         64    750     0.426304   99.978907  100.000000   99.897959   \n",
       "27  1500         64   1579     0.444462  100.000000  100.000000  100.000000   \n",
       "28  1750         64      1     0.421583   99.957806   99.915683  100.000000   \n",
       "29  1750         64    175     0.579675  100.000000  100.000000  100.000000   \n",
       "30  1750         64    875     0.450617  100.000000  100.000000  100.000000   \n",
       "31  1750         64   2369     0.410894  100.000000  100.000000  100.000000   \n",
       "32  2000         64      1     0.419797   99.978903   99.915683  100.000000   \n",
       "33  2000         64    200     0.513345  100.000000   99.915683  100.000000   \n",
       "34  2000         64   1000     0.508458  100.000000  100.000000   99.897959   \n",
       "35  2000         64   2369     0.455658   99.978903  100.000000  100.000000   \n",
       "\n",
       "      F1-Val   F1-Test  \n",
       "0   0.923077  0.900000  \n",
       "1   0.962963  0.952381  \n",
       "2   1.000000  1.000000  \n",
       "3   1.000000  0.956522  \n",
       "4   0.909091  0.666667  \n",
       "5   1.000000  1.000000  \n",
       "6   1.000000  0.800000  \n",
       "7   1.000000  1.000000  \n",
       "8   0.800000  1.000000  \n",
       "9   1.000000  1.000000  \n",
       "10  1.000000  1.000000  \n",
       "11  1.000000  0.800000  \n",
       "12  0.000000  0.000000  \n",
       "13  1.000000  1.000000  \n",
       "14  1.000000  1.000000  \n",
       "15  1.000000  1.000000  \n",
       "16  0.000000  0.000000  \n",
       "17  1.000000  1.000000  \n",
       "18  1.000000  1.000000  \n",
       "19  1.000000  1.000000  \n",
       "20  0.000000  0.000001  \n",
       "21  0.000000  0.000001  \n",
       "22  0.000000  0.000001  \n",
       "23  0.666667  0.000001  \n",
       "24  0.000000  0.000001  \n",
       "25  1.000000  0.000001  \n",
       "26  1.000000  0.000000  \n",
       "27  1.000000  0.000001  \n",
       "28  0.000000  0.000001  \n",
       "29  1.000000  0.000001  \n",
       "30  1.000000  0.000001  \n",
       "31  1.000000  0.000001  \n",
       "32  0.000000  0.000001  \n",
       "33  0.000000  0.000001  \n",
       "34  1.000000  0.000000  \n",
       "35  1.000000  0.000001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question3_df_resample.to_csv(f'q3_hyperopt_resampling_unsparsed_test.csv')\n",
    "display(question3_df_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4abf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
