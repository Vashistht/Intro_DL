{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c574ceac-8715-4368-9f04-1342c814e8fd",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch\n",
    "\n",
    "# Problem 1: Simple MLP for Binary Classification\n",
    "In this problem, you will train a simple MLP to classify two handwritten digits: 0 vs 1. We provide some starter codes to do this task with steps. However, you do not need to follow the exact steps as long as you can complete the task in sections marked as <span style=\"color:red\">[YOUR TASK]</span>.\n",
    "\n",
    "## Dataset Setup\n",
    "We will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The `torchvision` package has supported this dataset. We can load the dataset in this way (the dataset will take up 63M of your disk space):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "872aca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9801782f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('14.2.1', ('', '', ''), 'arm64')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_4518/3673749337.py:3: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform, time\n",
    "print(platform.mac_ver() )\n",
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94ec73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75b2483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not torch.backends.mps.is_available():\n",
    "#     if not torch.backends.mps.is_built():\n",
    "#         print(\"MPS not available because the current PyTorch install was not \"\n",
    "#               \"built with MPS enabled.\")\n",
    "#     else:\n",
    "#         print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "#               \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    \n",
    "# else:\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print('mps enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2af33ec5-782b-4a0b-819b-f649500627c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data pre-processing\n",
    "# convert the input to the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "    )\n",
    "\n",
    "# Load the MNIST dataset \n",
    "# this command requires Internet to download the dataset\n",
    "mnist = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transform)\n",
    "mnist_test = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data',   # './data'\n",
    "                            train=False, \n",
    "                            download=True, \n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a14f718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies:  tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vashisth/anaconda3/envs/deep_learning/lib/python3.11/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "print(\"Frequencies: \", torch.bincount(mnist.train_labels))\n",
    "print(len(torch.bincount(mnist.train_labels)))\n",
    "# Filter for digits 0 and 1\n",
    "# train_data = [data for data in mnist if data[1] < 2]\n",
    "# train_index = mnist.train_labels<2\n",
    "# mnist.data = mnist.data[train_index]\n",
    "# mnist.targets = mnist.targets[train_index]\n",
    "# # Your code goes here\n",
    "# mnist_test.data = mnist_test.data[test_index]\n",
    "# mnist_test.targets = mnist_test.targets[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ff472db-51ee-4835-8217-8557947f0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "# Your code goes here\n",
    "# train_set = ...\n",
    "# val_set = ...\n",
    "train_len = int(len(mnist) *.8)\n",
    "val_len = len(mnist) - train_len\n",
    "train_set, val_set = random_split(mnist, [train_len, val_len])\n",
    "\n",
    "# Define DataLoaders to access data in batches\n",
    "train_loader = DataLoader(train_set, batch_size=1028, shuffle=True)\n",
    "# Your code goes here\n",
    "val_loader = DataLoader(val_set, batch_size = 1028, shuffle=False)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 1028, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4cc4f021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7fc30-cef1-4cfc-bbde-4de36737ea88",
   "metadata": {},
   "source": [
    "# Problem 2: MNIST 10-class classification\n",
    "\n",
    "Now we want to train an MLP to handle multi-class classification for all 10 digits in the MNIST dataset. We will use the full MNIST dataset without filtering for specific digits. You may modify the MLP so that it can be used for multi-class classification.\n",
    "\n",
    "<span style=\"color:red\">[YOUR TASK]</span>\n",
    "- Implement the training loop and evaluation section. Report the hyper-parameters you choose.\n",
    "- Experiment with different numbers of neurons in the hidden layer and note any changes in performance.\n",
    "- Write a brief analysis of the model's performance, including any challenges faced and how they were addressed.\n",
    "\n",
    "In our implementations, we trained our network for 10 epochs in about 20 seconds on a laptop.\n",
    "When you define a new model, remember to update the optimizer!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f96f27ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MulticlassMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=88, bias=True)\n",
      "  (activation): Sigmoid()\n",
      "  (fc2): Linear(in_features=88, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MulticlassMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(MulticlassMLP, self).__init__()\n",
    "        # Your code goes here\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Your code goes here\n",
    "hidden_dim = int(np.sqrt(28*28*10))\n",
    "model = MulticlassMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f37802c-97df-490d-85c6-6a4d8fa69515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_digit(batch_size, hidden_dim, optimizer,  device = 'cpu'): # or mps lr=1e-3,\n",
    "    device = torch.device(device)\n",
    "    # Define DataLoaders to access data in batches\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # Your code goes here\n",
    "    val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    model = MulticlassMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=10).to(device)\n",
    "    # print(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        lr = 1e-3\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        lr=1e-2\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "        # print(f'Training accuracy: {train_acc:.2f}%')\n",
    "\n",
    "    training_time = time.time()- start_time\n",
    "    # print(training_time)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            \n",
    "    test_acc = 100. * correct / total\n",
    "    # print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    \n",
    "    return training_time, train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fff1ad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cpu', 64, 'adam', 0.001, 16, 29.785605907440186, 93.33333333333333, 92.89166666666667, 93.11]\n",
      "['cpu', 64, 'adam', 0.001, 32, 30.65951418876648, 95.59583333333333, 94.975, 95.12]\n",
      "['cpu', 64, 'adam', 0.001, 64, 30.598462104797363, 97.10208333333334, 95.88333333333334, 96.18]\n",
      "['mps', 64, 'adam', 0.001, 16, 70.20412874221802, 93.47916666666667, 93.10833333333333, 93.37]\n",
      "['mps', 64, 'adam', 0.001, 32, 64.60245871543884, 95.71458333333334, 94.96666666666667, 95.1]\n",
      "['mps', 64, 'adam', 0.001, 64, 68.29115414619446, 97.02083333333333, 95.975, 96.22]\n",
      "['cpu', 64, 'sgd', 0.01, 16, 28.56167697906494, 88.98125, 89.03333333333333, 89.35]\n",
      "['cpu', 64, 'sgd', 0.01, 32, 28.658032178878784, 90.04375, 90.06666666666666, 90.38]\n",
      "['cpu', 64, 'sgd', 0.01, 64, 29.286179065704346, 90.01875, 90.11666666666666, 90.72]\n",
      "['mps', 64, 'sgd', 0.01, 16, 54.447420835494995, 89.04166666666667, 89.4, 89.41]\n",
      "['mps', 64, 'sgd', 0.01, 32, 54.18169593811035, 89.82916666666667, 90.05, 90.57]\n",
      "['mps', 64, 'sgd', 0.01, 64, 53.712011098861694, 90.08541666666666, 90.14166666666667, 90.84]\n",
      "['cpu', 128, 'adam', 0.001, 16, 27.09119486808777, 92.86666666666666, 92.39166666666667, 92.28]\n",
      "['cpu', 128, 'adam', 0.001, 32, 27.34237790107727, 94.65833333333333, 93.95, 94.0]\n",
      "['cpu', 128, 'adam', 0.001, 64, 28.855186939239502, 96.47916666666667, 95.64166666666667, 95.99]\n",
      "['mps', 128, 'adam', 0.001, 16, 45.28487801551819, 92.74583333333334, 92.44166666666666, 92.43]\n",
      "['mps', 128, 'adam', 0.001, 32, 45.76998972892761, 94.88125, 94.44166666666666, 94.65]\n",
      "['mps', 128, 'adam', 0.001, 64, 45.41103911399841, 96.33541666666666, 95.73333333333333, 95.64]\n",
      "['cpu', 128, 'sgd', 0.01, 16, 26.004951238632202, 83.83958333333334, 83.76666666666667, 84.78]\n",
      "['cpu', 128, 'sgd', 0.01, 32, 26.085991859436035, 87.04375, 87.43333333333334, 88.13]\n",
      "['cpu', 128, 'sgd', 0.01, 64, 27.50149703025818, 87.92708333333333, 88.31666666666666, 88.81]\n",
      "['mps', 128, 'sgd', 0.01, 16, 40.28130888938904, 85.85625, 86.325, 87.06]\n",
      "['mps', 128, 'sgd', 0.01, 32, 40.95052623748779, 87.06458333333333, 87.425, 87.89]\n",
      "['mps', 128, 'sgd', 0.01, 64, 41.05940794944763, 87.60833333333333, 88.00833333333334, 88.34]\n",
      "['cpu', 1024, 'adam', 0.001, 16, 25.75090193748474, 88.90833333333333, 89.13333333333334, 89.5]\n",
      "['cpu', 1024, 'adam', 0.001, 32, 25.995569944381714, 90.67291666666667, 90.5, 91.03]\n",
      "['cpu', 1024, 'adam', 0.001, 64, 25.665560960769653, 92.19166666666666, 92.125, 92.48]\n",
      "['mps', 1024, 'adam', 0.001, 16, 28.267908811569214, 88.55625, 88.5, 89.16]\n",
      "['mps', 1024, 'adam', 0.001, 32, 27.744359016418457, 90.99583333333334, 90.9, 91.3]\n",
      "['mps', 1024, 'adam', 0.001, 64, 28.016613006591797, 92.34375, 92.25, 92.57]\n",
      "['cpu', 1024, 'sgd', 0.01, 16, 25.572664260864258, 58.08958333333333, 58.80833333333333, 60.01]\n",
      "['cpu', 1024, 'sgd', 0.01, 32, 25.616307973861694, 62.66875, 63.291666666666664, 64.48]\n",
      "['cpu', 1024, 'sgd', 0.01, 64, 25.645607233047485, 68.40625, 70.15833333333333, 70.63]\n",
      "['mps', 1024, 'sgd', 0.01, 16, 26.766538858413696, 63.36875, 64.5, 64.06]\n",
      "['mps', 1024, 'sgd', 0.01, 32, 26.71338200569153, 63.69583333333333, 64.85, 65.97]\n",
      "['mps', 1024, 'sgd', 0.01, 64, 26.389231204986572, 67.9875, 68.68333333333334, 69.5]\n",
      "   Device  Batch size Optimizer     LR  Hidden Dim  Training Time  Train Acc  \\\n",
      "0     cpu          64      adam  0.001          16      29.785606  93.333333   \n",
      "1     cpu          64      adam  0.001          32      30.659514  95.595833   \n",
      "2     cpu          64      adam  0.001          64      30.598462  97.102083   \n",
      "3     mps          64      adam  0.001          16      70.204129  93.479167   \n",
      "4     mps          64      adam  0.001          32      64.602459  95.714583   \n",
      "5     mps          64      adam  0.001          64      68.291154  97.020833   \n",
      "6     cpu          64       sgd  0.010          16      28.561677  88.981250   \n",
      "7     cpu          64       sgd  0.010          32      28.658032  90.043750   \n",
      "8     cpu          64       sgd  0.010          64      29.286179  90.018750   \n",
      "9     mps          64       sgd  0.010          16      54.447421  89.041667   \n",
      "10    mps          64       sgd  0.010          32      54.181696  89.829167   \n",
      "11    mps          64       sgd  0.010          64      53.712011  90.085417   \n",
      "12    cpu         128      adam  0.001          16      27.091195  92.866667   \n",
      "13    cpu         128      adam  0.001          32      27.342378  94.658333   \n",
      "14    cpu         128      adam  0.001          64      28.855187  96.479167   \n",
      "15    mps         128      adam  0.001          16      45.284878  92.745833   \n",
      "16    mps         128      adam  0.001          32      45.769990  94.881250   \n",
      "17    mps         128      adam  0.001          64      45.411039  96.335417   \n",
      "18    cpu         128       sgd  0.010          16      26.004951  83.839583   \n",
      "19    cpu         128       sgd  0.010          32      26.085992  87.043750   \n",
      "20    cpu         128       sgd  0.010          64      27.501497  87.927083   \n",
      "21    mps         128       sgd  0.010          16      40.281309  85.856250   \n",
      "22    mps         128       sgd  0.010          32      40.950526  87.064583   \n",
      "23    mps         128       sgd  0.010          64      41.059408  87.608333   \n",
      "24    cpu        1024      adam  0.001          16      25.750902  88.908333   \n",
      "25    cpu        1024      adam  0.001          32      25.995570  90.672917   \n",
      "26    cpu        1024      adam  0.001          64      25.665561  92.191667   \n",
      "27    mps        1024      adam  0.001          16      28.267909  88.556250   \n",
      "28    mps        1024      adam  0.001          32      27.744359  90.995833   \n",
      "29    mps        1024      adam  0.001          64      28.016613  92.343750   \n",
      "30    cpu        1024       sgd  0.010          16      25.572664  58.089583   \n",
      "31    cpu        1024       sgd  0.010          32      25.616308  62.668750   \n",
      "32    cpu        1024       sgd  0.010          64      25.645607  68.406250   \n",
      "33    mps        1024       sgd  0.010          16      26.766539  63.368750   \n",
      "34    mps        1024       sgd  0.010          32      26.713382  63.695833   \n",
      "35    mps        1024       sgd  0.010          64      26.389231  67.987500   \n",
      "\n",
      "      Val Acc  Test Acc  \n",
      "0   92.891667     93.11  \n",
      "1   94.975000     95.12  \n",
      "2   95.883333     96.18  \n",
      "3   93.108333     93.37  \n",
      "4   94.966667     95.10  \n",
      "5   95.975000     96.22  \n",
      "6   89.033333     89.35  \n",
      "7   90.066667     90.38  \n",
      "8   90.116667     90.72  \n",
      "9   89.400000     89.41  \n",
      "10  90.050000     90.57  \n",
      "11  90.141667     90.84  \n",
      "12  92.391667     92.28  \n",
      "13  93.950000     94.00  \n",
      "14  95.641667     95.99  \n",
      "15  92.441667     92.43  \n",
      "16  94.441667     94.65  \n",
      "17  95.733333     95.64  \n",
      "18  83.766667     84.78  \n",
      "19  87.433333     88.13  \n",
      "20  88.316667     88.81  \n",
      "21  86.325000     87.06  \n",
      "22  87.425000     87.89  \n",
      "23  88.008333     88.34  \n",
      "24  89.133333     89.50  \n",
      "25  90.500000     91.03  \n",
      "26  92.125000     92.48  \n",
      "27  88.500000     89.16  \n",
      "28  90.900000     91.30  \n",
      "29  92.250000     92.57  \n",
      "30  58.808333     60.01  \n",
      "31  63.291667     64.48  \n",
      "32  70.158333     70.63  \n",
      "33  64.500000     64.06  \n",
      "34  64.850000     65.97  \n",
      "35  68.683333     69.50  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "devices = ['cpu', 'mps']\n",
    "batch_sizes = [64, 128, 1024]\n",
    "optimizers = ['adam', 'sgd']\n",
    "# learning_rates= [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "hidden_dims = [16, 32, 64]\n",
    "for batch_size in batch_sizes:\n",
    "    for optimizer in optimizers:\n",
    "        for device in devices:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                training_time, train_acc, val_acc, test_acc = ten_digit(batch_size=batch_size, \n",
    "                                                                        optimizer=optimizer,\n",
    "                                                                        hidden_dim=hidden_dim,\n",
    "                                                                        # lr = lr, \n",
    "                                                                        device=device )\n",
    "                lr = 1e-3 if optimizer=='adam' else 1e-2\n",
    "                print([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "                results.append([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "\n",
    "\n",
    "\n",
    "headers = ['Device', 'Batch size', 'Optimizer', 'LR', 'Hidden Dim', \n",
    "           'Training Time', 'Train Acc', 'Val Acc', 'Test Acc']\n",
    "df = pd.DataFrame(results, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e34b0d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Device</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>LR</th>\n",
       "      <th>Hidden Dim</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>29.785606</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>92.891667</td>\n",
       "      <td>93.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>30.659514</td>\n",
       "      <td>95.595833</td>\n",
       "      <td>94.975000</td>\n",
       "      <td>95.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>30.598462</td>\n",
       "      <td>97.102083</td>\n",
       "      <td>95.883333</td>\n",
       "      <td>96.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mps</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>70.204129</td>\n",
       "      <td>93.479167</td>\n",
       "      <td>93.108333</td>\n",
       "      <td>93.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mps</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>64.602459</td>\n",
       "      <td>95.714583</td>\n",
       "      <td>94.966667</td>\n",
       "      <td>95.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mps</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>68.291154</td>\n",
       "      <td>97.020833</td>\n",
       "      <td>95.975000</td>\n",
       "      <td>96.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>28.561677</td>\n",
       "      <td>88.981250</td>\n",
       "      <td>89.033333</td>\n",
       "      <td>89.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>28.658032</td>\n",
       "      <td>90.043750</td>\n",
       "      <td>90.066667</td>\n",
       "      <td>90.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>29.286179</td>\n",
       "      <td>90.018750</td>\n",
       "      <td>90.116667</td>\n",
       "      <td>90.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mps</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>54.447421</td>\n",
       "      <td>89.041667</td>\n",
       "      <td>89.400000</td>\n",
       "      <td>89.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mps</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>54.181696</td>\n",
       "      <td>89.829167</td>\n",
       "      <td>90.050000</td>\n",
       "      <td>90.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mps</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>53.712011</td>\n",
       "      <td>90.085417</td>\n",
       "      <td>90.141667</td>\n",
       "      <td>90.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>27.091195</td>\n",
       "      <td>92.866667</td>\n",
       "      <td>92.391667</td>\n",
       "      <td>92.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>27.342378</td>\n",
       "      <td>94.658333</td>\n",
       "      <td>93.950000</td>\n",
       "      <td>94.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>28.855187</td>\n",
       "      <td>96.479167</td>\n",
       "      <td>95.641667</td>\n",
       "      <td>95.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mps</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>45.284878</td>\n",
       "      <td>92.745833</td>\n",
       "      <td>92.441667</td>\n",
       "      <td>92.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mps</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>45.769990</td>\n",
       "      <td>94.881250</td>\n",
       "      <td>94.441667</td>\n",
       "      <td>94.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mps</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>45.411039</td>\n",
       "      <td>96.335417</td>\n",
       "      <td>95.733333</td>\n",
       "      <td>95.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>26.004951</td>\n",
       "      <td>83.839583</td>\n",
       "      <td>83.766667</td>\n",
       "      <td>84.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>26.085992</td>\n",
       "      <td>87.043750</td>\n",
       "      <td>87.433333</td>\n",
       "      <td>88.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>27.501497</td>\n",
       "      <td>87.927083</td>\n",
       "      <td>88.316667</td>\n",
       "      <td>88.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mps</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>40.281309</td>\n",
       "      <td>85.856250</td>\n",
       "      <td>86.325000</td>\n",
       "      <td>87.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mps</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>40.950526</td>\n",
       "      <td>87.064583</td>\n",
       "      <td>87.425000</td>\n",
       "      <td>87.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mps</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>41.059408</td>\n",
       "      <td>87.608333</td>\n",
       "      <td>88.008333</td>\n",
       "      <td>88.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>25.750902</td>\n",
       "      <td>88.908333</td>\n",
       "      <td>89.133333</td>\n",
       "      <td>89.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>25.995570</td>\n",
       "      <td>90.672917</td>\n",
       "      <td>90.500000</td>\n",
       "      <td>91.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>25.665561</td>\n",
       "      <td>92.191667</td>\n",
       "      <td>92.125000</td>\n",
       "      <td>92.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mps</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>28.267909</td>\n",
       "      <td>88.556250</td>\n",
       "      <td>88.500000</td>\n",
       "      <td>89.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mps</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>27.744359</td>\n",
       "      <td>90.995833</td>\n",
       "      <td>90.900000</td>\n",
       "      <td>91.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mps</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>28.016613</td>\n",
       "      <td>92.343750</td>\n",
       "      <td>92.250000</td>\n",
       "      <td>92.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>25.572664</td>\n",
       "      <td>58.089583</td>\n",
       "      <td>58.808333</td>\n",
       "      <td>60.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>25.616308</td>\n",
       "      <td>62.668750</td>\n",
       "      <td>63.291667</td>\n",
       "      <td>64.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>25.645607</td>\n",
       "      <td>68.406250</td>\n",
       "      <td>70.158333</td>\n",
       "      <td>70.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>mps</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>26.766539</td>\n",
       "      <td>63.368750</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>64.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>mps</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>26.713382</td>\n",
       "      <td>63.695833</td>\n",
       "      <td>64.850000</td>\n",
       "      <td>65.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mps</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>26.389231</td>\n",
       "      <td>67.987500</td>\n",
       "      <td>68.683333</td>\n",
       "      <td>69.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Device  Batch size Optimizer     LR  Hidden Dim  Training Time  Train Acc  \\\n",
       "0     cpu          64      adam  0.001          16      29.785606  93.333333   \n",
       "1     cpu          64      adam  0.001          32      30.659514  95.595833   \n",
       "2     cpu          64      adam  0.001          64      30.598462  97.102083   \n",
       "3     mps          64      adam  0.001          16      70.204129  93.479167   \n",
       "4     mps          64      adam  0.001          32      64.602459  95.714583   \n",
       "5     mps          64      adam  0.001          64      68.291154  97.020833   \n",
       "6     cpu          64       sgd  0.010          16      28.561677  88.981250   \n",
       "7     cpu          64       sgd  0.010          32      28.658032  90.043750   \n",
       "8     cpu          64       sgd  0.010          64      29.286179  90.018750   \n",
       "9     mps          64       sgd  0.010          16      54.447421  89.041667   \n",
       "10    mps          64       sgd  0.010          32      54.181696  89.829167   \n",
       "11    mps          64       sgd  0.010          64      53.712011  90.085417   \n",
       "12    cpu         128      adam  0.001          16      27.091195  92.866667   \n",
       "13    cpu         128      adam  0.001          32      27.342378  94.658333   \n",
       "14    cpu         128      adam  0.001          64      28.855187  96.479167   \n",
       "15    mps         128      adam  0.001          16      45.284878  92.745833   \n",
       "16    mps         128      adam  0.001          32      45.769990  94.881250   \n",
       "17    mps         128      adam  0.001          64      45.411039  96.335417   \n",
       "18    cpu         128       sgd  0.010          16      26.004951  83.839583   \n",
       "19    cpu         128       sgd  0.010          32      26.085992  87.043750   \n",
       "20    cpu         128       sgd  0.010          64      27.501497  87.927083   \n",
       "21    mps         128       sgd  0.010          16      40.281309  85.856250   \n",
       "22    mps         128       sgd  0.010          32      40.950526  87.064583   \n",
       "23    mps         128       sgd  0.010          64      41.059408  87.608333   \n",
       "24    cpu        1024      adam  0.001          16      25.750902  88.908333   \n",
       "25    cpu        1024      adam  0.001          32      25.995570  90.672917   \n",
       "26    cpu        1024      adam  0.001          64      25.665561  92.191667   \n",
       "27    mps        1024      adam  0.001          16      28.267909  88.556250   \n",
       "28    mps        1024      adam  0.001          32      27.744359  90.995833   \n",
       "29    mps        1024      adam  0.001          64      28.016613  92.343750   \n",
       "30    cpu        1024       sgd  0.010          16      25.572664  58.089583   \n",
       "31    cpu        1024       sgd  0.010          32      25.616308  62.668750   \n",
       "32    cpu        1024       sgd  0.010          64      25.645607  68.406250   \n",
       "33    mps        1024       sgd  0.010          16      26.766539  63.368750   \n",
       "34    mps        1024       sgd  0.010          32      26.713382  63.695833   \n",
       "35    mps        1024       sgd  0.010          64      26.389231  67.987500   \n",
       "\n",
       "      Val Acc  Test Acc  \n",
       "0   92.891667     93.11  \n",
       "1   94.975000     95.12  \n",
       "2   95.883333     96.18  \n",
       "3   93.108333     93.37  \n",
       "4   94.966667     95.10  \n",
       "5   95.975000     96.22  \n",
       "6   89.033333     89.35  \n",
       "7   90.066667     90.38  \n",
       "8   90.116667     90.72  \n",
       "9   89.400000     89.41  \n",
       "10  90.050000     90.57  \n",
       "11  90.141667     90.84  \n",
       "12  92.391667     92.28  \n",
       "13  93.950000     94.00  \n",
       "14  95.641667     95.99  \n",
       "15  92.441667     92.43  \n",
       "16  94.441667     94.65  \n",
       "17  95.733333     95.64  \n",
       "18  83.766667     84.78  \n",
       "19  87.433333     88.13  \n",
       "20  88.316667     88.81  \n",
       "21  86.325000     87.06  \n",
       "22  87.425000     87.89  \n",
       "23  88.008333     88.34  \n",
       "24  89.133333     89.50  \n",
       "25  90.500000     91.03  \n",
       "26  92.125000     92.48  \n",
       "27  88.500000     89.16  \n",
       "28  90.900000     91.30  \n",
       "29  92.250000     92.57  \n",
       "30  58.808333     60.01  \n",
       "31  63.291667     64.48  \n",
       "32  70.158333     70.63  \n",
       "33  64.500000     64.06  \n",
       "34  64.850000     65.97  \n",
       "35  68.683333     69.50  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6558a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sigmoid_hyperopt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e10866be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MulticlassMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=88, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (fc2): Linear(in_features=88, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MulticlassMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(MulticlassMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Your code goes here\n",
    "hidden_dim = int(np.sqrt(28*28*10))\n",
    "model = MulticlassMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "908312df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cpu', 64, 'adam', 0.001, 16, 29.184563159942627, 88.03333333333333, 87.925, 88.14]\n",
      "['cpu', 64, 'adam', 0.001, 32, 30.543816089630127, 95.29583333333333, 94.175, 94.25]\n",
      "['cpu', 64, 'adam', 0.001, 64, 31.19202208518982, 96.96458333333334, 96.225, 96.5]\n",
      "['cpu', 64, 'adam', 0.001, 128, 32.99071264266968, 97.86458333333333, 96.95, 96.95]\n",
      "['cpu', 64, 'sgd', 0.01, 16, 27.672548055648804, 92.31875, 92.2, 92.65]\n",
      "['cpu', 64, 'sgd', 0.01, 32, 28.152845859527588, 92.80208333333333, 92.65, 92.93]\n",
      "['cpu', 64, 'sgd', 0.01, 64, 28.80337882041931, 93.4625, 93.3, 93.56]\n",
      "['cpu', 64, 'sgd', 0.01, 128, 29.64063310623169, 93.91458333333334, 93.63333333333334, 93.97]\n",
      "['cpu', 128, 'adam', 0.001, 16, 26.838134050369263, 93.07291666666667, 92.55, 92.78]\n",
      "['cpu', 128, 'adam', 0.001, 32, 27.21410608291626, 95.55833333333334, 95.125, 95.59]\n",
      "['cpu', 128, 'adam', 0.001, 64, 28.684025287628174, 96.66875, 96.06666666666666, 96.28]\n",
      "['cpu', 128, 'adam', 0.001, 128, 29.71784520149231, 97.76458333333333, 96.55833333333334, 96.38]\n",
      "['cpu', 128, 'sgd', 0.01, 16, 26.283130168914795, 91.30833333333334, 91.0, 91.4]\n",
      "['cpu', 128, 'sgd', 0.01, 32, 26.356427907943726, 91.24791666666667, 91.2, 91.74]\n",
      "['cpu', 128, 'sgd', 0.01, 64, 27.09726619720459, 91.7, 91.75, 92.25]\n",
      "['cpu', 128, 'sgd', 0.01, 128, 27.72050929069519, 91.6625, 91.63333333333334, 92.03]\n",
      "['cpu', 1024, 'adam', 0.001, 16, 25.402145862579346, 90.80833333333334, 90.7, 91.16]\n",
      "['cpu', 1024, 'adam', 0.001, 32, 25.0838680267334, 91.53125, 91.625, 91.89]\n",
      "['cpu', 1024, 'adam', 0.001, 64, 25.195961713790894, 92.72708333333334, 92.64166666666667, 92.77]\n",
      "['cpu', 1024, 'adam', 0.001, 128, 25.59655475616455, 94.75208333333333, 94.5, 94.54]\n",
      "['cpu', 1024, 'sgd', 0.01, 16, 25.069660186767578, 84.33125, 84.7, 85.12]\n",
      "['cpu', 1024, 'sgd', 0.01, 32, 24.970707178115845, 84.04166666666667, 84.775, 85.38]\n",
      "['cpu', 1024, 'sgd', 0.01, 64, 25.030744075775146, 86.14375, 86.625, 87.25]\n",
      "['cpu', 1024, 'sgd', 0.01, 128, 25.4376060962677, 86.57291666666667, 86.81666666666666, 87.31]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Device</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>LR</th>\n",
       "      <th>Hidden Dim</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>29.184563</td>\n",
       "      <td>88.033333</td>\n",
       "      <td>87.925000</td>\n",
       "      <td>88.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>30.543816</td>\n",
       "      <td>95.295833</td>\n",
       "      <td>94.175000</td>\n",
       "      <td>94.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>31.192022</td>\n",
       "      <td>96.964583</td>\n",
       "      <td>96.225000</td>\n",
       "      <td>96.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>32.990713</td>\n",
       "      <td>97.864583</td>\n",
       "      <td>96.950000</td>\n",
       "      <td>96.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>27.672548</td>\n",
       "      <td>92.318750</td>\n",
       "      <td>92.200000</td>\n",
       "      <td>92.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>28.152846</td>\n",
       "      <td>92.802083</td>\n",
       "      <td>92.650000</td>\n",
       "      <td>92.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>28.803379</td>\n",
       "      <td>93.462500</td>\n",
       "      <td>93.300000</td>\n",
       "      <td>93.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>29.640633</td>\n",
       "      <td>93.914583</td>\n",
       "      <td>93.633333</td>\n",
       "      <td>93.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>26.838134</td>\n",
       "      <td>93.072917</td>\n",
       "      <td>92.550000</td>\n",
       "      <td>92.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>27.214106</td>\n",
       "      <td>95.558333</td>\n",
       "      <td>95.125000</td>\n",
       "      <td>95.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>28.684025</td>\n",
       "      <td>96.668750</td>\n",
       "      <td>96.066667</td>\n",
       "      <td>96.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>29.717845</td>\n",
       "      <td>97.764583</td>\n",
       "      <td>96.558333</td>\n",
       "      <td>96.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>26.283130</td>\n",
       "      <td>91.308333</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>91.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>26.356428</td>\n",
       "      <td>91.247917</td>\n",
       "      <td>91.200000</td>\n",
       "      <td>91.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>27.097266</td>\n",
       "      <td>91.700000</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>92.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>27.720509</td>\n",
       "      <td>91.662500</td>\n",
       "      <td>91.633333</td>\n",
       "      <td>92.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>16</td>\n",
       "      <td>25.402146</td>\n",
       "      <td>90.808333</td>\n",
       "      <td>90.700000</td>\n",
       "      <td>91.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>25.083868</td>\n",
       "      <td>91.531250</td>\n",
       "      <td>91.625000</td>\n",
       "      <td>91.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>25.195962</td>\n",
       "      <td>92.727083</td>\n",
       "      <td>92.641667</td>\n",
       "      <td>92.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>25.596555</td>\n",
       "      <td>94.752083</td>\n",
       "      <td>94.500000</td>\n",
       "      <td>94.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16</td>\n",
       "      <td>25.069660</td>\n",
       "      <td>84.331250</td>\n",
       "      <td>84.700000</td>\n",
       "      <td>85.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>24.970707</td>\n",
       "      <td>84.041667</td>\n",
       "      <td>84.775000</td>\n",
       "      <td>85.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>25.030744</td>\n",
       "      <td>86.143750</td>\n",
       "      <td>86.625000</td>\n",
       "      <td>87.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>25.437606</td>\n",
       "      <td>86.572917</td>\n",
       "      <td>86.816667</td>\n",
       "      <td>87.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Device  Batch size Optimizer     LR  Hidden Dim  Training Time  Train Acc  \\\n",
       "0     cpu          64      adam  0.001          16      29.184563  88.033333   \n",
       "1     cpu          64      adam  0.001          32      30.543816  95.295833   \n",
       "2     cpu          64      adam  0.001          64      31.192022  96.964583   \n",
       "3     cpu          64      adam  0.001         128      32.990713  97.864583   \n",
       "4     cpu          64       sgd  0.010          16      27.672548  92.318750   \n",
       "5     cpu          64       sgd  0.010          32      28.152846  92.802083   \n",
       "6     cpu          64       sgd  0.010          64      28.803379  93.462500   \n",
       "7     cpu          64       sgd  0.010         128      29.640633  93.914583   \n",
       "8     cpu         128      adam  0.001          16      26.838134  93.072917   \n",
       "9     cpu         128      adam  0.001          32      27.214106  95.558333   \n",
       "10    cpu         128      adam  0.001          64      28.684025  96.668750   \n",
       "11    cpu         128      adam  0.001         128      29.717845  97.764583   \n",
       "12    cpu         128       sgd  0.010          16      26.283130  91.308333   \n",
       "13    cpu         128       sgd  0.010          32      26.356428  91.247917   \n",
       "14    cpu         128       sgd  0.010          64      27.097266  91.700000   \n",
       "15    cpu         128       sgd  0.010         128      27.720509  91.662500   \n",
       "16    cpu        1024      adam  0.001          16      25.402146  90.808333   \n",
       "17    cpu        1024      adam  0.001          32      25.083868  91.531250   \n",
       "18    cpu        1024      adam  0.001          64      25.195962  92.727083   \n",
       "19    cpu        1024      adam  0.001         128      25.596555  94.752083   \n",
       "20    cpu        1024       sgd  0.010          16      25.069660  84.331250   \n",
       "21    cpu        1024       sgd  0.010          32      24.970707  84.041667   \n",
       "22    cpu        1024       sgd  0.010          64      25.030744  86.143750   \n",
       "23    cpu        1024       sgd  0.010         128      25.437606  86.572917   \n",
       "\n",
       "      Val Acc  Test Acc  \n",
       "0   87.925000     88.14  \n",
       "1   94.175000     94.25  \n",
       "2   96.225000     96.50  \n",
       "3   96.950000     96.95  \n",
       "4   92.200000     92.65  \n",
       "5   92.650000     92.93  \n",
       "6   93.300000     93.56  \n",
       "7   93.633333     93.97  \n",
       "8   92.550000     92.78  \n",
       "9   95.125000     95.59  \n",
       "10  96.066667     96.28  \n",
       "11  96.558333     96.38  \n",
       "12  91.000000     91.40  \n",
       "13  91.200000     91.74  \n",
       "14  91.750000     92.25  \n",
       "15  91.633333     92.03  \n",
       "16  90.700000     91.16  \n",
       "17  91.625000     91.89  \n",
       "18  92.641667     92.77  \n",
       "19  94.500000     94.54  \n",
       "20  84.700000     85.12  \n",
       "21  84.775000     85.38  \n",
       "22  86.625000     87.25  \n",
       "23  86.816667     87.31  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "devices = ['cpu']\n",
    "batch_sizes = [64, 128, 1024]\n",
    "optimizers = ['adam', 'sgd']\n",
    "# learning_rates= [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "hidden_dims = [16, 32, 64, 128]\n",
    "for batch_size in batch_sizes:\n",
    "    for optimizer in optimizers:\n",
    "        for device in devices:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                training_time, train_acc, val_acc, test_acc = ten_digit(batch_size=batch_size, \n",
    "                                                                        optimizer=optimizer,\n",
    "                                                                        hidden_dim=hidden_dim,\n",
    "                                                                        # lr = lr, \n",
    "                                                                        device=device )\n",
    "                lr = 1e-3 if optimizer=='adam' else 1e-2\n",
    "                print([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "                results.append([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "\n",
    "\n",
    "\n",
    "headers = ['Device', 'Batch size', 'Optimizer', 'LR', 'Hidden Dim', \n",
    "           'Training Time', 'Train Acc', 'Val Acc', 'Test Acc']\n",
    "df = pd.DataFrame(results, columns=headers)\n",
    "df.to_csv('relu_hyperopt_q2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7de796-0049-4efe-9ea0-ce64a5e24c85",
   "metadata": {},
   "source": [
    "# Problem 3: Handling Class Imbalance in MNIST Dataset\n",
    "In this problem, we will explore how to handle class imbalance problems, which are very common in real-world applications. A modified MNIST dataset is created as follows: we choose all instances of digit “0”, and choose only 1\\% instances of digit “1” for both training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef050b66-e17c-406b-8e79-c2a1416e2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for digits 0 and 1\n",
    "train_0 = [data for data in mnist if data[1] == 0]\n",
    "train_1 = [data for data in mnist if data[1] == 1]\n",
    "train_1 = train_1[:len(train_1) // 100]\n",
    "train_data = train_0 + train_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22857293-024a-4252-a3fb-2cb72cafe0e0",
   "metadata": {},
   "source": [
    "For such a class imbalance problem, accuracy may not be a good metric. Always predicting \"0\" regardless of the input can be 99\\% accurate. Instead, we use the $F_1$ score as the evaluation metric:\n",
    "$$F_1 = 2\\cdot\\frac{\\text{precision}\\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "where precision and recall are defined as:\n",
    "$$\\text{precision}=\\frac{\\text{number of instances correctly predicted as \"1\"}}{\\text{number of instances predicted as \"1\"}}$$\n",
    "$$\\text{recall}=\\frac{\\text{number of instances correctly predicted as \"1\"}}{\\text{number of instances labeled as \"1\"}}$$\n",
    "\n",
    "To handle such a problem, some changes to the training may be necessary. Some suggestions include: \n",
    "1) Adjusting the class weights in the loss function, i.e., use a larger weight for the minority class when computing the loss.\n",
    "2) Implementing resampling techniques (either undersampling the majority class or oversampling the minority class).\n",
    "\n",
    "<span style=\"color:red\">[YOUR TASK]</span>\n",
    "- Create the imbalance datasets with all \"0\" digits and only 1\\% \"1\" digits.\n",
    "- Implement the training loop and evaluation section (implementing the $F_1$ metric). \n",
    "- Ignore the class imbalance problem and train the MLP. Report your hyper-parameter details and the $F_1$ score performance on the test set (as the baseline).\n",
    "- Explore modifications to improve the performance of the class imbalance problem. Report your modifications and the $F_1$ scores performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daf6df17-48b9-4617-b12d-1d37865a5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd07a3-e41b-4650-b4ae-ed4e2ff6c99d",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[EXTRA BONUS]</span>\n",
    "\n",
    "If the hyper-parameters are chosen properly, the baseline can perform satisfactorily on the class imbalance problem with 1% digit \"1\". We want to challenge the baseline and handle more class-imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e4a01-36aa-4fd2-9ec2-cb88db566772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "N = 1000\n",
    "# generate a class-imbalanced dataset controlled by \"N\"\n",
    "train_0 = [data for data in mnist if data[1] == 0]\n",
    "train_1 = [data for data in mnist if data[1] == 1]\n",
    "random.shuffle(train_1)\n",
    "train_1 = train_1[:len(train_1) // N]\n",
    "train_data = train_0 + train_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0b82a-def4-45d0-9350-d79f00bfb671",
   "metadata": {},
   "source": [
    "Can you propose new ways for the class imbalance problem and achieve stable and satisfactory performance for large $N = 500, \\; 1000, \\; \\cdots$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47fb7f41-2e28-4935-a9cd-124e4963fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ed5bf-680c-4548-9e47-e5c17316d891",
   "metadata": {},
   "source": [
    "# Problem 4: Reconstruct the MNIST images by Regression\n",
    "In this problem, we want to train the MLP (with only one hidden layer) to complete a regression task: reconstruct the input image. The goal of this task is dimension reduction, and we set the hidden layer dimension to a smaller number, say 50. Once we can train the MLP to reconstruct the input images perfectly, we find an lower dimension representation of the MNIST images.\n",
    "\n",
    "Since this is a reconstruction task, the labels of the images are not needed, and the target is the same as the inputs. Mean Squared Error (MSE) is recommended as the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5bf64-c614-4ba1-bf3a-2025232a2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c03c6-9b4a-4bc8-86b3-efe72b9db558",
   "metadata": {},
   "source": [
    "Another tip is to add a `torch.nn.Tanh()` activation layer to the end of the model. Recall that our data pre-processing converts the data into the range $[-1, 1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8cc1aa-cd69-4716-a8a5-ae7ba5a441f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data pre-processing\n",
    "# convert the input to the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db3945-7dc5-484b-b6f2-402b0f5a2c1a",
   "metadata": {},
   "source": [
    "Having a `torch.nn.Tanh()` activation layer at the end of the model can convert the output of the model into the range $[-1, 1]$, making the training easier.\n",
    "\n",
    "<span style=\"color:red\">[YOUR TASK]</span>\n",
    "- Define an MLP with only one hidden layer and set the hidden layer dimension as 50. Train the MLP to reconstruct input images from all 10 digits.\n",
    "- Report the Mean Squared Error on the training, validation and test set. Report your hyper-parameter details.\n",
    "- Pick 5 images for each digit from the test set. Visualize the original images and the reconstructed images using the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0eb902a8-e07a-421e-80db-12a4d33c1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3a88e-e3b2-4c82-bf42-ab789c452b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
