{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c574ceac-8715-4368-9f04-1342c814e8fd",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch\n",
    "\n",
    "# Problem 1: Simple MLP for Binary Classification\n",
    "In this problem, you will train a simple MLP to classify two handwritten digits: 0 vs 1. We provide some starter codes to do this task with steps. However, you do not need to follow the exact steps as long as you can complete the task in sections marked as <span style=\"color:red\">[YOUR TASK]</span>.\n",
    "\n",
    "## Dataset Setup\n",
    "We will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The `torchvision` package has supported this dataset. We can load the dataset in this way (the dataset will take up 63M of your disk space):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "872aca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_26113/3151982987.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9801782f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('14.2.1', ('', '', ''), 'arm64')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_26113/3673749337.py:3: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform, time\n",
    "print(platform.mac_ver() )\n",
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ec73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b2483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not torch.backends.mps.is_available():\n",
    "#     if not torch.backends.mps.is_built():\n",
    "#         print(\"MPS not available because the current PyTorch install was not \"\n",
    "#               \"built with MPS enabled.\")\n",
    "#     else:\n",
    "#         print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "#               \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    \n",
    "# else:\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print('mps enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af33ec5-782b-4a0b-819b-f649500627c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data pre-processing\n",
    "# convert the input to the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "    )\n",
    "\n",
    "# Load the MNIST dataset \n",
    "# this command requires Internet to download the dataset\n",
    "mnist = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transform)\n",
    "mnist_test = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data',   # './data'\n",
    "                            train=False, \n",
    "                            download=True, \n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14f718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies:  tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "print(\"Frequencies: \", torch.bincount(mnist.targets))\n",
    "print(len(torch.bincount(mnist.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff472db-51ee-4835-8217-8557947f0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "train_len = int(len(mnist) *.8)\n",
    "val_len = len(mnist) - train_len\n",
    "train_set, val_set = random_split(mnist, [train_len, val_len])\n",
    "\n",
    "# Define DataLoaders to access data in batches\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size = 64, shuffle=False)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc4f021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7fc30-cef1-4cfc-bbde-4de36737ea88",
   "metadata": {},
   "source": [
    "# Problem 2: MNIST 10-class classification\n",
    "\n",
    "Now we want to train an MLP to handle multi-class classification for all 10 digits in the MNIST dataset. We will use the full MNIST dataset without filtering for specific digits. You may modify the MLP so that it can be used for multi-class classification.\n",
    "\n",
    "<span style=\"color:red\">[YOUR TASK]</span>\n",
    "- Implement the training loop and evaluation section. Report the hyper-parameters you choose.\n",
    "- Experiment with different numbers of neurons in the hidden layer and note any changes in performance.\n",
    "- Write a brief analysis of the model's performance, including any challenges faced and how they were addressed.\n",
    "\n",
    "In our implementations, we trained our network for 10 epochs in about 20 seconds on a laptop.\n",
    "When you define a new model, remember to update the optimizer!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f96f27ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MulticlassMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=88, bias=True)\n",
      "  (activation): Sigmoid()\n",
      "  (fc2): Linear(in_features=88, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MulticlassMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(MulticlassMLP, self).__init__()\n",
    "        # Your code goes here\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Your code goes here\n",
    "hidden_dim = int(np.sqrt(28*28*10))\n",
    "model = MulticlassMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f37802c-97df-490d-85c6-6a4d8fa69515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_digit(batch_size, hidden_dim, optimizer,  device = 'cpu'): # or mps lr=1e-3,\n",
    "    device = torch.device(device)\n",
    "    # Define DataLoaders to access data in batches\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # Your code goes here\n",
    "    val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    model = MulticlassMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=10).to(device)\n",
    "    # print(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        lr = 1e-3\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        lr=1e-2\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "        # print(f'Training accuracy: {train_acc:.2f}%')\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    training_time = time.time()- start_time\n",
    "    # print(training_time)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            \n",
    "    test_acc = 100. * correct / total\n",
    "    # print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    print('hyperopt run done')\n",
    "    return training_time, train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff1ad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4778\n",
      "Epoch 2, Loss: 1.1644\n",
      "Epoch 3, Loss: 1.1134\n",
      "Epoch 4, Loss: 0.7658\n",
      "Epoch 5, Loss: 0.7291\n",
      "Epoch 6, Loss: 0.8017\n",
      "Epoch 7, Loss: 0.5732\n",
      "Epoch 8, Loss: 0.5302\n",
      "Epoch 9, Loss: 0.5318\n",
      "Epoch 10, Loss: 0.7484\n",
      "hyperopt run done\n",
      "['cpu', 64, 'adam', 0.001, 4, 20.82248091697693, 81.11666666666666, 80.5, 81.34]\n",
      "Epoch 1, Loss: 0.4266\n",
      "Epoch 2, Loss: 0.2298\n",
      "Epoch 3, Loss: 0.1818\n",
      "Epoch 4, Loss: 0.1993\n",
      "Epoch 5, Loss: 0.2510\n",
      "Epoch 6, Loss: 0.1186\n",
      "Epoch 7, Loss: 0.1753\n",
      "Epoch 8, Loss: 0.1715\n",
      "Epoch 9, Loss: 0.2112\n",
      "Epoch 10, Loss: 0.1112\n",
      "hyperopt run done\n",
      "['cpu', 64, 'adam', 0.001, 32, 21.138930320739746, 95.725, 94.58333333333333, 94.83]\n",
      "Epoch 1, Loss: 0.2527\n",
      "Epoch 2, Loss: 0.1573\n",
      "Epoch 3, Loss: 0.2421\n",
      "Epoch 4, Loss: 0.1623\n",
      "Epoch 5, Loss: 0.1341\n",
      "Epoch 6, Loss: 0.1033\n",
      "Epoch 7, Loss: 0.2037\n",
      "Epoch 8, Loss: 0.0764\n",
      "Epoch 9, Loss: 0.1392\n",
      "Epoch 10, Loss: 0.0966\n",
      "hyperopt run done\n",
      "['cpu', 64, 'adam', 0.001, 64, 22.073091745376587, 97.20833333333333, 95.56666666666666, 96.09]\n",
      "Epoch 1, Loss: 0.2130\n",
      "Epoch 2, Loss: 0.2982\n",
      "Epoch 3, Loss: 0.1674\n",
      "Epoch 4, Loss: 0.0555\n",
      "Epoch 5, Loss: 0.0363\n",
      "Epoch 6, Loss: 0.0632\n",
      "Epoch 7, Loss: 0.0830\n",
      "Epoch 8, Loss: 0.0157\n",
      "Epoch 9, Loss: 0.0618\n",
      "Epoch 10, Loss: 0.0344\n",
      "hyperopt run done\n",
      "['cpu', 64, 'adam', 0.001, 128, 22.68277382850647, 98.1, 96.59166666666667, 96.87]\n",
      "Epoch 1, Loss: 1.9539\n",
      "Epoch 2, Loss: 1.7180\n",
      "Epoch 3, Loss: 1.6575\n",
      "Epoch 4, Loss: 1.4635\n",
      "Epoch 5, Loss: 1.3582\n",
      "Epoch 6, Loss: 1.3422\n",
      "Epoch 7, Loss: 1.2889\n",
      "Epoch 8, Loss: 1.2607\n",
      "Epoch 9, Loss: 1.1511\n",
      "Epoch 10, Loss: 1.0349\n",
      "hyperopt run done\n",
      "['cpu', 64, 'sgd', 0.01, 4, 19.016015768051147, 69.94166666666666, 70.08333333333333, 70.49]\n",
      "Epoch 1, Loss: 1.6734\n",
      "Epoch 2, Loss: 1.1472\n",
      "Epoch 3, Loss: 1.0010\n",
      "Epoch 4, Loss: 0.6891\n",
      "Epoch 5, Loss: 0.6092\n",
      "Epoch 6, Loss: 0.7329\n",
      "Epoch 7, Loss: 0.5107\n",
      "Epoch 8, Loss: 0.4895\n",
      "Epoch 9, Loss: 0.4890\n",
      "Epoch 10, Loss: 0.3967\n",
      "hyperopt run done\n",
      "['cpu', 64, 'sgd', 0.01, 32, 19.240643978118896, 89.88333333333334, 89.66666666666667, 90.33]\n",
      "Epoch 1, Loss: 1.5164\n",
      "Epoch 2, Loss: 1.0273\n",
      "Epoch 3, Loss: 0.6324\n",
      "Epoch 4, Loss: 0.6278\n",
      "Epoch 5, Loss: 0.5124\n",
      "Epoch 6, Loss: 0.3271\n",
      "Epoch 7, Loss: 0.2803\n",
      "Epoch 8, Loss: 0.3499\n",
      "Epoch 9, Loss: 0.3146\n",
      "Epoch 10, Loss: 0.3593\n",
      "hyperopt run done\n",
      "['cpu', 64, 'sgd', 0.01, 64, 19.526298999786377, 90.02916666666667, 89.86666666666666, 90.57]\n",
      "Epoch 1, Loss: 1.2512\n",
      "Epoch 2, Loss: 0.8754\n",
      "Epoch 3, Loss: 0.6541\n",
      "Epoch 4, Loss: 0.3912\n",
      "Epoch 5, Loss: 0.5210\n",
      "Epoch 6, Loss: 0.4884\n",
      "Epoch 7, Loss: 0.4024\n",
      "Epoch 8, Loss: 0.3235\n",
      "Epoch 9, Loss: 0.3439\n",
      "Epoch 10, Loss: 0.2997\n",
      "hyperopt run done\n",
      "['cpu', 64, 'sgd', 0.01, 128, 20.448699712753296, 90.0375, 89.85, 90.69]\n",
      "Epoch 1, Loss: 1.6865\n",
      "Epoch 2, Loss: 1.4564\n",
      "Epoch 3, Loss: 1.2613\n",
      "Epoch 4, Loss: 1.2373\n",
      "Epoch 5, Loss: 1.0469\n",
      "Epoch 6, Loss: 0.9731\n",
      "Epoch 7, Loss: 1.0923\n",
      "Epoch 8, Loss: 0.8514\n",
      "Epoch 9, Loss: 0.8003\n",
      "Epoch 10, Loss: 0.8840\n",
      "hyperopt run done\n",
      "['cpu', 128, 'adam', 0.001, 4, 18.587870121002197, 74.94166666666666, 73.93333333333334, 75.06]\n",
      "Epoch 1, Loss: 0.5235\n",
      "Epoch 2, Loss: 0.4067\n",
      "Epoch 3, Loss: 0.2940\n",
      "Epoch 4, Loss: 0.1565\n",
      "Epoch 5, Loss: 0.2307\n",
      "Epoch 6, Loss: 0.1264\n",
      "Epoch 7, Loss: 0.3182\n",
      "Epoch 8, Loss: 0.1788\n",
      "Epoch 9, Loss: 0.2040\n",
      "Epoch 10, Loss: 0.2203\n",
      "hyperopt run done\n",
      "['cpu', 128, 'adam', 0.001, 32, 18.8074209690094, 95.02916666666667, 94.34166666666667, 94.47]\n",
      "Epoch 1, Loss: 0.4440\n",
      "Epoch 2, Loss: 0.2228\n",
      "Epoch 3, Loss: 0.2833\n",
      "Epoch 4, Loss: 0.1420\n",
      "Epoch 5, Loss: 0.2444\n",
      "Epoch 6, Loss: 0.1993\n",
      "Epoch 7, Loss: 0.1968\n",
      "Epoch 8, Loss: 0.0953\n",
      "Epoch 9, Loss: 0.1353\n",
      "Epoch 10, Loss: 0.1018\n",
      "hyperopt run done\n",
      "['cpu', 128, 'adam', 0.001, 64, 19.121339797973633, 96.4125, 95.41666666666667, 95.75]\n",
      "Epoch 1, Loss: 0.2889\n",
      "Epoch 2, Loss: 0.2309\n",
      "Epoch 3, Loss: 0.1552\n",
      "Epoch 4, Loss: 0.1249\n",
      "Epoch 5, Loss: 0.2419\n",
      "Epoch 6, Loss: 0.1416\n",
      "Epoch 7, Loss: 0.0965\n",
      "Epoch 8, Loss: 0.1175\n",
      "Epoch 9, Loss: 0.0925\n",
      "Epoch 10, Loss: 0.1040\n",
      "hyperopt run done\n",
      "['cpu', 128, 'adam', 0.001, 128, 20.28162407875061, 97.40416666666667, 96.43333333333334, 96.45]\n",
      "Epoch 1, Loss: 2.2451\n",
      "Epoch 2, Loss: 2.1222\n",
      "Epoch 3, Loss: 2.0546\n",
      "Epoch 4, Loss: 1.9382\n",
      "Epoch 5, Loss: 1.8954\n",
      "Epoch 6, Loss: 1.8248\n",
      "Epoch 7, Loss: 1.7163\n",
      "Epoch 8, Loss: 1.6299\n",
      "Epoch 9, Loss: 1.5886\n",
      "Epoch 10, Loss: 1.5396\n",
      "hyperopt run done\n",
      "['cpu', 128, 'sgd', 0.01, 4, 17.898255109786987, 63.18125, 63.475, 64.9]\n",
      "Epoch 1, Loss: 1.9713\n",
      "Epoch 2, Loss: 1.5207\n",
      "Epoch 3, Loss: 1.2462\n",
      "Epoch 4, Loss: 1.0905\n",
      "Epoch 5, Loss: 0.9167\n",
      "Epoch 6, Loss: 0.8679\n",
      "Epoch 7, Loss: 0.7049\n",
      "Epoch 8, Loss: 0.6626\n",
      "Epoch 9, Loss: 0.6591\n",
      "Epoch 10, Loss: 0.5902\n",
      "hyperopt run done\n",
      "['cpu', 128, 'sgd', 0.01, 32, 17.97628903388977, 87.2875, 87.29166666666667, 88.1]\n",
      "Epoch 1, Loss: 1.9402\n",
      "Epoch 2, Loss: 1.5055\n",
      "Epoch 3, Loss: 1.0904\n",
      "Epoch 4, Loss: 0.9819\n",
      "Epoch 5, Loss: 0.6629\n",
      "Epoch 6, Loss: 0.7243\n",
      "Epoch 7, Loss: 0.5777\n",
      "Epoch 8, Loss: 0.5428\n",
      "Epoch 9, Loss: 0.4584\n",
      "Epoch 10, Loss: 0.5723\n",
      "hyperopt run done\n",
      "['cpu', 128, 'sgd', 0.01, 64, 18.428339958190918, 87.92291666666667, 87.775, 88.92]\n",
      "Epoch 1, Loss: 1.9361\n",
      "Epoch 2, Loss: 1.3056\n",
      "Epoch 3, Loss: 1.1160\n",
      "Epoch 4, Loss: 0.9489\n",
      "Epoch 5, Loss: 0.7368\n",
      "Epoch 6, Loss: 0.6599\n",
      "Epoch 7, Loss: 0.4881\n",
      "Epoch 8, Loss: 0.5666\n",
      "Epoch 9, Loss: 0.5583\n",
      "Epoch 10, Loss: 0.4761\n",
      "hyperopt run done\n",
      "['cpu', 128, 'sgd', 0.01, 128, 18.753883123397827, 87.92708333333333, 87.9, 88.73]\n",
      "Epoch 1, Loss: 2.1429\n",
      "Epoch 2, Loss: 2.0317\n",
      "Epoch 3, Loss: 1.9835\n",
      "Epoch 4, Loss: 1.8927\n",
      "Epoch 5, Loss: 1.8184\n",
      "Epoch 6, Loss: 1.7639\n",
      "Epoch 7, Loss: 1.7181\n",
      "Epoch 8, Loss: 1.6830\n",
      "Epoch 9, Loss: 1.6392\n",
      "Epoch 10, Loss: 1.5926\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'adam', 0.001, 4, 17.380154848098755, 52.65, 52.416666666666664, 53.09]\n",
      "Epoch 1, Loss: 1.5973\n",
      "Epoch 2, Loss: 1.1750\n",
      "Epoch 3, Loss: 0.8883\n",
      "Epoch 4, Loss: 0.6954\n",
      "Epoch 5, Loss: 0.6192\n",
      "Epoch 6, Loss: 0.5233\n",
      "Epoch 7, Loss: 0.4794\n",
      "Epoch 8, Loss: 0.4362\n",
      "Epoch 9, Loss: 0.4379\n",
      "Epoch 10, Loss: 0.3772\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'adam', 0.001, 32, 17.465012073516846, 90.93958333333333, 90.625, 91.21]\n",
      "Epoch 1, Loss: 1.2954\n",
      "Epoch 2, Loss: 0.8583\n",
      "Epoch 3, Loss: 0.5934\n",
      "Epoch 4, Loss: 0.4764\n",
      "Epoch 5, Loss: 0.4071\n",
      "Epoch 6, Loss: 0.3596\n",
      "Epoch 7, Loss: 0.3512\n",
      "Epoch 8, Loss: 0.3406\n",
      "Epoch 9, Loss: 0.3040\n",
      "Epoch 10, Loss: 0.2716\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'adam', 0.001, 64, 17.708343744277954, 92.27083333333333, 91.86666666666666, 92.49]\n",
      "Epoch 1, Loss: 0.9329\n",
      "Epoch 2, Loss: 0.5370\n",
      "Epoch 3, Loss: 0.4081\n",
      "Epoch 4, Loss: 0.3582\n",
      "Epoch 5, Loss: 0.3589\n",
      "Epoch 6, Loss: 0.3063\n",
      "Epoch 7, Loss: 0.2498\n",
      "Epoch 8, Loss: 0.2872\n",
      "Epoch 9, Loss: 0.2665\n",
      "Epoch 10, Loss: 0.2169\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'adam', 0.001, 128, 18.143819093704224, 93.29791666666667, 92.76666666666667, 93.21]\n",
      "Epoch 1, Loss: 2.3031\n",
      "Epoch 2, Loss: 2.2829\n",
      "Epoch 3, Loss: 2.2561\n",
      "Epoch 4, Loss: 2.2124\n",
      "Epoch 5, Loss: 2.2145\n",
      "Epoch 6, Loss: 2.1952\n",
      "Epoch 7, Loss: 2.1693\n",
      "Epoch 8, Loss: 2.1399\n",
      "Epoch 9, Loss: 2.1436\n",
      "Epoch 10, Loss: 2.1178\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'sgd', 0.01, 4, 17.723569869995117, 20.575, 22.0, 23.66]\n",
      "Epoch 1, Loss: 2.2722\n",
      "Epoch 2, Loss: 2.2314\n",
      "Epoch 3, Loss: 2.2028\n",
      "Epoch 4, Loss: 2.1583\n",
      "Epoch 5, Loss: 2.1378\n",
      "Epoch 6, Loss: 2.0987\n",
      "Epoch 7, Loss: 2.0596\n",
      "Epoch 8, Loss: 1.9987\n",
      "Epoch 9, Loss: 1.9688\n",
      "Epoch 10, Loss: 1.9263\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'sgd', 0.01, 32, 17.807459115982056, 62.58958333333333, 63.075, 65.17]\n",
      "Epoch 1, Loss: 2.2605\n",
      "Epoch 2, Loss: 2.2172\n",
      "Epoch 3, Loss: 2.1803\n",
      "Epoch 4, Loss: 2.1428\n",
      "Epoch 5, Loss: 2.0939\n",
      "Epoch 6, Loss: 2.0362\n",
      "Epoch 7, Loss: 1.9973\n",
      "Epoch 8, Loss: 1.9344\n",
      "Epoch 9, Loss: 1.8694\n",
      "Epoch 10, Loss: 1.8077\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'sgd', 0.01, 64, 18.086972951889038, 67.32708333333333, 66.50833333333334, 68.73]\n",
      "Epoch 1, Loss: 2.2626\n",
      "Epoch 2, Loss: 2.2165\n",
      "Epoch 3, Loss: 2.1749\n",
      "Epoch 4, Loss: 2.1225\n",
      "Epoch 5, Loss: 2.0774\n",
      "Epoch 6, Loss: 2.0285\n",
      "Epoch 7, Loss: 1.9609\n",
      "Epoch 8, Loss: 1.8948\n",
      "Epoch 9, Loss: 1.8569\n",
      "Epoch 10, Loss: 1.7600\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'sgd', 0.01, 128, 18.75179123878479, 70.67916666666666, 70.29166666666667, 71.88]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "devices = ['cpu']\n",
    "batch_sizes = [64, 128, 1024]\n",
    "optimizers = ['adam', 'sgd']\n",
    "# learning_rates= [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "hidden_dims = [4, 32, 64, 128]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for optimizer in optimizers:\n",
    "        for device in devices:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                training_time, train_acc, val_acc, test_acc = ten_digit(batch_size=batch_size, \n",
    "                                                                        optimizer=optimizer,\n",
    "                                                                        hidden_dim=hidden_dim,\n",
    "                                                                        # lr = lr, \n",
    "                                                                        device=device )\n",
    "                lr = 1e-3 if optimizer=='adam' else 1e-2\n",
    "                print([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "                results.append([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "\n",
    "\n",
    "\n",
    "headers = ['Device', 'Batch size', 'Optimizer', 'LR', 'Hidden Dim', \n",
    "           'Training Time', 'Train Acc', 'Val Acc', 'Test Acc']\n",
    "df = pd.DataFrame(results, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34b0d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Device</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>LR</th>\n",
       "      <th>Hidden Dim</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>20.822481</td>\n",
       "      <td>81.116667</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>81.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>21.138930</td>\n",
       "      <td>95.725000</td>\n",
       "      <td>94.583333</td>\n",
       "      <td>94.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>22.073092</td>\n",
       "      <td>97.208333</td>\n",
       "      <td>95.566667</td>\n",
       "      <td>96.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>22.682774</td>\n",
       "      <td>98.100000</td>\n",
       "      <td>96.591667</td>\n",
       "      <td>96.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>4</td>\n",
       "      <td>19.016016</td>\n",
       "      <td>69.941667</td>\n",
       "      <td>70.083333</td>\n",
       "      <td>70.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>19.240644</td>\n",
       "      <td>89.883333</td>\n",
       "      <td>89.666667</td>\n",
       "      <td>90.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>19.526299</td>\n",
       "      <td>90.029167</td>\n",
       "      <td>89.866667</td>\n",
       "      <td>90.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>20.448700</td>\n",
       "      <td>90.037500</td>\n",
       "      <td>89.850000</td>\n",
       "      <td>90.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>18.587870</td>\n",
       "      <td>74.941667</td>\n",
       "      <td>73.933333</td>\n",
       "      <td>75.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>18.807421</td>\n",
       "      <td>95.029167</td>\n",
       "      <td>94.341667</td>\n",
       "      <td>94.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>19.121340</td>\n",
       "      <td>96.412500</td>\n",
       "      <td>95.416667</td>\n",
       "      <td>95.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>20.281624</td>\n",
       "      <td>97.404167</td>\n",
       "      <td>96.433333</td>\n",
       "      <td>96.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>4</td>\n",
       "      <td>17.898255</td>\n",
       "      <td>63.181250</td>\n",
       "      <td>63.475000</td>\n",
       "      <td>64.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>17.976289</td>\n",
       "      <td>87.287500</td>\n",
       "      <td>87.291667</td>\n",
       "      <td>88.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>18.428340</td>\n",
       "      <td>87.922917</td>\n",
       "      <td>87.775000</td>\n",
       "      <td>88.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>18.753883</td>\n",
       "      <td>87.927083</td>\n",
       "      <td>87.900000</td>\n",
       "      <td>88.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>17.380155</td>\n",
       "      <td>52.650000</td>\n",
       "      <td>52.416667</td>\n",
       "      <td>53.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>17.465012</td>\n",
       "      <td>90.939583</td>\n",
       "      <td>90.625000</td>\n",
       "      <td>91.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>17.708344</td>\n",
       "      <td>92.270833</td>\n",
       "      <td>91.866667</td>\n",
       "      <td>92.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>18.143819</td>\n",
       "      <td>93.297917</td>\n",
       "      <td>92.766667</td>\n",
       "      <td>93.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>4</td>\n",
       "      <td>17.723570</td>\n",
       "      <td>20.575000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>23.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>17.807459</td>\n",
       "      <td>62.589583</td>\n",
       "      <td>63.075000</td>\n",
       "      <td>65.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>18.086973</td>\n",
       "      <td>67.327083</td>\n",
       "      <td>66.508333</td>\n",
       "      <td>68.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>18.751791</td>\n",
       "      <td>70.679167</td>\n",
       "      <td>70.291667</td>\n",
       "      <td>71.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Device  Batch size Optimizer     LR  Hidden Dim  Training Time  Train Acc  \\\n",
       "0     cpu          64      adam  0.001           4      20.822481  81.116667   \n",
       "1     cpu          64      adam  0.001          32      21.138930  95.725000   \n",
       "2     cpu          64      adam  0.001          64      22.073092  97.208333   \n",
       "3     cpu          64      adam  0.001         128      22.682774  98.100000   \n",
       "4     cpu          64       sgd  0.010           4      19.016016  69.941667   \n",
       "5     cpu          64       sgd  0.010          32      19.240644  89.883333   \n",
       "6     cpu          64       sgd  0.010          64      19.526299  90.029167   \n",
       "7     cpu          64       sgd  0.010         128      20.448700  90.037500   \n",
       "8     cpu         128      adam  0.001           4      18.587870  74.941667   \n",
       "9     cpu         128      adam  0.001          32      18.807421  95.029167   \n",
       "10    cpu         128      adam  0.001          64      19.121340  96.412500   \n",
       "11    cpu         128      adam  0.001         128      20.281624  97.404167   \n",
       "12    cpu         128       sgd  0.010           4      17.898255  63.181250   \n",
       "13    cpu         128       sgd  0.010          32      17.976289  87.287500   \n",
       "14    cpu         128       sgd  0.010          64      18.428340  87.922917   \n",
       "15    cpu         128       sgd  0.010         128      18.753883  87.927083   \n",
       "16    cpu        1024      adam  0.001           4      17.380155  52.650000   \n",
       "17    cpu        1024      adam  0.001          32      17.465012  90.939583   \n",
       "18    cpu        1024      adam  0.001          64      17.708344  92.270833   \n",
       "19    cpu        1024      adam  0.001         128      18.143819  93.297917   \n",
       "20    cpu        1024       sgd  0.010           4      17.723570  20.575000   \n",
       "21    cpu        1024       sgd  0.010          32      17.807459  62.589583   \n",
       "22    cpu        1024       sgd  0.010          64      18.086973  67.327083   \n",
       "23    cpu        1024       sgd  0.010         128      18.751791  70.679167   \n",
       "\n",
       "      Val Acc  Test Acc  \n",
       "0   80.500000     81.34  \n",
       "1   94.583333     94.83  \n",
       "2   95.566667     96.09  \n",
       "3   96.591667     96.87  \n",
       "4   70.083333     70.49  \n",
       "5   89.666667     90.33  \n",
       "6   89.866667     90.57  \n",
       "7   89.850000     90.69  \n",
       "8   73.933333     75.06  \n",
       "9   94.341667     94.47  \n",
       "10  95.416667     95.75  \n",
       "11  96.433333     96.45  \n",
       "12  63.475000     64.90  \n",
       "13  87.291667     88.10  \n",
       "14  87.775000     88.92  \n",
       "15  87.900000     88.73  \n",
       "16  52.416667     53.09  \n",
       "17  90.625000     91.21  \n",
       "18  91.866667     92.49  \n",
       "19  92.766667     93.21  \n",
       "20  22.000000     23.66  \n",
       "21  63.075000     65.17  \n",
       "22  66.508333     68.73  \n",
       "23  70.291667     71.88  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('sigmoid_hyperopt.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6558a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sigmoid_hyperopt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e048ebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rlrlrrrrrr}\n",
      "\\toprule\n",
      "Unnamed: 0 & Device & Batch size & Optimizer & LR & Hidden Dim & Training Time & Train Acc & Val Acc & Test Acc \\\\\n",
      "\\midrule\n",
      "0 & cpu & 64 & adam & 0.001000 & 4 & 20.822481 & 81.116667 & 80.500000 & 81.340000 \\\\\n",
      "1 & cpu & 64 & adam & 0.001000 & 32 & 21.138930 & 95.725000 & 94.583333 & 94.830000 \\\\\n",
      "2 & cpu & 64 & adam & 0.001000 & 64 & 22.073092 & 97.208333 & 95.566667 & 96.090000 \\\\\n",
      "3 & cpu & 64 & adam & 0.001000 & 128 & 22.682774 & 98.100000 & 96.591667 & 96.870000 \\\\\n",
      "4 & cpu & 64 & sgd & 0.010000 & 4 & 19.016016 & 69.941667 & 70.083333 & 70.490000 \\\\\n",
      "5 & cpu & 64 & sgd & 0.010000 & 32 & 19.240644 & 89.883333 & 89.666667 & 90.330000 \\\\\n",
      "6 & cpu & 64 & sgd & 0.010000 & 64 & 19.526299 & 90.029167 & 89.866667 & 90.570000 \\\\\n",
      "7 & cpu & 64 & sgd & 0.010000 & 128 & 20.448700 & 90.037500 & 89.850000 & 90.690000 \\\\\n",
      "8 & cpu & 128 & adam & 0.001000 & 4 & 18.587870 & 74.941667 & 73.933333 & 75.060000 \\\\\n",
      "9 & cpu & 128 & adam & 0.001000 & 32 & 18.807421 & 95.029167 & 94.341667 & 94.470000 \\\\\n",
      "10 & cpu & 128 & adam & 0.001000 & 64 & 19.121340 & 96.412500 & 95.416667 & 95.750000 \\\\\n",
      "11 & cpu & 128 & adam & 0.001000 & 128 & 20.281624 & 97.404167 & 96.433333 & 96.450000 \\\\\n",
      "12 & cpu & 128 & sgd & 0.010000 & 4 & 17.898255 & 63.181250 & 63.475000 & 64.900000 \\\\\n",
      "13 & cpu & 128 & sgd & 0.010000 & 32 & 17.976289 & 87.287500 & 87.291667 & 88.100000 \\\\\n",
      "14 & cpu & 128 & sgd & 0.010000 & 64 & 18.428340 & 87.922917 & 87.775000 & 88.920000 \\\\\n",
      "15 & cpu & 128 & sgd & 0.010000 & 128 & 18.753883 & 87.927083 & 87.900000 & 88.730000 \\\\\n",
      "16 & cpu & 1024 & adam & 0.001000 & 4 & 17.380155 & 52.650000 & 52.416667 & 53.090000 \\\\\n",
      "17 & cpu & 1024 & adam & 0.001000 & 32 & 17.465012 & 90.939583 & 90.625000 & 91.210000 \\\\\n",
      "18 & cpu & 1024 & adam & 0.001000 & 64 & 17.708344 & 92.270833 & 91.866667 & 92.490000 \\\\\n",
      "19 & cpu & 1024 & adam & 0.001000 & 128 & 18.143819 & 93.297917 & 92.766667 & 93.210000 \\\\\n",
      "20 & cpu & 1024 & sgd & 0.010000 & 4 & 17.723570 & 20.575000 & 22.000000 & 23.660000 \\\\\n",
      "21 & cpu & 1024 & sgd & 0.010000 & 32 & 17.807459 & 62.589583 & 63.075000 & 65.170000 \\\\\n",
      "22 & cpu & 1024 & sgd & 0.010000 & 64 & 18.086973 & 67.327083 & 66.508333 & 68.730000 \\\\\n",
      "23 & cpu & 1024 & sgd & 0.010000 & 128 & 18.751791 & 70.679167 & 70.291667 & 71.880000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/Question/Q2/sigmoid_hyperopt.csv')\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e10866be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(MulticlassMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "908312df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0675\n",
      "Epoch 2, Loss: 1.9844\n",
      "Epoch 3, Loss: 1.7940\n",
      "Epoch 4, Loss: 1.6219\n",
      "Epoch 5, Loss: 1.6827\n",
      "Epoch 6, Loss: 1.6457\n",
      "Epoch 7, Loss: 1.6914\n",
      "Epoch 8, Loss: 1.5235\n",
      "Epoch 9, Loss: 1.7797\n",
      "Epoch 10, Loss: 1.5669\n",
      "hyperopt run done\n",
      "['cpu', 64, 'adam', 0.001, 4, 22.202066898345947, 33.3625, 33.99166666666667, 34.13]\n",
      "Epoch 1, Loss: 0.2392\n",
      "Epoch 2, Loss: 0.2426\n",
      "Epoch 3, Loss: 0.2713\n",
      "Epoch 4, Loss: 0.4272\n",
      "Epoch 5, Loss: 0.1802\n",
      "Epoch 6, Loss: 0.2217\n",
      "Epoch 7, Loss: 0.3104\n",
      "Epoch 8, Loss: 0.0754\n",
      "Epoch 9, Loss: 0.0838\n",
      "Epoch 10, Loss: 0.0959\n",
      "hyperopt run done\n",
      "['cpu', 64, 'adam', 0.001, 32, 22.753937244415283, 95.64791666666666, 94.56666666666666, 95.12]\n",
      "Epoch 1, Loss: 0.4181\n",
      "Epoch 2, Loss: 0.0817\n",
      "Epoch 3, Loss: 0.2968\n",
      "Epoch 4, Loss: 0.2856\n",
      "Epoch 5, Loss: 0.1629\n",
      "Epoch 6, Loss: 0.1173\n",
      "Epoch 7, Loss: 0.1449\n",
      "Epoch 8, Loss: 0.0339\n",
      "Epoch 9, Loss: 0.0786\n",
      "Epoch 10, Loss: 0.0486\n",
      "hyperopt run done\n",
      "['cpu', 64, 'adam', 0.001, 64, 23.67895817756653, 97.19166666666666, 96.525, 96.8]\n",
      "Epoch 1, Loss: 0.4118\n",
      "Epoch 2, Loss: 0.0782\n",
      "Epoch 3, Loss: 0.1094\n",
      "Epoch 4, Loss: 0.1519\n",
      "Epoch 5, Loss: 0.2429\n",
      "Epoch 6, Loss: 0.1242\n",
      "Epoch 7, Loss: 0.1099\n",
      "Epoch 8, Loss: 0.0632\n",
      "Epoch 9, Loss: 0.1031\n",
      "Epoch 10, Loss: 0.0271\n",
      "hyperopt run done\n",
      "['cpu', 64, 'adam', 0.001, 128, 24.853068113327026, 98.10833333333333, 96.425, 96.53]\n",
      "Epoch 1, Loss: 1.1978\n",
      "Epoch 2, Loss: 0.7631\n",
      "Epoch 3, Loss: 0.6916\n",
      "Epoch 4, Loss: 0.6713\n",
      "Epoch 5, Loss: 0.7640\n",
      "Epoch 6, Loss: 0.5983\n",
      "Epoch 7, Loss: 0.6145\n",
      "Epoch 8, Loss: 0.5952\n",
      "Epoch 9, Loss: 0.5461\n",
      "Epoch 10, Loss: 0.5250\n",
      "hyperopt run done\n",
      "['cpu', 64, 'sgd', 0.01, 4, 21.19923996925354, 82.97916666666667, 82.09166666666667, 82.65]\n",
      "Epoch 1, Loss: 0.3320\n",
      "Epoch 2, Loss: 0.2166\n",
      "Epoch 3, Loss: 0.2717\n",
      "Epoch 4, Loss: 0.1923\n",
      "Epoch 5, Loss: 0.4238\n",
      "Epoch 6, Loss: 0.2018\n",
      "Epoch 7, Loss: 0.3177\n",
      "Epoch 8, Loss: 0.2746\n",
      "Epoch 9, Loss: 0.1653\n",
      "Epoch 10, Loss: 0.0849\n",
      "hyperopt run done\n",
      "['cpu', 64, 'sgd', 0.01, 32, 23.314559936523438, 92.93125, 92.45833333333333, 92.94]\n",
      "Epoch 1, Loss: 0.3758\n",
      "Epoch 2, Loss: 0.3794\n",
      "Epoch 3, Loss: 0.3499\n",
      "Epoch 4, Loss: 0.2959\n",
      "Epoch 5, Loss: 0.2296\n",
      "Epoch 6, Loss: 0.3053\n",
      "Epoch 7, Loss: 0.1676\n",
      "Epoch 8, Loss: 0.2364\n",
      "Epoch 9, Loss: 0.1530\n",
      "Epoch 10, Loss: 0.1767\n",
      "hyperopt run done\n",
      "['cpu', 64, 'sgd', 0.01, 64, 24.677199840545654, 93.55, 93.05, 93.49]\n",
      "Epoch 1, Loss: 0.4950\n",
      "Epoch 2, Loss: 0.3397\n",
      "Epoch 3, Loss: 0.3059\n",
      "Epoch 4, Loss: 0.2751\n",
      "Epoch 5, Loss: 0.2888\n",
      "Epoch 6, Loss: 0.5195\n",
      "Epoch 7, Loss: 0.2425\n",
      "Epoch 8, Loss: 0.3822\n",
      "Epoch 9, Loss: 0.3251\n",
      "Epoch 10, Loss: 0.1354\n",
      "hyperopt run done\n",
      "['cpu', 64, 'sgd', 0.01, 128, 23.671363830566406, 93.88541666666667, 93.60833333333333, 94.1]\n",
      "Epoch 1, Loss: 1.5708\n",
      "Epoch 2, Loss: 1.3490\n",
      "Epoch 3, Loss: 1.3461\n",
      "Epoch 4, Loss: 1.1975\n",
      "Epoch 5, Loss: 1.2064\n",
      "Epoch 6, Loss: 1.2152\n",
      "Epoch 7, Loss: 0.9846\n",
      "Epoch 8, Loss: 1.2073\n",
      "Epoch 9, Loss: 1.2084\n",
      "Epoch 10, Loss: 1.0193\n",
      "hyperopt run done\n",
      "['cpu', 128, 'adam', 0.001, 4, 20.069331169128418, 65.71458333333334, 65.10833333333333, 66.38]\n",
      "Epoch 1, Loss: 0.3782\n",
      "Epoch 2, Loss: 0.1934\n",
      "Epoch 3, Loss: 0.4502\n",
      "Epoch 4, Loss: 0.2419\n",
      "Epoch 5, Loss: 0.2590\n",
      "Epoch 6, Loss: 0.2120\n",
      "Epoch 7, Loss: 0.2842\n",
      "Epoch 8, Loss: 0.1854\n",
      "Epoch 9, Loss: 0.3054\n",
      "Epoch 10, Loss: 0.2506\n",
      "hyperopt run done\n",
      "['cpu', 128, 'adam', 0.001, 32, 21.10404396057129, 93.7875, 92.925, 93.34]\n",
      "Epoch 1, Loss: 0.3942\n",
      "Epoch 2, Loss: 0.4841\n",
      "Epoch 3, Loss: 0.2258\n",
      "Epoch 4, Loss: 0.2811\n",
      "Epoch 5, Loss: 0.2676\n",
      "Epoch 6, Loss: 0.1576\n",
      "Epoch 7, Loss: 0.1378\n",
      "Epoch 8, Loss: 0.1089\n",
      "Epoch 9, Loss: 0.1213\n",
      "Epoch 10, Loss: 0.1959\n",
      "hyperopt run done\n",
      "['cpu', 128, 'adam', 0.001, 64, 22.60029101371765, 96.4375, 95.45, 95.97]\n",
      "Epoch 1, Loss: 0.2597\n",
      "Epoch 2, Loss: 0.1987\n",
      "Epoch 3, Loss: 0.1195\n",
      "Epoch 4, Loss: 0.2855\n",
      "Epoch 5, Loss: 0.0778\n",
      "Epoch 6, Loss: 0.1214\n",
      "Epoch 7, Loss: 0.1079\n",
      "Epoch 8, Loss: 0.0991\n",
      "Epoch 9, Loss: 0.1030\n",
      "Epoch 10, Loss: 0.0360\n",
      "hyperopt run done\n",
      "['cpu', 128, 'adam', 0.001, 128, 24.154393911361694, 97.73541666666667, 96.3, 96.72]\n",
      "Epoch 1, Loss: 1.2693\n",
      "Epoch 2, Loss: 0.8396\n",
      "Epoch 3, Loss: 1.0207\n",
      "Epoch 4, Loss: 0.7249\n",
      "Epoch 5, Loss: 0.7804\n",
      "Epoch 6, Loss: 0.6324\n",
      "Epoch 7, Loss: 0.5154\n",
      "Epoch 8, Loss: 0.4797\n",
      "Epoch 9, Loss: 0.5754\n",
      "Epoch 10, Loss: 0.8813\n",
      "hyperopt run done\n",
      "['cpu', 128, 'sgd', 0.01, 4, 20.192016124725342, 81.05625, 80.93333333333334, 81.43]\n",
      "Epoch 1, Loss: 0.8007\n",
      "Epoch 2, Loss: 0.5144\n",
      "Epoch 3, Loss: 0.4731\n",
      "Epoch 4, Loss: 0.2744\n",
      "Epoch 5, Loss: 0.3839\n",
      "Epoch 6, Loss: 0.3539\n",
      "Epoch 7, Loss: 0.2939\n",
      "Epoch 8, Loss: 0.3652\n",
      "Epoch 9, Loss: 0.3083\n",
      "Epoch 10, Loss: 0.1743\n",
      "hyperopt run done\n",
      "['cpu', 128, 'sgd', 0.01, 32, 19.052993059158325, 91.44583333333334, 91.43333333333334, 92.05]\n",
      "Epoch 1, Loss: 0.6890\n",
      "Epoch 2, Loss: 0.4530\n",
      "Epoch 3, Loss: 0.4172\n",
      "Epoch 4, Loss: 0.2917\n",
      "Epoch 5, Loss: 0.2891\n",
      "Epoch 6, Loss: 0.3201\n",
      "Epoch 7, Loss: 0.2846\n",
      "Epoch 8, Loss: 0.3490\n",
      "Epoch 9, Loss: 0.2372\n",
      "Epoch 10, Loss: 0.4514\n",
      "hyperopt run done\n",
      "['cpu', 128, 'sgd', 0.01, 64, 19.173183917999268, 91.56666666666666, 91.24166666666666, 91.92]\n",
      "Epoch 1, Loss: 0.7189\n",
      "Epoch 2, Loss: 0.3879\n",
      "Epoch 3, Loss: 0.4072\n",
      "Epoch 4, Loss: 0.2501\n",
      "Epoch 5, Loss: 0.3573\n",
      "Epoch 6, Loss: 0.3991\n",
      "Epoch 7, Loss: 0.1719\n",
      "Epoch 8, Loss: 0.2714\n",
      "Epoch 9, Loss: 0.3415\n",
      "Epoch 10, Loss: 0.2494\n",
      "hyperopt run done\n",
      "['cpu', 128, 'sgd', 0.01, 128, 20.03195595741272, 91.76458333333333, 91.45833333333333, 92.31]\n",
      "Epoch 1, Loss: 2.0572\n",
      "Epoch 2, Loss: 1.9516\n",
      "Epoch 3, Loss: 1.8470\n",
      "Epoch 4, Loss: 1.7215\n",
      "Epoch 5, Loss: 1.6621\n",
      "Epoch 6, Loss: 1.6889\n",
      "Epoch 7, Loss: 1.5957\n",
      "Epoch 8, Loss: 1.5999\n",
      "Epoch 9, Loss: 1.5024\n",
      "Epoch 10, Loss: 1.4903\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'adam', 0.001, 4, 17.754258155822754, 40.58541666666667, 41.825, 41.9]\n",
      "Epoch 1, Loss: 0.8973\n",
      "Epoch 2, Loss: 0.5491\n",
      "Epoch 3, Loss: 0.4262\n",
      "Epoch 4, Loss: 0.3434\n",
      "Epoch 5, Loss: 0.4068\n",
      "Epoch 6, Loss: 0.3191\n",
      "Epoch 7, Loss: 0.3155\n",
      "Epoch 8, Loss: 0.2843\n",
      "Epoch 9, Loss: 0.3071\n",
      "Epoch 10, Loss: 0.3253\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'adam', 0.001, 32, 17.71076512336731, 91.39583333333333, 91.46666666666667, 91.97]\n",
      "Epoch 1, Loss: 0.6585\n",
      "Epoch 2, Loss: 0.4013\n",
      "Epoch 3, Loss: 0.3571\n",
      "Epoch 4, Loss: 0.2998\n",
      "Epoch 5, Loss: 0.2997\n",
      "Epoch 6, Loss: 0.2880\n",
      "Epoch 7, Loss: 0.2865\n",
      "Epoch 8, Loss: 0.2718\n",
      "Epoch 9, Loss: 0.2360\n",
      "Epoch 10, Loss: 0.2690\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'adam', 0.001, 64, 18.001580953598022, 93.0375, 92.70833333333333, 93.05]\n",
      "Epoch 1, Loss: 0.4030\n",
      "Epoch 2, Loss: 0.3503\n",
      "Epoch 3, Loss: 0.2898\n",
      "Epoch 4, Loss: 0.3450\n",
      "Epoch 5, Loss: 0.2581\n",
      "Epoch 6, Loss: 0.2534\n",
      "Epoch 7, Loss: 0.2253\n",
      "Epoch 8, Loss: 0.2204\n",
      "Epoch 9, Loss: 0.2016\n",
      "Epoch 10, Loss: 0.1955\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'adam', 0.001, 128, 18.1184241771698, 94.625, 94.15833333333333, 94.6]\n",
      "Epoch 1, Loss: 2.1223\n",
      "Epoch 2, Loss: 2.0015\n",
      "Epoch 3, Loss: 1.9215\n",
      "Epoch 4, Loss: 1.7987\n",
      "Epoch 5, Loss: 1.7206\n",
      "Epoch 6, Loss: 1.6452\n",
      "Epoch 7, Loss: 1.5967\n",
      "Epoch 8, Loss: 1.5305\n",
      "Epoch 9, Loss: 1.4670\n",
      "Epoch 10, Loss: 1.3855\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'sgd', 0.01, 4, 17.477235317230225, 53.31875, 53.15, 54.65]\n",
      "Epoch 1, Loss: 2.0315\n",
      "Epoch 2, Loss: 1.7016\n",
      "Epoch 3, Loss: 1.3986\n",
      "Epoch 4, Loss: 1.1698\n",
      "Epoch 5, Loss: 1.0123\n",
      "Epoch 6, Loss: 0.8517\n",
      "Epoch 7, Loss: 0.7349\n",
      "Epoch 8, Loss: 0.6868\n",
      "Epoch 9, Loss: 0.6075\n",
      "Epoch 10, Loss: 0.5851\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'sgd', 0.01, 32, 17.53837013244629, 85.19166666666666, 84.94166666666666, 85.86]\n",
      "Epoch 1, Loss: 1.9453\n",
      "Epoch 2, Loss: 1.5236\n",
      "Epoch 3, Loss: 1.1598\n",
      "Epoch 4, Loss: 0.9730\n",
      "Epoch 5, Loss: 0.8442\n",
      "Epoch 6, Loss: 0.7225\n",
      "Epoch 7, Loss: 0.6683\n",
      "Epoch 8, Loss: 0.6098\n",
      "Epoch 9, Loss: 0.5702\n",
      "Epoch 10, Loss: 0.5808\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'sgd', 0.01, 64, 17.841422080993652, 86.21458333333334, 86.075, 87.12]\n",
      "Epoch 1, Loss: 1.9070\n",
      "Epoch 2, Loss: 1.4817\n",
      "Epoch 3, Loss: 1.1829\n",
      "Epoch 4, Loss: 0.9224\n",
      "Epoch 5, Loss: 0.8006\n",
      "Epoch 6, Loss: 0.6864\n",
      "Epoch 7, Loss: 0.6603\n",
      "Epoch 8, Loss: 0.6011\n",
      "Epoch 9, Loss: 0.5870\n",
      "Epoch 10, Loss: 0.5709\n",
      "hyperopt run done\n",
      "['cpu', 1024, 'sgd', 0.01, 128, 18.008393049240112, 86.64583333333333, 86.49166666666666, 87.38]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Device</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>LR</th>\n",
       "      <th>Hidden Dim</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>22.202067</td>\n",
       "      <td>33.362500</td>\n",
       "      <td>33.991667</td>\n",
       "      <td>34.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>22.753937</td>\n",
       "      <td>95.647917</td>\n",
       "      <td>94.566667</td>\n",
       "      <td>95.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>23.678958</td>\n",
       "      <td>97.191667</td>\n",
       "      <td>96.525000</td>\n",
       "      <td>96.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>24.853068</td>\n",
       "      <td>98.108333</td>\n",
       "      <td>96.425000</td>\n",
       "      <td>96.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>4</td>\n",
       "      <td>21.199240</td>\n",
       "      <td>82.979167</td>\n",
       "      <td>82.091667</td>\n",
       "      <td>82.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>23.314560</td>\n",
       "      <td>92.931250</td>\n",
       "      <td>92.458333</td>\n",
       "      <td>92.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>24.677200</td>\n",
       "      <td>93.550000</td>\n",
       "      <td>93.050000</td>\n",
       "      <td>93.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cpu</td>\n",
       "      <td>64</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>23.671364</td>\n",
       "      <td>93.885417</td>\n",
       "      <td>93.608333</td>\n",
       "      <td>94.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>20.069331</td>\n",
       "      <td>65.714583</td>\n",
       "      <td>65.108333</td>\n",
       "      <td>66.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>21.104044</td>\n",
       "      <td>93.787500</td>\n",
       "      <td>92.925000</td>\n",
       "      <td>93.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>22.600291</td>\n",
       "      <td>96.437500</td>\n",
       "      <td>95.450000</td>\n",
       "      <td>95.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>24.154394</td>\n",
       "      <td>97.735417</td>\n",
       "      <td>96.300000</td>\n",
       "      <td>96.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>4</td>\n",
       "      <td>20.192016</td>\n",
       "      <td>81.056250</td>\n",
       "      <td>80.933333</td>\n",
       "      <td>81.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>19.052993</td>\n",
       "      <td>91.445833</td>\n",
       "      <td>91.433333</td>\n",
       "      <td>92.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>19.173184</td>\n",
       "      <td>91.566667</td>\n",
       "      <td>91.241667</td>\n",
       "      <td>91.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cpu</td>\n",
       "      <td>128</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>20.031956</td>\n",
       "      <td>91.764583</td>\n",
       "      <td>91.458333</td>\n",
       "      <td>92.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>17.754258</td>\n",
       "      <td>40.585417</td>\n",
       "      <td>41.825000</td>\n",
       "      <td>41.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>32</td>\n",
       "      <td>17.710765</td>\n",
       "      <td>91.395833</td>\n",
       "      <td>91.466667</td>\n",
       "      <td>91.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>64</td>\n",
       "      <td>18.001581</td>\n",
       "      <td>93.037500</td>\n",
       "      <td>92.708333</td>\n",
       "      <td>93.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.001</td>\n",
       "      <td>128</td>\n",
       "      <td>18.118424</td>\n",
       "      <td>94.625000</td>\n",
       "      <td>94.158333</td>\n",
       "      <td>94.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>4</td>\n",
       "      <td>17.477235</td>\n",
       "      <td>53.318750</td>\n",
       "      <td>53.150000</td>\n",
       "      <td>54.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>32</td>\n",
       "      <td>17.538370</td>\n",
       "      <td>85.191667</td>\n",
       "      <td>84.941667</td>\n",
       "      <td>85.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>64</td>\n",
       "      <td>17.841422</td>\n",
       "      <td>86.214583</td>\n",
       "      <td>86.075000</td>\n",
       "      <td>87.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>cpu</td>\n",
       "      <td>1024</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.010</td>\n",
       "      <td>128</td>\n",
       "      <td>18.008393</td>\n",
       "      <td>86.645833</td>\n",
       "      <td>86.491667</td>\n",
       "      <td>87.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Device  Batch size Optimizer     LR  Hidden Dim  Training Time  Train Acc  \\\n",
       "0     cpu          64      adam  0.001           4      22.202067  33.362500   \n",
       "1     cpu          64      adam  0.001          32      22.753937  95.647917   \n",
       "2     cpu          64      adam  0.001          64      23.678958  97.191667   \n",
       "3     cpu          64      adam  0.001         128      24.853068  98.108333   \n",
       "4     cpu          64       sgd  0.010           4      21.199240  82.979167   \n",
       "5     cpu          64       sgd  0.010          32      23.314560  92.931250   \n",
       "6     cpu          64       sgd  0.010          64      24.677200  93.550000   \n",
       "7     cpu          64       sgd  0.010         128      23.671364  93.885417   \n",
       "8     cpu         128      adam  0.001           4      20.069331  65.714583   \n",
       "9     cpu         128      adam  0.001          32      21.104044  93.787500   \n",
       "10    cpu         128      adam  0.001          64      22.600291  96.437500   \n",
       "11    cpu         128      adam  0.001         128      24.154394  97.735417   \n",
       "12    cpu         128       sgd  0.010           4      20.192016  81.056250   \n",
       "13    cpu         128       sgd  0.010          32      19.052993  91.445833   \n",
       "14    cpu         128       sgd  0.010          64      19.173184  91.566667   \n",
       "15    cpu         128       sgd  0.010         128      20.031956  91.764583   \n",
       "16    cpu        1024      adam  0.001           4      17.754258  40.585417   \n",
       "17    cpu        1024      adam  0.001          32      17.710765  91.395833   \n",
       "18    cpu        1024      adam  0.001          64      18.001581  93.037500   \n",
       "19    cpu        1024      adam  0.001         128      18.118424  94.625000   \n",
       "20    cpu        1024       sgd  0.010           4      17.477235  53.318750   \n",
       "21    cpu        1024       sgd  0.010          32      17.538370  85.191667   \n",
       "22    cpu        1024       sgd  0.010          64      17.841422  86.214583   \n",
       "23    cpu        1024       sgd  0.010         128      18.008393  86.645833   \n",
       "\n",
       "      Val Acc  Test Acc  \n",
       "0   33.991667     34.13  \n",
       "1   94.566667     95.12  \n",
       "2   96.525000     96.80  \n",
       "3   96.425000     96.53  \n",
       "4   82.091667     82.65  \n",
       "5   92.458333     92.94  \n",
       "6   93.050000     93.49  \n",
       "7   93.608333     94.10  \n",
       "8   65.108333     66.38  \n",
       "9   92.925000     93.34  \n",
       "10  95.450000     95.97  \n",
       "11  96.300000     96.72  \n",
       "12  80.933333     81.43  \n",
       "13  91.433333     92.05  \n",
       "14  91.241667     91.92  \n",
       "15  91.458333     92.31  \n",
       "16  41.825000     41.90  \n",
       "17  91.466667     91.97  \n",
       "18  92.708333     93.05  \n",
       "19  94.158333     94.60  \n",
       "20  53.150000     54.65  \n",
       "21  84.941667     85.86  \n",
       "22  86.075000     87.12  \n",
       "23  86.491667     87.38  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "devices = ['cpu']\n",
    "batch_sizes = [64, 128, 1024]\n",
    "optimizers = ['adam', 'sgd']\n",
    "# learning_rates= [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "hidden_dims = [4, 32, 64, 128]\n",
    "for batch_size in batch_sizes:\n",
    "    for optimizer in optimizers:\n",
    "        for device in devices:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                training_time, train_acc, val_acc, test_acc = ten_digit(batch_size=batch_size, \n",
    "                                                                        optimizer=optimizer,\n",
    "                                                                        hidden_dim=hidden_dim,\n",
    "                                                                        # lr = lr, \n",
    "                                                                        device=device )\n",
    "                lr = 1e-3 if optimizer=='adam' else 1e-2\n",
    "                print([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "                results.append([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "\n",
    "\n",
    "\n",
    "headers = ['Device', 'Batch size', 'Optimizer', 'LR', 'Hidden Dim', \n",
    "           'Training Time', 'Train Acc', 'Val Acc', 'Test Acc']\n",
    "df = pd.DataFrame(results, columns=headers)\n",
    "df.to_csv('relu_hyperopt_q2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3999f34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rlrlrrrrrr}\n",
      "\\toprule\n",
      "Unnamed: 0 & Device & Batch size & Optimizer & LR & Hidden Dim & Training Time & Train Acc & Val Acc & Test Acc \\\\\n",
      "\\midrule\n",
      "0 & cpu & 64 & adam & 0.001000 & 4 & 22.202067 & 33.362500 & 33.991667 & 34.130000 \\\\\n",
      "1 & cpu & 64 & adam & 0.001000 & 32 & 22.753937 & 95.647917 & 94.566667 & 95.120000 \\\\\n",
      "2 & cpu & 64 & adam & 0.001000 & 64 & 23.678958 & 97.191667 & 96.525000 & 96.800000 \\\\\n",
      "3 & cpu & 64 & adam & 0.001000 & 128 & 24.853068 & 98.108333 & 96.425000 & 96.530000 \\\\\n",
      "4 & cpu & 64 & sgd & 0.010000 & 4 & 21.199240 & 82.979167 & 82.091667 & 82.650000 \\\\\n",
      "5 & cpu & 64 & sgd & 0.010000 & 32 & 23.314560 & 92.931250 & 92.458333 & 92.940000 \\\\\n",
      "6 & cpu & 64 & sgd & 0.010000 & 64 & 24.677200 & 93.550000 & 93.050000 & 93.490000 \\\\\n",
      "7 & cpu & 64 & sgd & 0.010000 & 128 & 23.671364 & 93.885417 & 93.608333 & 94.100000 \\\\\n",
      "8 & cpu & 128 & adam & 0.001000 & 4 & 20.069331 & 65.714583 & 65.108333 & 66.380000 \\\\\n",
      "9 & cpu & 128 & adam & 0.001000 & 32 & 21.104044 & 93.787500 & 92.925000 & 93.340000 \\\\\n",
      "10 & cpu & 128 & adam & 0.001000 & 64 & 22.600291 & 96.437500 & 95.450000 & 95.970000 \\\\\n",
      "11 & cpu & 128 & adam & 0.001000 & 128 & 24.154394 & 97.735417 & 96.300000 & 96.720000 \\\\\n",
      "12 & cpu & 128 & sgd & 0.010000 & 4 & 20.192016 & 81.056250 & 80.933333 & 81.430000 \\\\\n",
      "13 & cpu & 128 & sgd & 0.010000 & 32 & 19.052993 & 91.445833 & 91.433333 & 92.050000 \\\\\n",
      "14 & cpu & 128 & sgd & 0.010000 & 64 & 19.173184 & 91.566667 & 91.241667 & 91.920000 \\\\\n",
      "15 & cpu & 128 & sgd & 0.010000 & 128 & 20.031956 & 91.764583 & 91.458333 & 92.310000 \\\\\n",
      "16 & cpu & 1024 & adam & 0.001000 & 4 & 17.754258 & 40.585417 & 41.825000 & 41.900000 \\\\\n",
      "17 & cpu & 1024 & adam & 0.001000 & 32 & 17.710765 & 91.395833 & 91.466667 & 91.970000 \\\\\n",
      "18 & cpu & 1024 & adam & 0.001000 & 64 & 18.001581 & 93.037500 & 92.708333 & 93.050000 \\\\\n",
      "19 & cpu & 1024 & adam & 0.001000 & 128 & 18.118424 & 94.625000 & 94.158333 & 94.600000 \\\\\n",
      "20 & cpu & 1024 & sgd & 0.010000 & 4 & 17.477235 & 53.318750 & 53.150000 & 54.650000 \\\\\n",
      "21 & cpu & 1024 & sgd & 0.010000 & 32 & 17.538370 & 85.191667 & 84.941667 & 85.860000 \\\\\n",
      "22 & cpu & 1024 & sgd & 0.010000 & 64 & 17.841422 & 86.214583 & 86.075000 & 87.120000 \\\\\n",
      "23 & cpu & 1024 & sgd & 0.010000 & 128 & 18.008393 & 86.645833 & 86.491667 & 87.380000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/Question/Q2/relu_hyperopt_q2.csv')\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31411a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
