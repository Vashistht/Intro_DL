{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 1: Simple MLP for Binary Classification\n",
    "In this problem, you will train a simple MLP to classify two handwritten digits: 0 vs 1. We provide some starter codes to do this task with steps. However, you do not need to follow the exact steps as long as you can complete the task in sections marked as <span style=\"color:red\">[YOUR TASK]</span>.\n",
    "\n",
    "## Dataset Setup\n",
    "We will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The `torchvision` package has supported this dataset. We can load the dataset in this way (the dataset will take up 63M of your disk space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Link on how to use device\n",
    "https://stackoverflow.com/questions/68820453/how-to-run-pytorch-on-macbook-pro-m1-gpu\n",
    "'''\n",
    "\n",
    "import platform, time\n",
    "print(platform.mac_ver() )\n",
    "print(torch.has_mps)\n",
    "\n",
    "# if not torch.backends.mps.is_available():\n",
    "#     if not torch.backends.mps.is_built():\n",
    "#         print(\"MPS not available because the current PyTorch install was not \"\n",
    "#               \"built with MPS enabled.\")\n",
    "#     else:\n",
    "#         print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "#               \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    \n",
    "# else:\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print('mps enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data pre-processing\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "    )\n",
    "\n",
    "# Load the MNIST dataset \n",
    "mnist = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transform)\n",
    "mnist_test = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data',   # './data'\n",
    "                            train=False, \n",
    "                            download=True, \n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Filter for digits 0 and 1\n",
    "train_index = mnist.targets<2\n",
    "mnist.data = mnist.data[train_index]\n",
    "mnist.targets = mnist.targets[train_index]\n",
    "\n",
    "test_index = mnist_test.targets<2\n",
    "mnist_test.data = mnist_test.data[test_index]\n",
    "mnist_test.targets = mnist_test.targets[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "train_len = int(len(mnist) *.8)\n",
    "val_len = len(mnist) - train_len\n",
    "train_set, val_set = random_split(mnist, [train_len, val_len])\n",
    "\n",
    "# Define DataLoaders to access data in batches\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size = 64, shuffle=False)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Your code goes here\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)        \n",
    "        return x\n",
    "\n",
    "hidden_dim = 5\n",
    "model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the MLP\n",
    "To train the model, we need to define a loss function (criterion) and an optimizer. The loss function tells us how far away the model’s prediction is from the label. Once we have the loss, PyTorch can compute the gradient of the model automatically. The optimizer uses the gradient to update the model. For classification problems, we often use the Cross Entropy Loss. For the optimizer, we can use stochastic gradient descent optimizer or Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several hyper-parameters in the optimizer (please see the [PyTorch document](https://pytorch.org/docs/stable/optim.html) for details). You can play with the hyper-parameters and see how they influence the training.\n",
    "\n",
    "Now we have almost everything to train the model. We provide a sample code to complete the training loops:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform validation after each epoch. But remember not to train (backward and update) on the validation dataset. Use the validation set to optimize performance. After you are done with this, report performance on the test set(You are encouraged not to use the test set for validation, i.e., use the test set only once after you are happy with the validation performance).\n",
    "\n",
    "<span style=\"color:red\">[YOUR TASK]</span>\n",
    "- Filter all samples representing digits \"0\" or \"1\" from the MNIST datasets. \n",
    "- Randomly split the training data into a training set (80\\% training samples) of a validation set (20% training samples).\n",
    "- Define an MLP with 1 hidden layer and train the MLP to classify the digits \"0\" vs \"1\".  Report your MLP design and training details (which optimizer, number of epochs, learning rate, etc.)\n",
    "- Keep other hyper-parameters the same, and train the model with different batch sizes: 2, 16, 128, 1024. Report the time cost, training, validation, and test set accuracy of your model\n",
    "\n",
    "\n",
    "In our implementations, we trained our network for 10 epochs in about 10 seconds on a laptop, getting a test accuracy of 99\\% %.\n",
    "\n",
    "One tip about the hidden layer size is to begin with a small number, say $16\\sim 64$. Some people find $$\\text{hidden size} = \\sqrt{\\text{input size}\\times \\text{output size}}$$ is a good choice in practice. If your model's training accuracy is too low, you can double the hidden layer size. However, if you find the training accuracy is high. Still, the validation accuracy is much lower, you may consider a smaller hidden layer size because your model has the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    correct, count = 0, 0 \n",
    "    for data, target in train_loader:\n",
    "        # free the gradient from the previous batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # reshape the image into a vector\n",
    "        data = data.view(data.size(0), -1)\n",
    "        # model forward\n",
    "        output = model(data)\n",
    "        # compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        # model backward\n",
    "        loss.backward()\n",
    "        # update the model paramters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # adding this for train accuracy \n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        count += data.size(0)\n",
    "    \n",
    "    train_acc = 100. * correct / count\n",
    "    print(f'Training accuracy: {train_acc:.2f}%')\n",
    "\n",
    "training_time = time.time()- start_time\n",
    "print(training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "val_loss = count = 0\n",
    "correct = total = 0\n",
    "for data, target in val_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    data = data.view(data.size(0), -1)\n",
    "    output = model(data)\n",
    "    val_loss += criterion(output, target).item()\n",
    "    count += 1\n",
    "    pred = output.argmax(dim=1)\n",
    "    correct += (pred == target).sum().item()\n",
    "    total += data.size(0)\n",
    "    \n",
    "val_loss = val_loss / count\n",
    "val_acc = 100. * correct / total\n",
    "print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        \n",
    "test_acc = 100. * correct / total\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running it for different batch sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Two digit is a function I defined to be able to test for different batch sizes\n",
    "'''\n",
    "def two_digit(batch_size):\n",
    "    # Define DataLoaders to access data in batches\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # Your code goes here\n",
    "    val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "        train_acc = 100. * correct / count\n",
    "    training_time = time.time()- start_time\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            \n",
    "    test_acc = 100. * correct / total\n",
    "    print('Hyperopt run done')\n",
    "    return training_time, train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [2, 16, 128, 1024]\n",
    "results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    training_time, train_acc, val_acc, test_acc = two_digit(batch_size=batch_size)\n",
    "    results.append([batch_size,training_time, train_acc, val_acc, test_acc])\n",
    "\n",
    "# writing and saving the results to a csv\n",
    "headers = ['Batch size', 'Training Time ', 'Train Acc' ,' Val Acc', 'Test Acc']\n",
    "df =  pd.DataFrame(results, columns = headers)\n",
    "df.to_csv('question_1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the latex code for the table \n",
    "df = pd.read_csv('question_1.csv')\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: MNIST 10-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train an MLP to handle multi-class classification for all 10 digits in the MNIST dataset. We will use the full MNIST dataset without filtering for specific digits. You may modify the MLP so that it can be used for multi-class classification.\n",
    "\n",
    "- Implement the training loop and evaluation section. Report the hyper-parameters you choose.\n",
    "- Experiment with different numbers of neurons in the hidden layer and note any changes in performance.\n",
    "- Write a brief analysis of the model's performance, including any challenges faced and how they were addressed.\n",
    "\n",
    "In our implementations, we trained our network for 10 epochs in about 20 seconds on a laptop.\n",
    "When you define a new model, remember to update the optimizer!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoind activation MLP\n",
    "class MulticlassMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(MulticlassMLP, self).__init__()\n",
    "        # Your code goes here\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "hidden_dim = int(np.sqrt(28*28*10))\n",
    "model = MulticlassMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ten digit is a function I defined to be able to test for different hyper parameters\n",
    "Input: Different hyper parameters\n",
    "    - the function takes in the device (cpu vs mps) to see the difference inthe training times\n",
    "    - hidden dimensions to vary the complexity of the model\n",
    "    - optimizer (sgd at lr of 1e-2 and adam at lr 1e-3)\n",
    "'''\n",
    "\n",
    "def ten_digit(batch_size, hidden_dim, optimizer,  device = 'cpu'): # or mps lr=1e-3,\n",
    "    device = torch.device(device)\n",
    "    # Define DataLoaders to access data in batches\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    model = MulticlassMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        lr = 1e-3\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        lr=1e-2\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    training_time = time.time()- start_time\n",
    "    # print(training_time)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            \n",
    "    test_acc = 100. * correct / total\n",
    "    print('hyperopt run done')\n",
    "    return training_time, train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping over different combinations of hyper parameters\n",
    "results = []\n",
    "devices = ['cpu']\n",
    "batch_sizes = [64, 128, 1024]\n",
    "optimizers = ['adam', 'sgd']\n",
    "# learning_rates= [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "hidden_dims = [4, 32, 64, 128]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for optimizer in optimizers:\n",
    "        for device in devices:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                training_time, train_acc, val_acc, test_acc = ten_digit(batch_size=batch_size, \n",
    "                                                                        optimizer=optimizer,\n",
    "                                                                        hidden_dim=hidden_dim,\n",
    "                                                                        # lr = lr, \n",
    "                                                                        device=device )\n",
    "                lr = 1e-3 if optimizer=='adam' else 1e-2\n",
    "                print([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "                results.append([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "\n",
    "headers = ['Device', 'Batch size', 'Optimizer', 'LR', 'Hidden Dim', \n",
    "           'Training Time', 'Train Acc', 'Val Acc', 'Test Acc']\n",
    "df = pd.DataFrame(results, columns=headers)\n",
    "df.to_csv('sigmoid_hyperopt.csv')\n",
    "df = pd.read_csv('/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/Question/Q2/sigmoid_hyperopt.csv')\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function MLP\n",
    "\n",
    "class MulticlassMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(MulticlassMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "devices = ['cpu']\n",
    "batch_sizes = [64, 128, 1024]\n",
    "optimizers = ['adam', 'sgd']\n",
    "# learning_rates= [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "hidden_dims = [4, 32, 64, 128]\n",
    "for batch_size in batch_sizes:\n",
    "    for optimizer in optimizers:\n",
    "        for device in devices:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                training_time, train_acc, val_acc, test_acc = ten_digit(batch_size=batch_size, \n",
    "                                                                        optimizer=optimizer,\n",
    "                                                                        hidden_dim=hidden_dim,\n",
    "                                                                        # lr = lr, \n",
    "                                                                        device=device )\n",
    "                lr = 1e-3 if optimizer=='adam' else 1e-2\n",
    "                print([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "                results.append([device, batch_size, optimizer, lr, hidden_dim,  training_time, train_acc, val_acc, test_acc])\n",
    "\n",
    "\n",
    "\n",
    "headers = ['Device', 'Batch size', 'Optimizer', 'LR', 'Hidden Dim', \n",
    "           'Training Time', 'Train Acc', 'Val Acc', 'Test Acc']\n",
    "df = pd.DataFrame(results, columns=headers)\n",
    "df.to_csv('relu_hyperopt_q2.csv')\n",
    "df = pd.read_csv('/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/Question/Q2/relu_hyperopt_q2.csv')\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Handling Class Imbalance in MNIST Dataset\n",
    "In this problem, we will explore how to handle class imbalance problems, which are very common in real-world applications. A modified MNIST dataset is created as follows: we choose all instances of digit “0”, and choose only 1\\% instances of digit “1” for both training and test sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a class imbalance problem, accuracy may not be a good metric. Always predicting \"0\" regardless of the input can be 99\\% accurate. Instead, we use the $F_1$ score as the evaluation metric:\n",
    "$$F_1 = 2\\cdot\\frac{\\text{precision}\\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "where precision and recall are defined as:\n",
    "$$\\text{precision}=\\frac{\\text{number of instances correctly predicted as \"1\"}}{\\text{number of instances predicted as \"1\"}}$$\n",
    "$$\\text{recall}=\\frac{\\text{number of instances correctly predicted as \"1\"}}{\\text{number of instances labeled as \"1\"}}$$\n",
    "\n",
    "To handle such a problem, some changes to the training may be necessary. Some suggestions include: \n",
    "1) Adjusting the class weights in the loss function, i.e., use a larger weight for the minority class when computing the loss.\n",
    "2) Implementing resampling techniques (either undersampling the majority class or oversampling the minority class).\n",
    "\n",
    "<span style=\"color:red\">[YOUR TASK]</span>\n",
    "- Create the imbalance datasets with all \"0\" digits and only 1\\% \"1\" digits.\n",
    "- Implement the training loop and evaluation section (implementing the $F_1$ metric). \n",
    "- Ignore the class imbalance problem and train the MLP. Report your hyper-parameter details and the $F_1$ score performance on the test set (as the baseline).\n",
    "- Explore modifications to improve the performance of the class imbalance problem. Report your modifications and the $F_1$ scores performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_33525/852345841.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from IPython.display import display\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "    )\n",
    "\n",
    "mnist = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transform)\n",
    "mnist_test = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data',   # './data'\n",
    "                            train=False, \n",
    "                            download=True, \n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequencies: \", torch.bincount(mnist.targets))\n",
    "print(len(torch.bincount(mnist.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Your code goes here\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Your code goes here\n",
    "hidden_dim = 4\n",
    "model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def precision_score(labels, predictions):\n",
    "    predictions, labels = np.array(labels), np.array(predictions)\n",
    "    predictions_1 = np.sum(predictions==1)\n",
    "    correct_1 = np.sum( (predictions==1) & (labels==1))\n",
    "    precision = correct_1/ predictions_1 if predictions_1 > 0 else 1e-6\n",
    "    return precision\n",
    "\n",
    "def recall_score(labels, predictions):\n",
    "    predictions, labels = np.array(labels), np.array(predictions)\n",
    "    correct_1 = np.sum( (predictions==1) & (labels==1))\n",
    "    labels_1 = np.sum(labels==1)\n",
    "    recall = correct_1/ labels_1 if labels_1 > 0 else 1e-6\n",
    "    return recall\n",
    "\n",
    "def f1_score(labels, predictions):\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = (2 * (recall * precision)) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP (again for binary classification)\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Your code goes here\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Your code goes here\n",
    "hidden_dim = 4\n",
    "model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''same as before doing this so its easier to loop over hyper parameters'''\n",
    "\n",
    "def two_digit(batch_size=64):\n",
    "    model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "    \n",
    "    # no modificaitons here \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "\n",
    "    training_time = time.time()- start_time\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    val_preds = []; val_labels=[]\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        val_preds.append(pred)\n",
    "        val_labels.append(target)\n",
    "\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "    assert len(val_preds) == len(val_set)\n",
    "    \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    f1_validation = f1_score(labels = val_labels, predictions = val_preds)\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    test_preds = []; test_labels=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            test_preds.append(pred)\n",
    "            test_labels.append(target)\n",
    "        \n",
    "    test_preds = torch.cat(test_preds).numpy()\n",
    "    test_labels = torch.cat(test_labels).numpy()\n",
    "    assert len(test_preds) == len(test_set)   \n",
    "    test_acc = 100. * correct / total\n",
    "    f1_test = f1_score(labels = test_labels, predictions =test_preds)\n",
    "    \n",
    "    return training_time, train_acc, val_acc, test_acc, f1_validation, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_0_original = [data for data in mnist if data[1] == 0]\n",
    "train_1_original = [data for data in mnist if data[1] == 1]\n",
    "print('Train set (before sparsing)', len(train_0_original), len(train_1_original), len(train_1_original) + len( train_0_original) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[EXTRA BONUS]</span>\n",
    "\n",
    "If the hyper-parameters are chosen properly, the baseline can perform satisfactorily on the class imbalance problem with 1% digit \"1\". We want to challenge the baseline and handle more class-imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['N', 'Batch size', 'Train Time ', 'Train Acc' ,' Val Acc', 'Test Acc', 'F1-Val', 'F1-Test']\n",
    "question3_df =  pd.DataFrame(columns = headers)\n",
    "question3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_list = [100] + [250*(i+1) for i in range(8)]\n",
    "\n",
    "# test_1 = test_1[:len(test_1) // N]  ## comment this out if you are doing sparsed vs unsparsed while runnign the code \n",
    "\n",
    "\n",
    "for N in N_list:\n",
    "    train_0 = train_0_original.copy()\n",
    "    train_1 =  train_1_original.copy()\n",
    "    random.shuffle(train_1)\n",
    "    train_1 = train_1[:len(train_1) // N]\n",
    "    print(N, 'Train set (before sparsing)', len(train_0), len(train_1), len(train_1) + len( train_0) )# train_set = train_0 + train_1\n",
    "\n",
    "    # Split training data (1s)into training and validation sets\n",
    "    train_1len = int(len(train_1) *.8)\n",
    "    val_1len = len(train_1) - train_1len\n",
    "    train1_set, val1_set = random_split(train_1, [train_1len, val_1len])\n",
    "\n",
    "    # Split training data (0s) into training and validation sets\n",
    "    train_0len = int(len(train_0) *.8)\n",
    "    val_0len = len(train_0) - train_0len\n",
    "    train0_set, val0_set = random_split(train_0, [train_0len, val_0len])\n",
    "    \n",
    "    train_set = train0_set + train1_set\n",
    "    val_set = val0_set + val1_set\n",
    "    len(train_set), len(val_set)\n",
    "\n",
    "    # creating test set\n",
    "    test_0 = [data for data in mnist_test if data[1] == 0]\n",
    "    test_1 = [data for data in mnist_test if data[1] == 1]\n",
    "    print(N,'Test set (before sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "\n",
    "    test_1 = test_1[:len(test_1) // N]\n",
    "    print(N,'Test set (after sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "    test_set = test_0 + test_1\n",
    "    print('\\n')\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size = 64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size = 64, shuffle=False)\n",
    "    batch_size = 64\n",
    "\n",
    "    training_time, train_acc, val_acc, test_acc, f1_validation, f1_test = two_digit(batch_size=batch_size)\n",
    "    \n",
    "    row = [N, batch_size, training_time, train_acc, val_acc, test_acc, f1_validation, f1_test]\n",
    "    question3_df = pd.concat([question3_df, pd.DataFrame([row], columns=headers)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications to deal with sparsity\n",
    "### 1. Reweighting in the loss function to over upweight the sparse class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the two_digit function to take in the weight for loss function as as parameter\n",
    "\n",
    "def two_digit(weight, batch_size=64):\n",
    "    model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "        # print(f'Training accuracy: {train_acc:.2f}%')\n",
    "\n",
    "    training_time = time.time()- start_time\n",
    "    # print(training_time)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    val_preds = []; val_labels=[]\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        val_preds.append(pred)\n",
    "        val_labels.append(target)\n",
    "        # print(type(target))\n",
    "\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "    assert len(val_preds) == len(val_set)\n",
    "    \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_validation = f1_score(labels = val_labels, predictions = val_preds)\n",
    "    # print(f'F1 score validation: {f1_validation:.2f}')\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    test_preds = []; test_labels=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            test_preds.append(pred)\n",
    "            test_labels.append(target)\n",
    "        \n",
    "    test_preds = torch.cat(test_preds).numpy()\n",
    "    test_labels = torch.cat(test_labels).numpy()\n",
    "    assert len(test_preds) == len(test_set)   \n",
    "    test_acc = 100. * correct / total\n",
    "    # print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    f1_test = f1_score(labels = test_labels, predictions =test_preds)\n",
    "    # print(f'F1 score test: {f1_test:.2f}')\n",
    "\n",
    "    \n",
    "    return training_time, train_acc, val_acc, test_acc, f1_validation, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0_original = [data for data in mnist if data[1] == 0]\n",
    "train_1_original = [data for data in mnist if data[1] == 1]\n",
    "print('Train set (before sparsing)', len(train_0_original), len(train_1_original), len(train_1_original) + len( train_0_original) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['N', 'Batch size', 'Weight', 'Train Time ', 'Train Acc' ,' Val Acc', 'Test Acc', 'F1-Val', 'F1-Test']\n",
    "question3_df =  pd.DataFrame(columns = headers)\n",
    "print(question3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_list = [100] + [250*(i+1) for i in range(8)]\n",
    "for N in N_list:\n",
    "    train_0 = train_0_original.copy()\n",
    "    train_1 =  train_1_original.copy()\n",
    "    random.shuffle(train_1)\n",
    "    train_1 = train_1[:len(train_1) // N]\n",
    "    print(N, 'Train set (before sparsing)', len(train_0), len(train_1), len(train_1) + len( train_0) )# train_set = train_0 + train_1\n",
    "\n",
    "    # Split training data (1s)into training and validation sets\n",
    "    train_1len = int(len(train_1) *.8)\n",
    "    val_1len = len(train_1) - train_1len\n",
    "    train1_set, val1_set = random_split(train_1, [train_1len, val_1len])\n",
    "\n",
    "    # Split training data (0s) into training and validation sets\n",
    "    train_0len = int(len(train_0) *.8)\n",
    "    val_0len = len(train_0) - train_0len\n",
    "    train0_set, val0_set = random_split(train_0, [train_0len, val_0len])\n",
    "    \n",
    "    train_set = train0_set + train1_set\n",
    "    val_set = val0_set + val1_set\n",
    "    len(train_set), len(val_set)\n",
    "\n",
    "    # creating test set\n",
    "    test_0 = [data for data in mnist_test if data[1] == 0]\n",
    "    test_1 = [data for data in mnist_test if data[1] == 1]\n",
    "    print(N,'Test set (before sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "\n",
    "    test_1 = test_1[:len(test_1) // N]\n",
    "    print(N,'Test set (after sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "    test_set = test_0 + test_1\n",
    "    print('\\n')\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size = 64, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size = 64, shuffle=False)\n",
    "\n",
    "    # compensation = torch.tensor([1, N], dtype=torch.float32)\n",
    "    compensation = torch.tensor([1, (train_0len/ train_1len )], dtype=torch.float32)\n",
    "    weights = [[1,1], [1, int(N/10)], [1, int(N/2)], compensation]\n",
    "    batch_size = 64\n",
    "    results = []\n",
    "\n",
    "    # for batch_size in batch_sizes:\n",
    "    for weight in weights:\n",
    "        reweight_factor = weight[1]/ weight[0]\n",
    "        reweight_factor = float(reweight_factor)\n",
    "        weight = torch.tensor(weight, dtype=torch.float32)\n",
    "        weight = weight.to(device)\n",
    "        training_time, train_acc, val_acc, test_acc, f1_validation, f1_test = two_digit(batch_size=batch_size, weight = weight)\n",
    "        \n",
    "        row = [N, batch_size, reweight_factor, training_time, train_acc, val_acc, test_acc, f1_validation, f1_test]\n",
    "        question3_df = pd.concat([question3_df, pd.DataFrame([row], columns=headers)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3_df.to_csv(f'q3_hyperopt_weight_unsparsted_test.csv')\n",
    "display(question3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weighted Resampling in the data loader to oversample the under-represented class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_digit_resampling():\n",
    "    model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "    batch_size = 64\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        \n",
    "        train_acc = 100. * correct / count\n",
    "\n",
    "    training_time = time.time()- start_time\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    val_preds = []; val_labels=[]\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        val_preds.append(pred)\n",
    "        val_labels.append(target)\n",
    "\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "    assert len(val_preds) == len(val_set)\n",
    "    \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    f1_validation = f1_score(labels = val_labels, predictions = val_preds)\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    test_preds = []; test_labels=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            test_preds.append(pred)\n",
    "            test_labels.append(target)\n",
    "        \n",
    "    test_preds = torch.cat(test_preds).numpy()\n",
    "    test_labels = torch.cat(test_labels).numpy()\n",
    "    assert len(test_preds) == len(test_set)   \n",
    "    test_acc = 100. * correct / total\n",
    "    f1_test = f1_score(labels = test_labels, predictions =test_preds)\n",
    "    \n",
    "    return training_time, train_acc, val_acc, test_acc, f1_validation, f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "headers = ['N', 'Batch Size', 'Weight', 'Train Time ', 'Train Acc' ,' Val Acc', 'Test Acc', 'F1-Val', 'F1-Test']\n",
    "question3_df_resample =  pd.DataFrame(columns = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_list = [100] + [250*(i+1) for i in range(8)]\n",
    "for N in N_list:\n",
    "    train_0 = train_0_original.copy()\n",
    "    train_1 =  train_1_original.copy()\n",
    "    random.shuffle(train_1)\n",
    "    train_1 = train_1[:len(train_1) // N]\n",
    "    print(N, 'Train set (before sparsing)', len(train_0), len(train_1), len(train_1) + len( train_0) )# train_set = train_0 + train_1\n",
    "\n",
    "    # Split training data (1s)into training and validation sets\n",
    "    train_1len = int(len(train_1) *.8)\n",
    "    val_1len = len(train_1) - train_1len\n",
    "    train1_set, val1_set = train_1[:train_1len], train_1[train_1len:]\n",
    "\n",
    "    # Split training data (0s) into training and validation sets\n",
    "    train_0len = int(len(train_0) *.8)\n",
    "    val_0len = len(train_0) - train_0len\n",
    "    train0_set, val0_set = train_0[:train_0len], train_0[train_0len:]\n",
    "    \n",
    "    # train and val set\n",
    "    train_set = train0_set + train1_set\n",
    "    val_set = val0_set + val1_set\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(val_set)\n",
    "    len(train_set), len(val_set)\n",
    "\n",
    "    # creating test set\n",
    "    test_0 = [data for data in mnist_test if data[1] == 0]\n",
    "    test_1 = [data for data in mnist_test if data[1] == 1]\n",
    "    print(N,'Test set (before sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "\n",
    "    test_1 = test_1[:len(test_1) // N]\n",
    "    print(N,'Test set (after sparsing)',len(test_0), len(test_1), len(test_1) + len( test_0) )\n",
    "    test_set = test_0 + test_1\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    print('\\n')\n",
    "    # compensation = torch.tensor([1, N], dtype=torch.float32)\n",
    "    compensation = int(train_0len/ train_1len)\n",
    "    weight_factors = [1, int(N/10), int(N/2), compensation]\n",
    "    batch_size = 64\n",
    "    results = []\n",
    "\n",
    "    # for batch_size in batch_sizes:\n",
    "    for weight_factor in weight_factors:\n",
    "        \n",
    "        weights = np.array( [1.0 if data[1] == 0 else weight_factor for data in train_set])\n",
    "        weights = torch.from_numpy(weights)\n",
    "        \n",
    "        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "        \n",
    "        train_loader = DataLoader(train_set, batch_size=64, sampler=sampler)\n",
    "        val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "        \n",
    "        training_time, train_acc, val_acc, test_acc, f1_validation, f1_test = two_digit_resampling()\n",
    "        \n",
    "        row = [N, batch_size, weight_factor, training_time, train_acc, val_acc, test_acc, f1_validation, f1_test]\n",
    "        question3_df_resample = pd.concat([question3_df_resample, pd.DataFrame([row], columns=headers)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question3_df_resample.to_csv(f'q3_hyperopt_resampling_unsparsed_test.csv')\n",
    "display(question3_df_resample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Reconstruct the MNIST images by Regression\n",
    "In this problem, we want to train the MLP (with only one hidden layer) to complete a regression task: reconstruct the input image. The goal of this task is dimension reduction, and we set the hidden layer dimension to a smaller number, say 50. Once we can train the MLP to reconstruct the input images perfectly, we find an lower dimension representation of the MNIST images.\n",
    "\n",
    "Since this is a reconstruction task, the labels of the images are not needed, and the target is the same as the inputs. Mean Squared Error (MSE) is recommended as the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data pre-processing\n",
    "# convert the input to the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "    )\n",
    "\n",
    "# Load the MNIST dataset \n",
    "# this command requires Internet to download the dataset\n",
    "mnist = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transform)\n",
    "mnist_test = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data',   # './data'\n",
    "                            train=False, \n",
    "                            download=True, \n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "print(\"Frequencies: \", torch.bincount(mnist.targets))\n",
    "print(len(torch.bincount(mnist.targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "train_len = int(len(mnist) *.8)\n",
    "val_len = len(mnist) - train_len\n",
    "train_set, val_set = random_split(mnist, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size = 64, shuffle=False)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your MLP\n",
    "class RegressionMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(RegressionMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation= nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.activation_output= nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_output(x)\n",
    "        return x\n",
    "\n",
    "hidden_dim = 50\n",
    "model = RegressionMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=28*28).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    correct, count = 0, 0 \n",
    "    for data, target in train_loader:\n",
    "        # free the gradient from the previous batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # reshape the image into a vector\n",
    "        data = data.view(data.size(0), -1)\n",
    "        # print(data.size())\n",
    "        # print(output.size())\n",
    "        # model forward\n",
    "        output = model(data)\n",
    "        # compute the loss\n",
    "        loss = criterion(output, data)\n",
    "        # model backward\n",
    "        loss.backward()\n",
    "        # update the model paramters\n",
    "        optimizer.step()\n",
    "       \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "training_time = time.time()- start_time\n",
    "print(training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = count = 0\n",
    "correct = total = 0\n",
    "for data, target in val_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    data = data.view(data.size(0), -1)\n",
    "    output = model(data)\n",
    "    val_loss += criterion(output, data).item()\n",
    "    count += 1\n",
    "\n",
    "val_loss = val_loss / count\n",
    "print(f'Validation loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "loss = count= 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        loss += criterion(output, data).item()\n",
    "        count +=1\n",
    "        \n",
    "test_loss= loss/count\n",
    "print(f'Test Loss: {test_loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyError: had trouble writing over empty dic looked up that defaultdict can handle it without me having to explicitly handle constraints \n",
    "# a = {}\n",
    "# if \n",
    "# a[0] = [[1,2]]\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dict = defaultdict(list)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():  \n",
    "    for data, target in test_loader:\n",
    "        data = data.view(data.size(0), -1).to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        for idx, label in enumerate(target):\n",
    "            label = int(label.item())\n",
    "            \n",
    "            #loss for each image\n",
    "            individual_loss = criterion(output[idx].unsqueeze(0), data[idx].unsqueeze(0)).item()\n",
    "            \n",
    "            # for storing arrays and outputs\n",
    "            img_np = data[idx].numpy()\n",
    "            output_np = output[idx].numpy()\n",
    "\n",
    "            check_dict[label].append([img_np, output_np, individual_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- https://stackoverflow.com/questions/55466298/pytorch-cant-call-numpy-on-variable-that-requires-grad-use-var-detach-num\n",
    "\n",
    "- https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html\n",
    "''' \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(10):\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(4,10))\n",
    "    fig.suptitle('Left: Original, Right: Reconstructed')\n",
    "    for j in range(5):\n",
    "        axes[j,0].imshow(check_dict[i][j][0].reshape(28,28))\n",
    "        axes[j,1].imshow(check_dict[i][j][1].reshape(28,28))\n",
    "        axes[j,1].set_title(f'Loss {check_dict[i][j][2]:.3f}')\n",
    "    fig.savefig(f'output_{i}.png')\n",
    "    fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
