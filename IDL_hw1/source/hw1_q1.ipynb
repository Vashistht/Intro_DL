{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c574ceac-8715-4368-9f04-1342c814e8fd",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch\n",
    "\n",
    "# Problem 1: Simple MLP for Binary Classification\n",
    "In this problem, you will train a simple MLP to classify two handwritten digits: 0 vs 1. We provide some starter codes to do this task with steps. However, you do not need to follow the exact steps as long as you can complete the task in sections marked as <span style=\"color:red\">[YOUR TASK]</span>.\n",
    "\n",
    "## Dataset Setup\n",
    "We will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The `torchvision` package has supported this dataset. We can load the dataset in this way (the dataset will take up 63M of your disk space):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Exploring MLPs with PyTorch\n",
    "\n",
    "# Problem 1: Simple MLP for Binary Classification\n",
    "In this problem, you will train a simple MLP to classify two handwritten digits: 0 vs 1. We provide some starter codes to do this task with steps. However, you do not need to follow the exact steps as long as you can complete the task in sections marked as <span style=\"color:red\">[YOUR TASK]</span>.\n",
    "\n",
    "## Dataset Setup\n",
    "We will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The `torchvision` package has supported this dataset. We can load the dataset in this way (the dataset will take up 63M of your disk space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "872aca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9801782f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('14.2.1', ('', '', ''), 'arm64')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/29/2n699yc91qs65qqg0cqgqtlr0000gp/T/ipykernel_25404/3673749337.py:3: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform, time\n",
    "print(platform.mac_ver() )\n",
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94ec73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75b2483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not torch.backends.mps.is_available():\n",
    "#     if not torch.backends.mps.is_built():\n",
    "#         print(\"MPS not available because the current PyTorch install was not \"\n",
    "#               \"built with MPS enabled.\")\n",
    "#     else:\n",
    "#         print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "#               \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    \n",
    "# else:\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print('mps enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2af33ec5-782b-4a0b-819b-f649500627c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data pre-processing\n",
    "# convert the input to the range [-1, 1].\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(0.5, 0.5)]\n",
    "    )\n",
    "\n",
    "# Load the MNIST dataset \n",
    "# this command requires Internet to download the dataset\n",
    "mnist = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data', \n",
    "                       train=True, \n",
    "                       download=True, \n",
    "                       transform=transform)\n",
    "mnist_test = datasets.MNIST(root='/Users/vashisth/Documents/GitHub/Intro_DL/IDL_hw1/data',   # './data'\n",
    "                            train=False, \n",
    "                            download=True, \n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3208785-2d53-403a-9939-eaccf646f5cd",
   "metadata": {},
   "source": [
    "In Problem 1, we only focus on a binary classification between digits 0 and 1. Thus we filter the dataset to contain only samples of digits 0 and 1. Besides, we want to randomly split the original training data into two disjoint datasets: a new training set containing 80\\% original training samples and a validation dataset containing 20\\% original training samples. We provide the incomplete code as a hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a14f718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Filter for digits 0 and 1\n",
    "# train_data = [data for data in mnist if data[1] < 2]\n",
    "train_index = mnist.targets<2\n",
    "mnist.data = mnist.data[train_index]\n",
    "mnist.targets = mnist.targets[train_index]\n",
    "# Your code goes here\n",
    "test_index = mnist_test.targets<2\n",
    "mnist_test.data = mnist_test.data[test_index]\n",
    "mnist_test.targets = mnist_test.targets[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ff472db-51ee-4835-8217-8557947f0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "# Your code goes here\n",
    "# train_set = ...\n",
    "# val_set = ...\n",
    "train_len = int(len(mnist) *.8)\n",
    "val_len = len(mnist) - train_len\n",
    "train_set, val_set = random_split(mnist, [train_len, val_len])\n",
    "\n",
    "# Define DataLoaders to access data in batches\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "# Your code goes here\n",
    "val_loader = DataLoader(val_set, batch_size = 64, shuffle=False)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9c20b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10132"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c50a08-aee3-49ea-b220-7bffbc63a56d",
   "metadata": {},
   "source": [
    "## Define an MLP\n",
    "We want to define a simple MLP with only one hidden layer. You can use ``torch.nn.Linear`` to define a single MLP layer and pick an activation layer you like. Since our inputs are images with $28\\times28$ pixels, the input dimension is $28\\times28=784$. The problem is a binary classification, thus, the output dimension is 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0db945f5-97c2-4db0-84c6-939383b2ecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=5, bias=True)\n",
      "  (activation): Sigmoid()\n",
      "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Your code goes here\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Your code goes here\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Your code goes here\n",
    "hidden_dim = 5\n",
    "model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c8553-0271-4868-aa38-662c3fffd403",
   "metadata": {},
   "source": [
    "## Train the MLP\n",
    "To train the model, we need to define a loss function (criterion) and an optimizer. The loss function tells us how far away the modelâ€™s prediction is from the label. Once we have the loss, PyTorch can compute the gradient of the model automatically. The optimizer uses the gradient to update the model. For classification problems, we often use the Cross Entropy Loss. For the optimizer, we can use stochastic gradient descent optimizer or Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a7f6874-687e-4eed-ad45-c366511c7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8587b-30cb-4939-860a-ad4094f4446e",
   "metadata": {},
   "source": [
    "There are several hyper-parameters in the optimizer (please see the [PyTorch document](https://pytorch.org/docs/stable/optim.html) for details). You can play with the hyper-parameters and see how they influence the training.\n",
    "\n",
    "Now we have almost everything to train the model. We provide a sample code to complete the training loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c03f28f-29fe-46bc-9661-9a2870050d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.33%\n",
      "Training accuracy: 99.74%\n",
      "Training accuracy: 99.85%\n",
      "Training accuracy: 99.89%\n",
      "Training accuracy: 99.90%\n",
      "Training accuracy: 99.92%\n",
      "Training accuracy: 99.93%\n",
      "Training accuracy: 99.93%\n",
      "Training accuracy: 99.94%\n",
      "Training accuracy: 99.97%\n",
      "6.095792293548584\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    correct, count = 0, 0 \n",
    "    for data, target in train_loader:\n",
    "        # free the gradient from the previous batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # reshape the image into a vector\n",
    "        data = data.view(data.size(0), -1)\n",
    "        # model forward\n",
    "        output = model(data)\n",
    "        # compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        # model backward\n",
    "        loss.backward()\n",
    "        # update the model paramters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # adding this for train accuracy \n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        count += data.size(0)\n",
    "    \n",
    "    train_acc = 100. * correct / count\n",
    "    print(f'Training accuracy: {train_acc:.2f}%')\n",
    "\n",
    "training_time = time.time()- start_time\n",
    "print(training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7bef23-b755-4510-b626-f08c83fc1cb4",
   "metadata": {},
   "source": [
    "After the training, we can use the validation dataset to know the performance of our model on new samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d596ea2-15c9-4be4-b38a-514c9ba102b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.02, accuracy: 99.80%\n"
     ]
    }
   ],
   "source": [
    "val_loss = count = 0\n",
    "correct = total = 0\n",
    "for data, target in val_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    data = data.view(data.size(0), -1)\n",
    "    output = model(data)\n",
    "    val_loss += criterion(output, target).item()\n",
    "    count += 1\n",
    "    pred = output.argmax(dim=1)\n",
    "    correct += (pred == target).sum().item()\n",
    "    total += data.size(0)\n",
    "    \n",
    "val_loss = val_loss / count\n",
    "val_acc = 100. * correct / total\n",
    "print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "934d1f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.95%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        \n",
    "test_acc = 100. * correct / total\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d741576-15d4-4427-81b1-a6025a05e496",
   "metadata": {},
   "source": [
    "You can also perform validation after each epoch. But remember not to train (backward and update) on the validation dataset. Use the validation set to optimize performance. After you are done with this, report performance on the test set(You are encouraged not to use the test set for validation, i.e., use the test set only once after you are happy with the validation performance).\n",
    "\n",
    "<span style=\"color:red\">[YOUR TASK]</span>\n",
    "- Filter all samples representing digits \"0\" or \"1\" from the MNIST datasets. \n",
    "- Randomly split the training data into a training set (80\\% training samples) of a validation set (20% training samples).\n",
    "- Define an MLP with 1 hidden layer and train the MLP to classify the digits \"0\" vs \"1\".  Report your MLP design and training details (which optimizer, number of epochs, learning rate, etc.)\n",
    "- Keep other hyper-parameters the same, and train the model with different batch sizes: 2, 16, 128, 1024. Report the time cost, training, validation, and test set accuracy of your model\n",
    "\n",
    "\n",
    "In our implementations, we trained our network for 10 epochs in about 10 seconds on a laptop, getting a test accuracy of 99\\% %.\n",
    "\n",
    "One tip about the hidden layer size is to begin with a small number, say $16\\sim 64$. Some people find $$\\text{hidden size} = \\sqrt{\\text{input size}\\times \\text{output size}}$$ is a good choice in practice. If your model's training accuracy is too low, you can double the hidden layer size. However, if you find the training accuracy is high. Still, the validation accuracy is much lower, you may consider a smaller hidden layer size because your model has the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1f96821-15f9-42d7-9499-8469a8fd1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_digit(batch_size):\n",
    "    # Define DataLoaders to access data in batches\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # Your code goes here\n",
    "    val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle=False)\n",
    "    \n",
    "    model = SimpleMLP(in_dim=28 * 28,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  out_dim=2).to(device)\n",
    "    # print(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # You can play with different optimizers\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    \n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        correct, count = 0, 0 \n",
    "        for data, target in train_loader:\n",
    "            # free the gradient from the previous batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # reshape the image into a vector\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # model forward\n",
    "            output = model(data)\n",
    "            # compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            # model backward\n",
    "            loss.backward()\n",
    "            # update the model paramters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding this for train accuracy \n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            count += data.size(0)\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "        train_acc = 100. * correct / count\n",
    "        # print(f'Training accuracy: {train_acc:.2f}%')\n",
    "\n",
    "    training_time = time.time()- start_time\n",
    "    # print(training_time)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = count = 0\n",
    "    correct = total = 0\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        count += 1\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += data.size(0)\n",
    "        \n",
    "    val_loss = val_loss / count\n",
    "    val_acc = 100. * correct / total\n",
    "    # print(f'Validation loss: {val_loss:.2f}, accuracy: {val_acc:.2f}%')\n",
    "    \n",
    "    # test\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            \n",
    "    test_acc = 100. * correct / total\n",
    "    # print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    print('Hyperopt run done')\n",
    "    return training_time, train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a96b2f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0038\n",
      "Epoch 2, Loss: 0.0013\n",
      "Epoch 3, Loss: 0.0003\n",
      "Epoch 4, Loss: 0.0012\n",
      "Epoch 5, Loss: 0.0003\n",
      "Epoch 6, Loss: 0.0001\n",
      "Epoch 7, Loss: 0.0001\n",
      "Epoch 8, Loss: 0.0001\n",
      "Epoch 9, Loss: 0.0001\n",
      "Epoch 10, Loss: 0.0001\n",
      "Hyperopt run done\n",
      "Epoch 1, Loss: 0.0708\n",
      "Epoch 2, Loss: 0.0350\n",
      "Epoch 3, Loss: 0.0138\n",
      "Epoch 4, Loss: 0.0130\n",
      "Epoch 5, Loss: 0.0053\n",
      "Epoch 6, Loss: 0.0045\n",
      "Epoch 7, Loss: 0.0065\n",
      "Epoch 8, Loss: 0.0022\n",
      "Epoch 9, Loss: 0.0016\n",
      "Epoch 10, Loss: 0.0012\n",
      "Hyperopt run done\n",
      "Epoch 1, Loss: 0.2647\n",
      "Epoch 2, Loss: 0.1728\n",
      "Epoch 3, Loss: 0.1197\n",
      "Epoch 4, Loss: 0.0973\n",
      "Epoch 5, Loss: 0.0759\n",
      "Epoch 6, Loss: 0.0663\n",
      "Epoch 7, Loss: 0.0509\n",
      "Epoch 8, Loss: 0.0491\n",
      "Epoch 9, Loss: 0.0399\n",
      "Epoch 10, Loss: 0.0346\n",
      "Hyperopt run done\n",
      "Epoch 1, Loss: 0.6370\n",
      "Epoch 2, Loss: 0.5631\n",
      "Epoch 3, Loss: 0.5066\n",
      "Epoch 4, Loss: 0.4771\n",
      "Epoch 5, Loss: 0.4590\n",
      "Epoch 6, Loss: 0.4270\n",
      "Epoch 7, Loss: 0.4068\n",
      "Epoch 8, Loss: 0.3801\n",
      "Epoch 9, Loss: 0.3821\n",
      "Epoch 10, Loss: 0.3466\n",
      "Hyperopt run done\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [2, 16, 128, 1024]\n",
    "results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    training_time, train_acc, val_acc, test_acc = two_digit(batch_size=batch_size)\n",
    "    results.append([batch_size,training_time, train_acc, val_acc, test_acc])\n",
    "\n",
    "headers = ['Batch size', 'Training Time ', 'Train Acc' ,' Val Acc', 'Test Acc']\n",
    "df =  pd.DataFrame(results, columns = headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c85873ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Test Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>28.409046</td>\n",
       "      <td>99.940782</td>\n",
       "      <td>99.842084</td>\n",
       "      <td>99.905437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>10.273708</td>\n",
       "      <td>99.950651</td>\n",
       "      <td>99.802606</td>\n",
       "      <td>99.905437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>5.733365</td>\n",
       "      <td>99.940782</td>\n",
       "      <td>99.763127</td>\n",
       "      <td>99.905437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>5.083993</td>\n",
       "      <td>98.874852</td>\n",
       "      <td>99.368338</td>\n",
       "      <td>99.479905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Batch size  Training Time   Train Acc    Val Acc   Test Acc\n",
       "0           2       28.409046  99.940782  99.842084  99.905437\n",
       "1          16       10.273708  99.950651  99.802606  99.905437\n",
       "2         128        5.733365  99.940782  99.763127  99.905437\n",
       "3        1024        5.083993  98.874852  99.368338  99.479905"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(device)\n",
    "df.to_csv('question_1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e91ac444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrrr}\n",
      "\\toprule\n",
      "Unnamed: 0 & Batch size & Training Time  & Train Acc &  Val Acc & Test Acc \\\\\n",
      "\\midrule\n",
      "0 & 2 & 28.409046 & 99.940782 & 99.842084 & 99.905437 \\\\\n",
      "1 & 16 & 10.273708 & 99.950651 & 99.802606 & 99.905437 \\\\\n",
      "2 & 128 & 5.733365 & 99.940782 & 99.763127 & 99.905437 \\\\\n",
      "3 & 1024 & 5.083993 & 98.874852 & 99.368338 & 99.479905 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('question_1.csv')\n",
    "latex_table = df.to_latex(index=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544a0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
