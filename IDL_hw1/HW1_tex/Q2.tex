
\section{Question 2}

\begin{itemize}
    \item  Implement the training loop and evaluation section. Report the hyper-parameters you choose
    
    \item Experiment with different numbers of neurons in the hidden layer and note any changes in performance.
    
    \item Write a brief analysis of the model's performance, including any challenges faced and how they were addressed
\end{itemize}

\begin{solve}

    \subsection{Model for multi-class (10 digits) classificaiton}

    \begin{lstlisting}[language=python]
print(model)
hidden_dim = int(np.sqrt(28*28*10)) # changed in hyperopt

MulticlassMLP(
    (fc1): Linear(in_features=784, out_features=88, bias=True)
    (activation): Sigmoid() 
    (fc2): Linear(in_features=88, out_features=10, bias=True)
)

# criterion = nn.CrossEntropyLoss()

    \end{lstlisting}


    \subsection{Hyper parameter optimization for Multi-Class Optimization}
    
    I first experimented with device to see the most optimal device for training between cpu and the gpu on my Mac-M1 Pro. The results are given in the appendix for the model with sigmoid. For each of them we found that \textbf{device = `cpu'} was faster than  \textbf{device = `mps'} (mac gpu in M1 Pro).

    \subsubsection{Results:}
    Two models were optimized for this task, one with sigmoid activation and the other with ReLU activation.

    \begin{enumerate}
        \item \textbf{Sigmoid Activation Layer}
\begin{lstlisting}[language=python]
print(model)
hidden_dim = int(np.sqrt(28*28*10)) 
# hidden_dim changed in hyperopt later
MulticlassMLP(
    (fc1): Linear(in_features=784, out_features=88, 
    bias=True)
    (activation): Sigmoid()
    (fc2): Linear(in_features=88, out_features=10, 
    bias=True)
)
# criterion = nn.CrossEntropyLoss()
        \end{lstlisting}


        The hyper-parameters tested are as follows 
        \begin{lstlisting}[language=python]
batch_sizes = [64, 128, 1024]
optimizers = ['adam', 'sgd']
# learning_rates= [1e-4, 1e-3, 1e-2, 1e-1]
hidden_dims = [4, 32, 64, 128]
        \end{lstlisting}
        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        \begin{table}[H]
            \centering
        \begin{tabular}{rlrlrrrrrr}
            \toprule
            Opt\# & Batch & Opt & LR & HidDim& Training Time & Train Acc & Val Acc & Test Acc \\
            \midrule
            0 & 64 & adam & 0.001& 4 & 20.822481 & 81.116667 & 80.50 & 81.34 \\
            1 & 64 & adam & 0.001& 32 & 21.138930 & 95.725& 94.583333 & 94.83 \\
            2 & 64 & adam & 0.001& 64 & 22.073092 & 97.208333 & 95.566667 & 96.09 \\
            \rowcolor{yellow!50}
            3 & 64 & adam & 0.001& 128 & 22.682774 & 98.10 & 96.591667 & 96.87 \\
            4 & 64 & sgd & 0.01 & 4 & 19.016016 & 69.941667 & 70.083333 & 70.49 \\
            5 & 64 & sgd & 0.01 & 32 & 19.240644 & 89.883333 & 89.666667 & 90.33 \\
            6 & 64 & sgd & 0.01 & 64 & 19.526299 & 90.029167 & 89.866667 & 90.57 \\
            \rowcolor{orange!50}
            7 & 64 & sgd & 0.01 & 128 & 20.448700 & 90.037500 & 89.85 & 90.69 \\
            8 & 128 & adam & 0.001& 4 & 18.587870 & 74.941667 & 73.933333 & 75.06 \\
            9 & 128 & adam & 0.001& 32 & 18.807421 & 95.029167 & 94.341667 & 94.47 \\
            10 & 128 & adam & 0.001& 64 & 19.121340 & 96.412500 & 95.416667 & 95.75 \\
            \rowcolor{yellow!50}
            11 & 128 & adam & 0.001& 128 & 20.281624 & 97.404167 & 96.433333 & 96.45 \\            
            12 & 128 & sgd & 0.01 & 4 & 17.898255 & 63.181250 & 63.475& 64.90 \\
            13 & 128 & sgd & 0.01 & 32 & 17.976289 & 87.287500 & 87.291667 & 88.10 \\
            14 & 128 & sgd & 0.01 & 64 & 18.428340 & 87.922917 & 87.775& 88.92 \\
            15 & 128 & sgd & 0.01 & 128 & 18.753883 & 87.927083 & 87.90 & 88.73 \\
            \rowcolor{red!50}
            16 & 1024 & adam & 0.001& 4 & 17.380155 & 52.65 & 52.416667 & 53.09 \\
            17 & 1024 & adam & 0.001& 32 & 17.465012 & 90.939583 & 90.625& 91.21 \\
            18 & 1024 & adam & 0.001& 64 & 17.708344 & 92.270833 & 91.866667 & 92.49 \\
            19 & 1024 & adam & 0.001& 128 & 18.143819 & 93.297917 & 92.766667 & 93.21 \\
            \rowcolor{red!50}
            20 & 1024 & sgd & 0.01 & 4 & 17.723570 & 20.575& 22.00 & 23.66 \\
            21 & 1024 & sgd & 0.01 & 32 & 17.807459 & 62.589583 & 63.075& 65.17 \\
            22 & 1024 & sgd & 0.01 & 64 & 18.086973 & 67.327083 & 66.508333 & 68.73 \\
            23 & 1024 & sgd & 0.01 & 128 & 18.751791 & 70.679167 & 70.291667 & 71.88 \\
            \bottomrule
            \end{tabular}
            \caption{Hyperopt results for different optmizers, learning rate, batch size, and hidden dimension of the MulticlassMLP Network with sigmoid activation layer}
            \label{TableHyperoptSigmoid}
        \end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%
        \item \textbf{ReLU Activation Layer}

        \begin{lstlisting}[language=python]
class MulticlassMLP(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super(MulticlassMLP, self).__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.activation = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, out_dim)
        
    def forward(self, x):
        # Your code goes here
        x = self.fc1(x)
        x = self.activation(x)
        x = self.fc2(x)
        
        return x

# criterion = nn.CrossEntropyLoss()
    \end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[H]
    \centering
\begin{tabular}{rlrlrrrrrr}
    Opt\# & Batch & Opt & LR & HidDim& Training Time & Train Acc & Val Acc & Test Acc\\
    \midrule
    \rowcolor{red!50}
    0 & 64 & adam & 0.001 & 4 & 22.202067 & 33.362500 & 33.991667 & 34.13 \\
    1 & 64 & adam & 0.001 & 32 & 22.753937 & 95.647917 & 94.566667 & 95.12 \\
    \rowcolor{yellow!50}
    2 & 64 & adam & 0.001 & 64 & 23.678958 & 97.191667 & 96.525 & 96.80 \\
    3 & 64 & adam & 0.001 & 128 & 24.853068 & 98.108333 & 96.425 & 96.53 \\
    4 & 64 & sgd & 0.01 & 4 & 21.199240 & 82.979167 & 82.091667 & 82.65 \\
    5 & 64 & sgd & 0.01 & 32 & 23.314560 & 92.931250 & 92.458333 & 92.94 \\
    6 & 64 & sgd & 0.01 & 64 & 24.677200 & 93.55 & 93.05 & 93.49 \\
    \rowcolor{orange!50}
    7 & 64 & sgd & 0.01 & 128 & 23.671364 & 93.885417 & 93.608333 & 94.10 \\
    8 & 128 & adam & 0.001 & 4 & 20.069331 & 65.714583 & 65.108333 & 66.38 \\
    9 & 128 & adam & 0.001 & 32 & 21.104044 & 93.787500 & 92.925 & 93.34 \\
    10 & 128 & adam & 0.001 & 64 & 22.600291 & 96.437500 & 95.45 & 95.97 \\
    \rowcolor{yellow!50}
    11 & 128 & adam & 0.001 & 128 & 24.154394 & 97.735417 & 96.30 & 96.72 \\
    12 & 128 & sgd & 0.01 & 4 & 20.192016 & 81.056250 & 80.933333 & 81.43 \\
    13 & 128 & sgd & 0.01 & 32 & 19.052993 & 91.445833 & 91.433333 & 92.05 \\
    14 & 128 & sgd & 0.01 & 64 & 19.173184 & 91.566667 & 91.241667 & 91.92 \\
    15 & 128 & sgd & 0.01 & 128 & 20.031956 & 91.764583 & 91.458333 & 92.31 \\
    \rowcolor{red!50}
    16 & 1024 & adam & 0.001 & 4 & 17.754258 & 40.585417 & 41.825 & 41.90 \\
    17 & 1024 & adam & 0.001 & 32 & 17.710765 & 91.395833 & 91.466667 & 91.97 \\
    18 & 1024 & adam & 0.001 & 64 & 18.001581 & 93.037500 & 92.708333 & 93.05 \\
    19 & 1024 & adam & 0.001 & 128 & 18.118424 & 94.625 & 94.158333 & 94.60 \\
    20 & 1024 & sgd & 0.01 & 4 & 17.477235 & 53.318750 & 53.15 & 54.65 \\
    21 & 1024 & sgd & 0.01 & 32 & 17.538370 & 85.191667 & 84.941667 & 85.86 \\
    22 & 1024 & sgd & 0.01 & 64 & 17.841422 & 86.214583 & 86.075 & 87.12 \\
    23 & 1024 & sgd & 0.01 & 128 & 18.008393 & 86.645833 & 86.491667 & 87.38 \\
    \bottomrule
    \end{tabular}
    \caption{Hyperopt results for different optmizers, learning rate, batch size, and hidden dimension of the MulticlassMLP Network with ReLU activation layer}
    \label{TableHyperoptRelu}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%
    \end{enumerate}

    \subsubsection{Conclusion}
    From \ref{TableHyperoptSigmoid} and \ref{TableHyperoptRelu}, we see that the highlighted rows.

    The \textbf{yellow} rows highlight the highest performing hyper-parameters for `adam', highest performing hyper-parameters for `sgd' are highlighted in \textbf{orange}, and \textbf{red} highligts the worse performances across the two optimizers.

    \textbf{Observations: } 
    \begin{itemize}

        \item Sigmoid Activation
        
        \begin{itemize}
            \item 
            
            For batch size, optimizer, learning rate, hidden dimension, training time as follows
            64, adam, 0.00, 128, 22.682, we get a train accuracy of 98.10\%, validation accuracy of 96.59\%, and the test accuracy of 96.87\%. For batch size, optimizer, learning rate, hidden dimension, training time as follows 64, sgd, 0.01, 128, 20.4487, we get a train accuracy of 90.04\%, validation accuracy of  89.85\%, and the test accuracy of 90.69\%.
            
            
            Across this we note that `adam' gets slightly higher test accuracy than `sgd'. We also see that 128 hidden neurons do slightly better across the test set for both optimizers. This highlights that it is better able to not only capture the train data patterns, but also does well with test data. This shows that the given range of 16~64 might not be best suited for our multi class problem. 
            

            For batch size, optimizer, learning rate, hidden dimension, training time as follows
            128, adam, 0.001, 128, 20.2816, wget a train accuracy of 97.40\%, validation accuracy of  96.43\%, and the test accuracy of 96.45\%.

            - Here we see that the batch size of 64 does slightly better than the batch size of 128, while also being slightly faster. However, the difference in performance is not statistically enough.
            
            - In the red highlighted rows of \ref{TableHyperoptSigmoid}, we see that Hidden dimension of 4 does worst across the board, which makes sense given the number of neurons are much lesser than what we expect to capture the complexity of the 10 class classification problem. 
        \end{itemize}        

        \item  ReLU Activation
        
        \begin{itemize}
            \item For batch size, optimizer, learning rate, hidden dimension, training time as follows  
            64, adam, 0.00, 128, 22.68, we get a train accuracy of 97.19\%, validation accuracy of 96.52\%, and the test accuracy of 96.80\%.

            For batch size, optimizer, learning rate, hidden dimension, training time as follows 64, sgd, 0.01, 128, 23.6713, we get a train accuracy of  93.88\%, validation accuracy of 93.60\%, and the test accuracy of 94.10\%.
            

            For batch size, optimizer, learning rate, hidden dimension, training time as follows  
            128, adam, 0.001, 128, 24.154, we get a train accuracy of  97.73\%, validation accuracy of 93.60\%, and the test accuracy of 96.72\%.

            Across this we note that `adam' gets slightly higher test accuracy than `sgd'. We also see that 128 hidden neurons do slightly worse across the test set for both optimizers, but better for training accuracy. This highlights that we might be over fitting on the training dataset for adam. 
            
            - We also see that ReLU activation does slightly worse than sigmoid which is not I had expected. This hightlights that there is no universally best performing activation function. However, again the difference is not what we will call statistically significant.
            
            - Here we see that the batch size of 64 does slightly better than the batch size of 128, while also being slightly faster. However, the difference in performance is not statistically enough.
            
            - In the red highlighted rows of \ref{TableHyperoptRelu}, we see that Hidden dimension of 4 does worst across the board, which makes sense given the number of neurons are much lesser than what we expect to capture the complexity of the 10 class classification problem. 

        
        \end{itemize}        


        \item Challenges: As such there were no specific challenges other than the compute time that this grid based hyper parameter optimization took. To reduce the timing I tested both cpu and gpu times and to my surprise, cpu was significally faster. 
        After choosing the faster device, the whole process for the given number of hyper parameters took around 15 minutes for each network. So in total around 30 for both sigmoid and relu networks combined together.
    
    \end{itemize}

\end{solve}