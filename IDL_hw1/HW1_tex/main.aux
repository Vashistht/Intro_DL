\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bbl@cs{beforestart}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Question 1}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Model for binary classificaiton}{1}{subsection.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparing the performance of SimpleMLP model for different batch sizes 2, 16, 128, 1024\relax }}{1}{table.caption.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Question 2}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Model for multi-class (10 digits) classificaiton}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Hyper parameter optimization for Multi-Class Optimization}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Results:}{2}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Hyperopt results for different optmizers, learning rate, batch size, and hidden dimension of the MulticlassMLP Network with sigmoid activation layer\relax }}{3}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{TableHyperoptSigmoid}{{2}{3}{Hyperopt results for different optmizers, learning rate, batch size, and hidden dimension of the MulticlassMLP Network with sigmoid activation layer\relax }{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Hyperopt results for different optmizers, learning rate, batch size, and hidden dimension of the MulticlassMLP Network with ReLU activation layer\relax }}{4}{table.caption.3}\protected@file@percent }
\newlabel{TableHyperoptRelu}{{3}{4}{Hyperopt results for different optmizers, learning rate, batch size, and hidden dimension of the MulticlassMLP Network with ReLU activation layer\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Conclusion}{4}{subsubsection.2.2.2}\protected@file@percent }
\citation{No Shuffle}
\@writefile{toc}{\contentsline {section}{\numberline {3}Question 3}{7}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.1}Creation of Imbalanced Dataset where we sample every $N^{th}$ point}{7}{subsubsection.3.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.2}Implement $F_1$ metric}{8}{subsubsection.3.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.3}Analysis of the model performance for different degrees of sparsity (larger N means more sparse dataset)}{10}{subsubsection.3.0.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces no modifications, test: sparsted data where N shows every Nth data point from the train, validation, and test datasets were sampled\relax }}{10}{figure.caption.4}\protected@file@percent }
\newlabel{no_mods}{{1}{10}{no modifications, test: sparsted data where N shows every Nth data point from the train, validation, and test datasets were sampled\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces no modifications, here N shows every Nth data point from the train, validation were sampled. Test data remained unchanged\relax }}{11}{figure.caption.5}\protected@file@percent }
\newlabel{no_mods_og}{{2}{11}{no modifications, here N shows every Nth data point from the train, validation were sampled. Test data remained unchanged\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.4}Adjusted Class Weights in the Loss Function: Aalysis of the model performance for different degrees of sparsity for different loss weights}{11}{subsubsection.3.0.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Adjusted Class weights in the Loss Function, test: sparsted data where N shows every Nth data point from the train, validation, and test datasets were sampled\relax }}{12}{figure.caption.6}\protected@file@percent }
\newlabel{weight_mod_on_sparsed_test}{{3}{12}{Adjusted Class weights in the Loss Function, test: sparsted data where N shows every Nth data point from the train, validation, and test datasets were sampled\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Adjusted Class weights in the Loss Function, test: sparsted data where N shows every Nth data point from the train, validation. Test datasets was left as original test data set\relax }}{13}{figure.caption.7}\protected@file@percent }
\newlabel{weight_mod_on_unsparsed_test}{{4}{13}{Adjusted Class weights in the Loss Function, test: sparsted data where N shows every Nth data point from the train, validation. Test datasets was left as original test data set\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.0.5}Resampling in the Data Loader: Aalysis of the model performance for different degrees of sparsity for different resampling weights }{14}{subsubsection.3.0.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Adjusted Class weights in the Loss Function, test: sparsted data where N shows every Nth data point from the train, validation, and test datasets were sampled\relax }}{16}{figure.caption.8}\protected@file@percent }
\newlabel{resampling_sparsed_test}{{5}{16}{Adjusted Class weights in the Loss Function, test: sparsted data where N shows every Nth data point from the train, validation, and test datasets were sampled\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Adjusted Class weights in the Loss Function, test: sparsted data where N shows every Nth data point from the train, validation. Test datasets was left as original test data set\relax }}{17}{figure.caption.9}\protected@file@percent }
\newlabel{resampling_unsparsed_test}{{6}{17}{Adjusted Class weights in the Loss Function, test: sparsted data where N shows every Nth data point from the train, validation. Test datasets was left as original test data set\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Question 4}{19}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model for Reconstruction}{19}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Reporting Train, Val, and Test MSE Loss }{19}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Reconstructed Images vs Original Images for Digits 0 to 9}{20}{subsection.4.3}\protected@file@percent }
