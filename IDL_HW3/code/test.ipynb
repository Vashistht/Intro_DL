{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyFConv2D(input, weight, bias=None, stride=1, padding=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    My custom Convolution 2D calculation.\n",
    "\n",
    "    [input]\n",
    "    * input    : (batch_size, in_channels, input_height, input_width)\n",
    "    * weight   : (you have to derive the shape :-)\n",
    "    * bias     : bias term\n",
    "    * stride   : stride size\n",
    "    * padding  : padding size\n",
    "\n",
    "    [output]\n",
    "    * output   : (batch_size, out_channels, output_height, output_width)\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(input.shape) == len(weight.shape) , \"weight shape incorrect\"\n",
    "    assert len(input.shape) == 4, \"input shape incorrect\"\n",
    "        \n",
    "    # batch_size, in_channels, input_height, input_width = input.shape\n",
    "    N, C_in, H_in, W_in = input.shape\n",
    "    # weight shape  (C_out, C_in, kH, kW)\n",
    "    C_out, C_in, kH, kW = weight.shape \n",
    "    bias, s, p = bias, stride, padding\n",
    "\n",
    "    x_pad =  F.pad(input,(p,p,p,p),mode='constant',value=0) # add 2p to last two dims of input\n",
    "    assert x_pad.shape == (N, C_in, H_in + 2*p, W_in + 2*p) , \"padding incorrect\"\n",
    "\n",
    "    H_out = int( (H_in + 2*p - kH) / s) + 1 # output sizes \n",
    "    W_out = int( (W_in + 2*p - kW) / s) + 1\n",
    "    \n",
    "    output = torch.zeros(N, C_out, H_out, W_out) # initalise output tensor\n",
    "    \n",
    "    ## Convolution\n",
    "    for h in range(H_out):\n",
    "        for w in range(W_out):\n",
    "            \n",
    "            window = x_pad[:, :, h*s:h*s+kH, w*s:w*s+kW]\n",
    "            # (b,c_in, kH, KW) * (c_o, c_i, kH, kW) -> (b, c_o, _)\n",
    "            # sum over c_in, kH, kW\n",
    "            output[:,:, h, w] = torch.tensordot(window, weight, dims=([1,2,3],[1,2,3]))\n",
    "            if bias is not None:\n",
    "                output[:,:, h, w] += bias[:]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=True):\n",
    "\n",
    "        \"\"\"\n",
    "        My custom Convolution 2D layer.\n",
    "\n",
    "        [input]\n",
    "        * in_channels  : input channel number\n",
    "        * out_channels : output channel number\n",
    "        * kernel_size  : kernel size\n",
    "        * stride       : stride size\n",
    "        * padding      : padding size\n",
    "        * bias         : taking into account the bias term or not (bool)\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "\n",
    "        ## Create the torch.nn.Parameter for the weights and bias (if bias=True)\n",
    "        ## Be careful about the size\n",
    "        # weight shape  (C_out, C_in, kH, kW)\n",
    "        # N, C_in, H_in, W_in = input.shape\n",
    "\n",
    "        self.W =  nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.b = nn.Parameter(torch.zeros(out_channels)) if bias else None\n",
    "\n",
    "        # out = ( (2*p + H - k) / s ) + 1            \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "        [input]\n",
    "        x (torch.tensor)      : (batch_size, in_channels, input_height, input_width)\n",
    "\n",
    "        [output]\n",
    "        output (torch.tensor) : (batch_size, out_channels, output_height, output_width)\n",
    "        \"\"\"\n",
    "\n",
    "        # call MyFConv2D here\n",
    "        output = MyFConv2D(x, self.W, self.b, self.stride, self.padding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Difference: tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create inputs for validation\n",
    "batch_size, in_channels, H, W = 4,5,8,10 # Example dimensions\n",
    "out_channels = 3\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 2\n",
    "\n",
    "# Input and weights\n",
    "input_tensor = torch.rand(batch_size, in_channels, H, W)\n",
    "weights = torch.rand(out_channels, in_channels, kernel_size, kernel_size)\n",
    "bias = torch.zeros(out_channels)\n",
    "\n",
    "# custom_conv_output = MyFConv2D(input_tensor, weights, bias, stride, padding)\n",
    "\n",
    "myConvd2D = MyConv2D(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "\n",
    "conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "conv2d.weight.data = myConvd2D.W.data.clone()\n",
    "conv2d.bias.data = myConvd2D.b.data.clone()\n",
    "\n",
    "# Apply PyTorch Conv2D\n",
    "with torch.no_grad():  # Ensure Conv2D weight and bias are not updated\n",
    "    torch_conv_output = conv2d(input_tensor)\n",
    "\n",
    "\n",
    "my_conv_output = myConvd2D(input_tensor)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"Output Difference:\", torch.sum(torch_conv_output - my_conv_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 8, 10])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = b = 0\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
