{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.1618, -1.5055],\n",
       "         [ 0.4743,  0.1000]]),\n",
       " tensor([[-1.3834,  0.8644],\n",
       "         [ 0.5144,  0.0782]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,2)\n",
    "y = torch.randn(2,2)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5055,  0.1000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3200, -2.4219],\n",
       "         [-0.0705, -0.3499]]),\n",
       " 0.32002588)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiply(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 6, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = F.pad(x,(1,1,1,1),mode='constant',value=0)\n",
    "padded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 6, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Padding incorrect",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m padded\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPadding incorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Padding incorrect"
     ]
    }
   ],
   "source": [
    "assert padded.size() == (1,2,6,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyFConv2D(input, weight, bias=None, stride=1, padding=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    My custom Convolution 2D calculation.\n",
    "\n",
    "    [input]\n",
    "    * input    : (batch_size, in_channels, input_height, input_width)\n",
    "    * weight   : (you have to derive the shape :-)\n",
    "    * bias     : bias term\n",
    "    * stride   : stride size\n",
    "    * padding  : padding size\n",
    "\n",
    "    [output]\n",
    "    * output   : (batch_size, out_channels, output_height, output_width)\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(input.shape) == len(weight.shape) , \"weight shape incorrect\"\n",
    "    assert len(input.shape) == 4, \"input shape incorrect\"\n",
    "    \n",
    "    k = weight.shape[-1]\n",
    "    \n",
    "    # batch_size, in_channels, input_height, input_width = input.shape\n",
    "    N, C_in, H_in, W_in = input.shape\n",
    "    # weight shape  (C_out, C_in, kH, kW)\n",
    "    C_out, C_in, kH, kW = weight.shape \n",
    "    b, s, p = bias, stride, padding\n",
    "\n",
    "    x_pad =  F.pad(input,(p,p,p,p),mode='constant',value=0) # add 2p to last two dims of input\n",
    "    assert x_pad.shape == (N, C_in, H_in + 2*p, W_in + 2*p) , \"padding incorrect\"\n",
    "\n",
    "    # k = 0 # kernel size\n",
    "    H_out = int( (H_in + 2*p - kH) / s) + 1\n",
    "    W_out = int( (W_in + 2*p - kW) / s) + 1\n",
    "    \n",
    "    ## Derive the output size\n",
    "    ## Create the output tensor and initialize it with 0\n",
    "    output = torch.zeros(N, C_out, H_out, W_out)\n",
    "    \n",
    "    ## Convolution process\n",
    "    for b in range(N):\n",
    "        for c_o in range(C_out):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    # get the window\n",
    "                    window = x_pad[b, :, h*s:h*s+kH, w*s:w*s+kW]\n",
    "                    # apply the kernel: element-wise multiplication\n",
    "                    # (b,c_in, kH, KW) * (c_o, c_i, kH, kW) -> (b, c_o, kH, kW)\n",
    "                    output[b, c_o, h, w] = torch.sum(window * weight[c_o,:,:,:])\n",
    "                    if bias is not None:\n",
    "                        output[b, c_o, h, w] += bias[c_o]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs for validation\n",
    "batch_size, in_channels, H, W = 1, 3, 5, 5  # Example dimensions\n",
    "out_channels = 2\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "# Input and weights\n",
    "input_tensor = torch.rand(batch_size, in_channels, H, W)\n",
    "weights = torch.rand(out_channels, in_channels, kernel_size, kernel_size)\n",
    "bias = torch.rand(out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3.6337, 5.5385, 6.2107, 5.8576, 2.7936],\n",
       "          [4.5595, 7.3083, 7.9239, 9.0680, 4.7402],\n",
       "          [4.7501, 5.8573, 6.5903, 6.8003, 4.6322],\n",
       "          [5.0260, 6.2258, 6.5572, 7.0312, 4.8152],\n",
       "          [2.9814, 4.2286, 4.1540, 3.8835, 2.8146]],\n",
       "\n",
       "         [[3.6522, 6.6804, 5.6448, 6.1156, 4.0989],\n",
       "          [5.1535, 7.9776, 7.5715, 8.2140, 5.4167],\n",
       "          [4.9752, 5.7482, 7.6578, 6.8342, 4.7245],\n",
       "          [5.0539, 5.4425, 7.1029, 6.7609, 5.3278],\n",
       "          [3.4588, 4.8966, 4.3990, 4.2119, 3.2847]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_conv_output = MyFConv2D(input_tensor, weights, bias, stride, padding)\n",
    "custom_conv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Conv2D for comparison\n",
    "conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "conv2d.weight = torch.nn.Parameter(weights)\n",
    "conv2d.bias = torch.nn.Parameter(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Conv2D Output Shape: torch.Size([1, 2, 5, 5])\n",
      "PyTorch Conv2D Output Shape: torch.Size([1, 2, 5, 5])\n",
      "Output Difference: tensor(5.9605e-06)\n"
     ]
    }
   ],
   "source": [
    "# Apply PyTorch Conv2D\n",
    "with torch.no_grad():  # Ensure Conv2D weight and bias are not updated\n",
    "    torch_conv_output = conv2d(input_tensor)\n",
    "\n",
    "# Compare outputs\n",
    "print(\"Custom Conv2D Output Shape:\", custom_conv_output.shape)\n",
    "print(\"PyTorch Conv2D Output Shape:\", torch_conv_output.shape)\n",
    "print(\"Output Difference:\", torch.sum(torch_conv_output - custom_conv_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
